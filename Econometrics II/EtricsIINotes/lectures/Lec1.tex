\section{Basic assumptions}
\label{sec:basic-assumptions}

Firstly, we recall the basic assumptions of the linear regression model.
\begin{assumption}[Basic Assumptions]
    \

    \begin{enumerate}
        \item[A0.](Correct Specification). Model is correctly specified: $y_i = x_i^{\prime} \beta + u_i$

        \item[A1.](Independent Sampling). Observations $z_i = \{ y_t, x_i \}_{i = 1}^n$ are independent across $i$.
          
        \item[A2.](Full rank). The matrix $X'X = \sum x_i x'_i$ is of full rank.

        \item[A3.](Conditional Independence). $\mathbb{E}[u_i | x_i] = 0$.
          
        \item[A4.](Homoskedasticity). $\mathbb{V}[u_i | x_i] = \sigma^2$ for all $i$.
        
        $\mathbb{V}[y_i] = \mathbb{V}[x'_i \beta + u_i | x_i] = \sigma^2$
    \end{enumerate}
\end{assumption}

Under these four basic assumptions, and that $x_i$ is exogenous, giving $\mathbb{E}[x_i u_i] = 0$, then
\[
\hat{\beta} = (X^{\prime} X)^{-1}X^{\prime} Y \overset{p}{\to} \beta.
\]

\section{Frisch-Waugh-Lovell Theorem}\label{sec:FWL}

\begin{definition}[Partitioned regression]
    \

    We consider a normal linear regression model $Y = X \beta + U$.
    Let $X$ be partitioned as:
    \[X = \begin{bmatrix}
        X_1 & X_2
    \end{bmatrix}\]
    where $X$ is $n \times K$, $X_1$ is $n \times K_1$ and $X_2$ is $n \times K_2$.
    And we partition the parameter vector $\beta$ accordingly:
    \[\beta = \begin{bmatrix}
        \beta_1 \\
        \beta_2
    \end{bmatrix}\]
    where $\beta_1$ is $K_1 \times 1$ and $\beta_2$ is $K_2 \times 1$.
    Thus, the model can be written as:
    \[Y = X_1 \beta_1 + X_2 \beta_2 + U\]
    where $U$ is the error term.

    Also take the following notation:
    \begin{align*}
        P_1 = X_1 (X_1^{\prime} X_1)^{-1}X_1^{\prime}, \quad M_1 = I - P_1, \quad \tilde{X}_2 = M_1 X_2, \quad \tilde{U} = M_1 Y
    \end{align*}
    thus $\tilde{U}$ is the residual vector from the regression of $Y$ on $X_1$,
    and the $k$-th column of $\tilde{X}_2$ is the residual vector from the regression of the corresponding $k$-th column of $X_2$ on $X_1$.
\end{definition}

The OLS estimator $\beta = (\beta_1, \beta_2)$ can be obtained by regression of $Y$ on $X = [X_1, X_2]$, and can be written as:
\[
Y = X \hat{\beta} + \hat{U} = X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 + \hat{U}
\]
We are interested in algebraic expressions for $\hat{\beta}_1$ and $\hat{\beta}_2$.

Let's first focus on $\hat{\beta}_1$. The least squares estimator $\hat{\beta}_1$ is found by the joint minimization:
\[
(\hat{\beta}_1, \hat{\beta}_2) = \arg\min_{\beta_1, \beta_2} \left( Y - X_1 \beta_1 - X_2 \beta_2 \right)^{\prime} \left( Y - X_1 \beta_1 - X_2 \beta_2 \right)
\]
Denote $\left( Y - X_1 \beta_1 - X_2 \beta_2 \right)^{\prime} \left( Y - X_1 \beta_1 - X_2 \beta_2 \right)$ as $\operatorname{SSE}(\beta_1, \beta_2)$.

By nested minimization, we can rewrite the above as:
\[
\hat{\beta}_1 = \arg\min_{\beta_1} \left( \min_{\beta_2} \operatorname{SSE}(\beta_1, \beta_2) \right) 
\]
For the inner minimization problem: $\min_{\beta_2} \operatorname{SSE}(\beta_1, \beta_2)$,
this is simply the regression of $Y - X_1 \beta_1$ on $X_2$, with the solution:
\[
\arg\min_{\beta_2} \operatorname{SSE}(\beta_1, \beta_2)= (X_2^{\prime} X_2)^{-1} X_2^{\prime} (Y - X_1 \beta_1)
\]
with residuals:
\[
Y - X_1 \beta_1 - X_2 (X_2^{\prime} X_2)^{-1} X_2^{\prime} (Y - X_1 \beta_1) = (M_2 Y - M_2 X_1 \beta_1) = M_2 (Y - X_1 \beta_1)
\]
where $M_2 = I - X_2 (X_2^{\prime} X_2)^{-1} X_2^{\prime}$ is the annihilator matrix for $X_2$.

So the inner minimization problem has minimized value:
\[
\min_{\beta_2} \operatorname{SSE}(\beta_1, \beta_2)= (Y - X_1 \beta_1)^{\prime} M_2^{\prime} M_2 (Y - X_1 \beta_1) = (Y - X_1 \beta_1)^{\prime} M_2 (Y - X_1 \beta_1)
\]
Substituting this into the outer minimization problem, we have:
\begin{align*}
    \hat{\beta}_1 &= \arg \min_{\beta_1} (Y - X_1 \beta_1)^{\prime} M_2 (Y - X_1 \beta_1) \\
              &= \left( X_1^{\prime} M_2 X_1 \right)^{-1} \left( X_1^{\prime} M_2 Y \right)
\end{align*}
By a similar argument we find
\[
\hat{\beta}_2 = \left( X_2^{\prime} M_1 X_2 \right)^{-1} \left( X_2^{\prime} M_1 Y \right)
\]
By the previous notation and the fact that $M_1$ and $M_2$ are idempotent matrices, we can have:
\begin{align*}
    \hat{\beta}_2 &= \left( X_2^{\prime} M_1 X_2 \right)^{-1} \left( X_2^{\prime} M_1 Y \right) \\
              &= \left( X_2^{\prime} M_1 M_1 X_2 \right)^{-1}  \left( X_2^{\prime} M_1 M_1 Y \right) \\
              &= \left( \tilde{X}_2^{\prime} \tilde{X}_2 \right)^{-1} \left( \tilde{X}_2 \tilde{U} \right)
\end{align*}
Thus the coefficient estimator $\hat{\beta}_2$ is algebraically equivalent to the OLS estimator of the regression of $\tilde{U}$ on $\tilde{X}_2$.
Notice that these two are $M_1 Y$ and $M_1 X_2$, and we know that pre-multiplication by $M_1$ creates least squares residuals.
Therefoe, $\tilde{U}$ is simply the least squares residual from a regression of $Y$ on $X_1$,
and the columns of $\tilde{X}_2$ are the least squares residuals from a regression of the columns of $X_2$ on $X_1$.
From the above steps, we have proven the following theorem.

\begin{theorem}[Frisch-Waugh-Lovell (FWL) theorem]\label{thm:FWL}
    \

    In the model $Y = X_1 \beta_1 + X_2 \beta_2 + U$, the OLS estimator of $\beta_2$ and the OLS residuals $\hat{U}$ may be
    computed via the following two steps:
    \begin{enumerate}
        \item Regress $Y$ on $X_1$ and obtain the residuals $\hat{U}_1 = Y - X_1 \hat{\beta}_1$.
        \item Regress the columns of $X_2$ on $X_1$ and obtain the residuals $\tilde{X}_2 = M_1 X_2$.
        \item Regress $\hat{U}$ on $\tilde{X}_2$: $\hat{U} = \tilde{X}_2 b + V$ and obtain the OLS estimator $\hat{\beta}_2 = \hat{b}$ and the residual $\hat{U} = \hat{V}$.
    \end{enumerate}
    % And write $M_1 Y = M_1 X_2 b + M_1 U$, then
    % \[\hat{\beta}_{2, OLS} = \hat{b}.\]
\end{theorem}

\section{Endogeneity}
\label{sec:endogeneity}

We say that there's endogeneity in the linear model
\[y_i = x_i^{\prime} \beta + u_i\]
if $\beta$ is the parameter of interest and
\[\mathbb{E}[x_i u_i] \neq 0.\]
This is a core problem in econometrics and largely differentiates the field from statistics.

Endogeneity implies that the least squares estimator is inconsistent for the structural parameter.
Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient.
\begin{gather*}
    \hat{\beta} \overset{p}{\rightarrow} \beta + \Bigl(\mathbb{E}[X X^{\prime}] \Bigr)^{-1} \mathbb{E}[X u] \neq \beta 
\end{gather*}
The inconsistency of least squares is typically referred to as \textbf{endogeneity bias} or \textbf{estimation bias} due to
endogeneity.

Commonly, there are three reasons for endogeneity:
\begin{enumerate}
    \item Measurement error: $x_i$ is measured with error.
        
        Suppose our true Regression is: $y_i = x_i^{* \prime} \beta +\epsilon_i$, $\mathbb{E}[x_i^* \epsilon_i] = 0$, $\beta$ is the structural parameter.
        But, $x_i^{* \prime}$ is not observed. Instead, we observe: $x_i = x_i^* + v_i$, 
        where $v_i$ is the measurement error, independent of $x_i^*$ and $\epsilon_i$: $\mathbb{E}[x_i^* v_i^{\prime}] = 0$, $\mathbb{E}[v_i \epsilon_i] = 0$
        \footnote{This is an example of a latent variable model, where ``latent'' refers to an unobserved structural variable.}.

        The model $x_i = x_i^* + v_i$ with $x_i^*$ and $v_{i}$ uncorrelated, and $\mathbb{E}[v_i] = 0$ is known as the \textbf{classical measurement error model}.
        This means that $x_i$ is a noisy but unbiased estimate of $x_i^*$.
        By substitution we can express $y_i$ as a function of the observed variable $x_i$.
        \begin{gather*}
            y_i = x_i^{* \prime} \beta +\epsilon_i = (x_i - v_i)^{\prime} \beta +\epsilon_i = x_i^{\prime} \beta + u_i
        \end{gather*}
        where $u_i = \epsilon_i - v_i^{\prime} \beta$.

        This means that $(y_i, x_i)$ satisfy the linear equation $y_i = x_i^{\prime} \beta + u_i$ with an error $u_i$.
        But this error is not a projection error.
        \begin{align*}
            \mathbb{E}[x_i u_i] &= \underset{0}{\underbrace{\mathbb{E}[x_i \epsilon_i]}} - \mathbb{E}[x_i v_i^{\prime} ]\beta \\
            &= -\mathbb{E}[(x_i^* + v_i) v_i^{\prime} ]\beta \\
            &= -\underset{0}{\underbrace{\mathbb{E}[x_i^* v_i^{\prime}]}}\beta - \mathbb{E}[v_i v_i^{\prime} ]\beta \\
            &= - \mathbb{E}[v_i v_i^{\prime} ]\beta \neq 0
        \end{align*}
        if $\mathbb{E}[v_i v_i^{\prime}] \neq 0$ and $\beta \neq 0$.

        \begin{remark}[Measurement Error Bias]
            \

            Let's rewrite in matrix form: $Y = X^{* \prime} \beta + \epsilon $, $X = X^* + v$, $\mathbb{E}[X^* v] = 0$, $\mathbb{E}[\epsilon v] = 0$,
            $v$ is a $k \times 1$ error. We can write $Y = (X - v)^{\prime} \beta + \epsilon = X^{\prime} \beta + u$,
            where $u = \epsilon - v^{\prime} \beta.$ And we have: $\mathbb{E}[X u] = \mathbb{E}[(X^* + v)(\epsilon - v^{\prime} \beta)] = -\mathbb{E}[v v^{\prime}]\beta \neq 0$
            if $\beta \neq 0$ and $\mathbb{E}[v v^{\prime}] \neq 0.$

            We can calculate the form of the projection coefficient (which is consistently estimated by least squares).
            For simplicity suppose that $k=1$, we find:
            \[\beta^* = \beta + \frac{\mathbb{E}[X u]}{\mathbb{E}[X^2]} = \beta \left( 1 - \frac{\mathbb{E}[v^2]}{\mathbb{E}[X^2]} \right) \] 
            Since $\frac{\mathbb{E}[v^2]}{\mathbb{E}[X^2]} < 1$, the projection coefficient shrinks the structural parameter $\beta$ towards zero.
            This is called \textbf{measurement error bias} or \textbf{attenuation bias}.
        \end{remark}
    \item Simultaneity (Reverse causality): Simultaneity arises when at least one of the explanatory variables is determined simultaneously along with $y$.
        If, say, $x_i$ is determined partly by $y$, and $x_i$ and $u_i$ are generally correlated.
        \[y_i = x_i^{\prime} \beta + u_i = x_{i1}^* \beta_1 + x_{i2} \beta_2  + u_i, \quad x_i = z_i^{\prime} \gamma + y_i \delta + v_i.\]

        \begin{eg}[Supply and Demand]
            \
            
            The variables $Q$ and $P$ (quantity and price) are determined jointly by the demand equation:
            \[Q = -\beta_1 P + u_1\]
            and supply function:
            \[Q = \beta_2 P + u_2\]
            where $u_1$ and $u_2$ are the demand and supply shocks, respectively.
            Assume that $u = (u_1, u_2)$ satisfy that $\mathbb{E}[u] = 0$ and $\mathbb{E}[u u^{\prime}] = I_2$ (for simplicity).
            The question is: If we regress $Q$ on $P$, what will happen?
            Let's solve $P$ and $Q$ in error terms:
            \begin{align*}
                & \begin{bmatrix}
                    1 & \beta_1 \\
                    1 & -\beta_2
                \end{bmatrix}
                \begin{bmatrix}
                    Q \\
                    P
                \end{bmatrix}
                = \begin{bmatrix}
                    u_1 \\
                    u_2
                \end{bmatrix} \\
                \begin{bmatrix}
                    Q \\
                    P
                \end{bmatrix}
                &= \begin{bmatrix}
                    1 & \beta_1 \\
                    1 & -\beta_2
                \end{bmatrix}^{-1}
                \begin{bmatrix}
                    u_1 \\
                    u_2
                \end{bmatrix} \\
                &= \frac{1}{\beta_1 + \beta_2}
                \begin{bmatrix}
                    \beta_2 & \beta_1 \\
                    1 & -1
                \end{bmatrix}
                \begin{bmatrix}
                    u_1 \\
                    u_2
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    \frac{\beta_2 u_1 + \beta_1 u_2}{\beta_1 + \beta_2} \\
                    \frac{u_1 - u_2}{\beta_1 + \beta_2}
                \end{bmatrix}
            \end{align*}
            The projection of $Q$ on $P$ yields $Q = \beta^* P + u^*$, where $\mathbb{E}[P u^*]=0$ and the projection coefficient is
            \[\beta^* = \frac{\mathbb{E}[PQ]}{\mathbb{E}[P^2]} = \frac{\beta_2 - \beta_1}{2}.\]
            The OLS estimator satisfies $\hat{\beta} \underset{p}{\rightarrow} \beta^*$ and the limit does not equal either $\beta_1$ or $\beta_2$.
            This is called \textbf{simultaneity bias} or \textbf{simultaneous equation bias}.

            This occurs generally when Y and X are jointly determined, as in a market equilibrium.
            Generally, when both the dependent variable and a regressor are simultaneously determined then the regressor should be treated as endogenous.
        \end{eg}

    \item Omitted variables: The most prominent cause of endogeneity are omitted variables (OVs).

        Suppose the true regression is: $y_i = x_i^{\prime} \beta + w_i^{\prime} \delta +\epsilon_i$, 
        where exogeneity holds: $\mathbb{E}[x_i \epsilon_i] = 0$, $\mathbb{E}[w_i \epsilon_i] = 0$.
 
        If we omit $w_i$ and instead estimates:
        \[y_i = x_i^{\prime} \beta + u_i\]
        where $u_i = w_i^{\prime} \delta + \epsilon_i$,
        then in this misspecified model, exogeneity is only given if $x_i$ and $w_i$ are uncorrelated, since:
        \begin{align*}
            \mathbb{E}[x_i u_i] &= \mathbb{E}[x_i(w_i^{\prime} \delta + \epsilon_i)] \\
            &= \mathbb{E}[x_i w_i^{\prime}]\delta + \underset{0}{\underbrace{\mathbb{E}[x_i \epsilon_i]}}
        \end{align*}
        Since $\hat{\beta} - \beta \overset{p}{\rightarrow} \mathbb{E}[x_i x_i^{\prime}]^{-1} \mathbb{E}[x_i u_i]$,
        we can assess the sign and size of the asymptotic bias based ont the signs of correlation between $x_i$ and $w_i$.
\end{enumerate}

