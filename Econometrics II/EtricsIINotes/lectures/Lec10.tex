Many time series arising in practice are best considered as components of
some vector-valued (multivariate) time series $y_t = (y_{1t}, y_{2t}, \cdots, y_{nt})^{\prime}$,
which is an $n \times 1$ vector process observed in sequence series $y_t$,
whose specification includes two parts:
\begin{enumerate}
    \item[(1)] The serial dependence of each component series $\{y_{it}\}$;
    \item[(2)] THe interdependence between different component series $\{y_{it}\}$ and $\{y_{jt}\}$.
\end{enumerate}
The most common multivariate time series models used by economists are vector autoregressions (VARs).

\section{Multivaraite Processes}
Stationarity and ergodicity of a multivariate process are defined analogously to the univariate case.
If all the finite dimensional joint distributions of the random variables $\{y_{it}\}$ were multivariate normal,
then the distributional properties of $\{y_{it}\}$ would be completely determined by the means,
\begin{gather*}
    \mu_{it} = \mathbb{E}[y_{it}]
\end{gather*}
and covariances,
\begin{gather*}
    \gamma_{ij}(t, t-h) = \Cov[y_{i,t}, y_{j,t-h}] = \mathbb{E}\left[ (y_{i,t} - \mu_{i,t})(y_{j,t-h} - \mu_{j,t-h}) \right].
\end{gather*}
It is more convenient when dealing with $n$ interrelated series to use vector notation. 
The second-order properties of the multivariate process are specified by the mean vectors:
\begin{gather*}
    \mu_t = \mathbb{E}[y_t]
\end{gather*}
and covariance matrices:
\begin{gather*}
    \Gamma_t(h) = \Cov[y_t, y_{t-h}] = \mathbb{E}\left[ (y_t - \mu_t)(y_{t-h} - \mu_{t-h})^{\prime} \right].
\end{gather*}
\begin{definition}[Stationary Multivariate Time Series]\label{def:stationary-multivariate}
    \

    A multivariate time series $\{y_t\}$ is said to be weakly stationary if:
    \begin{enumerate}
        \item[(1)] $\mu_t = \mu$ for all $t$;
        \item[(2)] $\Gamma_t(h) = \Gamma(h) = \mathbb{E}\left[(y_t - \mu)(y_{t-h} - \mu)^{\prime}\right]$ for all $t$ and $h$.
    \end{enumerate}
\end{definition}
We shall refer to $\mu$ as the mean of the series and $\Gamma(h)$ as the covariance function of the series at lag $h$. 
\begin{eg}
    \

    Take the bivariate case($n=2$) as an example, we have:
    \begin{gather*}
        \Gamma(h) = \begin{bmatrix}
            \Cov[y_{1,t}, y_{1,t-h}] & \Cov[y_{1,t}, y_{2,t-h}] \\
            \Cov[y_{1,t-h}, y_{2,t}] & \Cov[y_{2,t}, y_{2,t-h}] \\
        \end{bmatrix}
        = \begin{bmatrix}
            \gamma_{11}(h) & \gamma_{12}(h) \\
            \gamma_{21}(h) & \gamma_{22}(h) \\
        \end{bmatrix}
    \end{gather*}
    The diagonal elements are simply the univariate autocovariances of $y_{1t}$ and $y_{2t}$,
    while the off-diagonal elements are the cross-covariances between $y_{1t}$ and $y_{2t}$,
    with the former (latter) leading the latter (former) by a displacement of $h$ lags.
    We should notice that $\gamma_{12}(h) \neq \gamma_{21}(h)$,
    as the correlation of $y_{1t}$ with the past movement in $y_{2t}$ is not necessarily the same as the correlation of $y_{2t}$ with the past movement in $y_{1t}$.

    As a result, in contrast to the univariate case, in multivariate case, $\Gamma(h) \neq \Gamma(-h).$
    Instead, we have $\gamma_{12} (h) = \gamma_{21} (-h)$, because:
    \begin{gather*}
        \Gamma_t(-h)^{\prime} = \mathbb{E}\left[(y_t - \mu_t)(y_{t+h} - \mu_{t+h})^{\prime}\right]^{\prime} = \mathbb{E}\left[(y_{t+h} - \mu_{t+h})(y_t - \mu_t)^{\prime} \right] = \Gamma_{t+h}(h)
    \end{gather*}
    and under weakly stationarity, time doesn't matter: $\Gamma_{t+h}(h) = \Gamma_t(h) = \Gamma(h)$,
    and therefore, $\Gamma(h) = \Gamma(-h)^{\prime}$
\end{eg}
A multivariate White Noise (WN) process and the multivariate General Linear Process
(GLP) are also defined analogously to the univariate case.
\begin{definition}[Multivariate White Noise]\label{def:multivariate-wn}
    \

    The $n$-variate series $u_t$ is said to be a WN process with mean zero and covariance $\Sigma$,
    written $u_t \sim WN(0, \Sigma)$,
    if and only if $u_t$ is weakly stationary with mean vector $\mathbf{0}$ and covariance matrix function:
    \begin{gather*}
        \Gamma(h) = \Cov[u_t, u_{t-h}] = \mathbb{E}[u_t u_{t-h}^{\prime}]\begin{cases}
            \Sigma, & h=0 \\
            0, & h \neq 0
        \end{cases}
    \end{gather*}
    Note that $\Sigma$ is not necessarily diagonal, i.e. the individual components of the vector valued process $u_t$ may be contemporaneously correlated.
    However, the defining feature of a WN process is that each of the components is uncorrelated with its own past (and future) movements as well as those of any of the other components.
\end{definition}
The multivariate GLP is defined as:
\begin{gather*}
    y_t = B(L) u_t = \sum_{l=0}^{\infty} B_l u_{t-l}, \quad u_t \sim WN(0, \Sigma)
\end{gather*}
where $B_0 = I$ and $\sum_{l=0}^{\infty} \lVert B_l \rVert^2 < \infty.$

\section{Vector Autoregressions (VARs)}\label{sec:VAR}
A VAR is a common way to approximate the multivariate GLP
As in the previous lecture, we have built a VAR(1) model for solving the AR(2) process.
\begin{gather*}
    y_t = \Phi_0 + \Phi_1 y_{t-1} + u_t, \quad u_t \sim WN(0, \Sigma)
\end{gather*}
\begin{gather*}
    \begin{bmatrix}
        y_{1t} \\
        y_{2t}
    \end{bmatrix} = \begin{bmatrix}
         \Phi_{0,1} \\
         \Phi_{0,2} \\
    \end{bmatrix} + \begin{bmatrix}
        \Phi_{1,11} & \Phi_{1,12}  \\
        \Phi_{1,21} & \Phi_{1,22}  \\
    \end{bmatrix}
    \begin{bmatrix}
        y_{1,t-1} \\
        y_{2,t-1}
    \end{bmatrix} + \begin{bmatrix}
         u_{1t} \\
         u_{2t} \\
    \end{bmatrix}, \quad u_t \sim WN(0, \Sigma)
\end{gather*}
where $u_t \sim  WN\left( \begin{bmatrix}
     0 \\
     0 \\
\end{bmatrix}, \begin{bmatrix}
    \sigma_{11} & \sigma_{12} \\
    \sigma_{21} & \sigma_{22} \\
\end{bmatrix} \right)$.
There are two sources of interaction between $y_{1t}$ and $y_{2t}$:
\begin{enumerate}
    \item[(1)] The value of each series today depends on its own, but also on the other variable's value at time $t-1$;
    \item[(2)] The innovations of the two series are potentially correlated (contemporaneously).
\end{enumerate}
If the equations include $p$ lags of each variable we obtain the $p$-th order vector autoregressive (VAR) model:
\begin{gather}\label{eq:varp}
    y_t = \Phi_0 + \Phi_1 y_{t-1} + \cdots + \Phi_p y_{t-p} + u_t
\end{gather}
Our notation allows $y_t = [y_{1t}, y_{2t}, \cdots, y_{nt}]^{\prime}$ to be a vector of dimension $n$ in which case the matrices $\Phi_i$ are $n \times n$ and error $u_t$ is $n \times 1$.
We will denote the elements of $\Phi_i$ using the notation:
\begin{gather*}
    \Phi_i = \begin{bmatrix}
        \Phi_{i,11} & \Phi_{i,12} & \cdots & \Phi_{i,1n} \\
        \Phi_{i,21} & \Phi_{i,22} & \cdots & \Phi_{i,2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \Phi_{i,n1} & \Phi_{i,n2} & \cdots & \Phi_{i,nn} \\
    \end{bmatrix}.
\end{gather*}
The error $u_t = [u_{1t}, u_{2t}, \cdots, u_{nt}]^{\prime}$ is the component of $y_t$ which is unforecastable at time $t-1$.
However, the components of $y_t$ are contemporaneously correlated, therefore the contemporabeous covariance matrix $\Sigma = \mathbb{E}[u u^{\prime}]$ is non-diagonal.
\begin{definition}[VAR$(p)$]
    \

    A VAR(p) model is a system of $n$ equations of the form:
    \begin{gather}\label{eq:varp-def}
        y_t = \Phi_0 + \sum_{i=1}^{p} \Phi_i y_{t-i} + u_t, \quad u_t \sim WN(0, \Sigma)
    \end{gather}
    where $y_t$ is an $n$-dimensional vector, $\Phi_0$ is an $n$-dimensional vector of constants, $\Phi_i$ are $n \times n$ matrices of coefficients, and $u_t$ is an $n$-dimensional vector of white noise errors.
\end{definition}

\subsection{Stationarity of VAR\texorpdfstring{$(p)$}{(p)}}\label{sec:stationarity-var}
We are interested in conditions under which $y_t$ is a stationary process.
Let $\lambda_i(\Phi)$ denote the $i$-th eigenvalue of the matrix $\Phi$.
\begin{theorem}[SS and Ergodic of VAR(1)]\label{thm:stationarity-var1}
    \

    If $u_t$ is strictly stationary and ergodic, with $\mathbb{E}[u_t] < \infty$, and $\vert \lambda_i(\Phi_1) \vert < 1$,
    then the VAR$(1)$ process $y_t$ is also strictly stationary and ergodic.
\end{theorem}
\begin{eg}
    \

    Consider the VAR(2) model:
    \begin{gather*}
        y_t = \Phi_0 + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + u_t
    \end{gather*}
    or we write in companion form:
    \begin{gather*}
        \begin{bmatrix}
            y_t \\
            y_{t-1}
        \end{bmatrix} = \underset{F_0}{\underbrace{\begin{bmatrix}
            \Phi_0 \\
            0
        \end{bmatrix}}} + 
        \underset{F_1(np \times np)}{\underbrace{\begin{bmatrix}
            \Phi_1 & \Phi_2 \\
            I & 0
        \end{bmatrix}}}
        \begin{bmatrix}
            y_{t-1} \\
            y_{t-2}
        \end{bmatrix} + \begin{bmatrix}
             u_t \\
             0 \\
        \end{bmatrix}
    \end{gather*}
    we can simply write:
    \begin{gather*}
        X_t = F_0 + F_1 X_{t-1} + v_t
    \end{gather*}
    We take mean on both sides,
    \begin{gather*}
        \mathbb{E}[y_t] = \Phi_0 + \Phi_1 \mathbb{E}[y_{t-1}] + \Phi_2 \mathbb{E}[y_{t-2}] + 0
    \end{gather*}
    as $\mathbb{E}[y_t] = \mathbb{E}[y_{t-1}] = \mathbb{E}[y_{t-2}] = \mu$,
    we know that $\mu = \left(1 - \Phi_1 - \Phi_2\right)^{-1} \Phi_0.$
\end{eg}
For the $VAR(p)$ model \ref{eq:varp-def}, we can write using the lag operator notation:
\begin{gather*}
    \Phi (L) y_t = \Phi_0 + u_t
\end{gather*}
where
\begin{gather*}
    \Phi(z) = I_n - \Phi_1 z - \Phi_2 z^2 - \cdots - \Phi_p z^p.
\end{gather*}
The condition for stationarity of the system can be expressed as a restriction on the roots of the
determinantal equation of the autoregressive polynomial.
Recall that a root $r$ of $\det \left(\Phi(z) \right)$ is a solution to $\det \left(\Phi(r) \right) = 0$.
\begin{theorem}[SS and Ergodicity of VAR(p)]\label{thm:stationarity-varp}
    \

    If all roots $r$ of the polynomial $\det \left(\Phi(z) \right)$ satisfy $\vert r \vert > 1$,
    then the VAR(p) process $y_t$ is strictly stationary and ergodic.
\end{theorem}
Or, equivalently, we write the VAR$(p)$ model in companion form:
\begin{gather*}
    \begin{bmatrix}
        y_t \\
        y_{t-1} \\
        \vdots \\
        y_{t-p+1}
    \end{bmatrix} = \begin{bmatrix}
         \Phi_0 \\
         0 \\
         \vdots \\
         0 \\
    \end{bmatrix} + 
    \underset{F_{np \times np}}{\underbrace{\begin{bmatrix}
        \Phi_1 & \Phi_2 & \cdots & \Phi_p \\
        I & 0 & \cdots & 0 \\
        0 & I & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & \cdots & I & 0 \\
    \end{bmatrix}}}
    \begin{bmatrix}
        y_{t-1} \\
        y_{t-2} \\
        \vdots \\
        y_{t-p}
    \end{bmatrix} + 
    \begin{bmatrix}
         u_t \\
         0 \\
         \vdots \\
         0 \\
    \end{bmatrix}
\end{gather*}
We can simply write $x_t = F_0 + F x_{t-1} + v_t$,
hence theorem \ref{thm:stationarity-var1} can be reorganize to:
\begin{theorem}[SS and Ergodicity of VAR(p)]\label{thm:stationarity-varp-2}
    \

    If all eigenvalues $\lambda_i(F)$ of the matrix $F$, given by:
    \begin{gather*}
        \vert I_n \lambda^p - \Phi_1 \lambda^{p-1} - \Phi_2 \lambda^{p-2} - \cdots - \Phi_p \vert = 0
    \end{gather*}
    satisfy $\vert \lambda_i(F) \vert < 1$,
    then the VAR$(p)$ process $y_t$ is strictly stationary and ergodic.
\end{theorem}

\subsection{Moments \& GLP-Representation}
Once we know that a VAR$(p)$ is Weakly Stationary, it is quite trivial to calculate its mean.

Consider first for a VAR$(1)$. Repeatedly inserting for lags $y_{t-1}$, we get:
\begin{align*}
    y_t &= \Phi_0 + \Phi_1 y_{t-1} + u_t \\
    &= \Phi_0 + \Phi_1 (\Phi_0 + \Phi_1 y_{t-2} + u_{t-1}) + u_t \\
    &= \Phi_0 + \Phi_1 \Phi_0 + \Phi_1^2 y_{t-2} + \Phi_1 u_{t-1} + u_t \\
    &= \cdots \\
    &= \sum_{l=0}^{\infty} \Phi_1^l \Phi_0 + \sum_{l=0}^{\infty} \Phi_1^l u_{t-l} + \lim_{k \to \infty} \Phi_1^k y_{t-k} \\
    &= \frac{\Phi_0}{1 - \Phi_1} + \sum_{l=0}^{\infty} \Phi_1^l u_{t-l}
\end{align*}
if $\Phi_1$ jas all eigenvalues in the unit circle, then $\lim_{k \to \infty} \Phi_1^k y_{t-k} = 0$.
Based on this expresison, we can get the ACF:
\begin{align*}
    \Gamma_h = \Cov[y_t, y_{t-h}] &= \mathbb{E}\left[ \left(\sum_{t=0}^{\infty}\Phi_1^l u_{t-l} \right) \left( \sum_{k=0}^{\infty}\Phi_1^k u_{t-h-k} \right)^{\prime} \right] \\
    &= \sum_{k=0}^{\infty} \mathbb{E}\left[ \left(\Phi_1^k u_{t-h-k} \right) \left(\Phi_1^k u_{t-h-k} \right)^{\prime} \right] \\
    &= \sum_{k=0}^{\infty} \Phi_1^{h+k}  \mathbb{E}\left[ u_{t-h-k} u_{t-h-k}^{\prime} \right] \Phi_1^{k^{\prime}} \\
    &= \sum_{k=0}^{\infty} \Phi_1^{h+k} \Sigma \Phi_1^{k^{\prime}}
\end{align*}
As we generalize this to VAR(p) ($X_t = F_0 + F_1 X_{t-1} + v_t$), we can get:
\begin{gather*}
    \Gamma_h^X = \sum_{k=0}^{\infty} F_1^{h+k} \Sigma^v F_1^{k^{\prime}}
\end{gather*}
where $\mathbb{V}[u_t] = \Sigma^v = \begin{bmatrix}
    \Sigma^v & 0 \\
    0 & 0 \\
\end{bmatrix}$.
And we can have the ACF for the original $y_t$:
\begin{align*}
    \Gamma_h^y = \Cov[y_t, y_{t-h}] &= \Cov[MX_t, MX_{t-h}] \\
    &= M \Cov[X_t, X_{t-h}] M^{\prime} \\
    &= \sum_{k=0}^{\infty} M F_1^{h+k} \Sigma^v F_1^{k^{\prime}} M^{\prime} \\
    &= \sum_{k=0}^{\infty} \left(F_1^{k+h} \right)_{11} \Sigma \left( F_1^k \right)_{11}^{\prime}
\end{align*}

\subsection{Reduced-Form vs. Structural Representation}
The above expressions for the VAR model are called reduced-form representation and the $u_t$ that appear in it are called reduced-form errors.
They are the forecasting errors obtained when predicting $y_t$ one step ahead:
\begin{align*}
    y_t &= \Phi_0 + \Phi_1 y_{t-1} + u_t \\
    \Rightarrow \mathbb{E}_{t-1} [y_t] &= \Phi_0 + \Phi_1 y_{t-1} + \mathbb{E}_{t-1} [u_t]\\
    \Rightarrow u_t &= y_t - \mathbb{E}_{t-1} [y_t]
\end{align*}

To make causal statements in VAR, we need to decompose the reduced-form errors into underlying, independent driving forces of $y_t$, referred to as shocks.
We can write:
\begin{gather*}
    u_t = \Phi_{\varepsilon} \varepsilon_t, \quad \varepsilon_t \sim WN(0, I)
\end{gather*}
where $\varepsilon_t$ is the vector of shocks.

\begin{gather*}
    \text{Reduced-form: } y_t = \Phi_0 + \Phi_1 y_{t-1} + u_t, \quad u_t \sim WN(0, \Sigma) \\
    \text{Structural: } y_t = \Phi_0 + \Phi_1 y_{t-1} + \Phi_{\varepsilon} \varepsilon_t, \quad \varepsilon_t \sim WN(0, I) \\
    \Leftrightarrow A y_t = B_0 + B_1 y_{t-1} + \varepsilon_t, A = \Phi_{\varepsilon}^{-1} , B_0 = \Phi_{\varepsilon}^{-1} \Phi_0, B_1 = \Phi_{\varepsilon}^{-1} \Phi_1
\end{gather*}


For the structural representation, we can write:
\begin{gather*}
    y_t = \left(I - \Phi_1\right)^{-1} \Phi_0 + \sum_{l=0}^{\infty} \Phi_1^l \Phi_{\varepsilon} \varepsilon_{t-l}  \\
    \Gamma_0 = \sum_{l=0}^{\infty} \Phi_1^l \Sigma^v \Phi_1^{l^\prime}, \quad \Sigma_v = \Phi_{\varepsilon} \Phi_{\varepsilon}^{\prime}
\end{gather*}

We can define the impulse response function (IRF) as:
\begin{gather}\label{eq:IRF}
    \pd{y_{1, t+h}}{\varepsilon_{2, t}} = \left[ \pd{y_{t+h}}{\varepsilon_t} \right]_{12} = \left[\Phi_1^h \Phi_{\varepsilon}\right]_{12}
\end{gather}

A variance decomposition of $y_t$ determines the contribution of each shock to the variance of $y_t$.
\begin{gather*}
    \Gamma_0 = \mathbb{V}[y_t] = \sum_{l=0}^{\infty} \Phi_1^l \Phi_\varepsilon I \Phi_\varepsilon^{\prime} \Phi_1^{l^\prime}.
\end{gather*}
Let $I^{(j)}$ be the identity matrix with all but the $j$-th diagonal element equal to zero. Then
\begin{gather*}
    \Gamma_0^{(j)} = \sum_{l=0}^{\infty} \Phi_1^l \Phi_\varepsilon I^{(j)} \Phi_\varepsilon^{\prime} \Phi_1^{l^\prime}
\end{gather*}
We have that $\Gamma_0 = \sum_{j=1}^{n} \Gamma_0^{(j)}$, as a result,
the fraction of the variance of a particular series $y_{it}$, $\mathbb{V}[y_{it}] = \left[\Gamma_0\right]_{ij}$ is explained by $\varepsilon_{jt}$ given by:
\begin{gather*}
    \left[\Gamma_0^{(j)}\right]_{ij} / \left[\Gamma_0\right]_{ij}.
\end{gather*}

\section{Estimation of Reduced-Form VARs}
\label{sec:estimation-var}

Consider the general VAR(p) model:
\begin{gather}\label{eq:varest}
    y_t = \Phi_0 + \Phi_1 y_{t-1} + \cdots + \Phi_p y_{t-p} + u_t, \quad u_t \sim WN(0, \Sigma)
\end{gather}
and we write it into a simple regression form:
\begin{gather*}
    y_t^{\prime} = x_t^{\prime} \Phi + u_t^{\prime}, \quad x_t = \begin{bmatrix}
         y_{t-1} \\
         y_{t-2} \\
         \vdots \\
         1 \\
    \end{bmatrix}, \Phi = \begin{bmatrix}
         \Phi_1^{\prime} \\
         \Phi_2^{\prime} \\
         \vdots \\
         \Phi_0^{\prime} \\
    \end{bmatrix}
\end{gather*}
in matrix form:
\begin{gather*}
    Y = X \Phi + U, Y = \begin{bmatrix}
         y_1^{\prime} \\
         y_2^{\prime} \\
         \vdots \\
         y_T^{\prime} \\
    \end{bmatrix}.
\end{gather*}
To simplify, we only analyze for the OLS estimator:
\begin{align*}
    \min_{\Phi} \sum_{t} \sum_{i} u_{it}^2 &= \min_{\Phi} \sum_{t} u_t^{\prime} u_t \\
    % &= \min_{\Phi} (Y - X\Phi)^{\prime} (Y - X\Phi) \\
    &= \min_{\Phi} \tr\left[ (Y - X\Phi)^{\prime} (Y - X\Phi) \right] \\
    \Rightarrow \hat{\Phi} &= (X^{\prime} X)^{-1} X^{\prime} Y \\
    \Sigma = \mathbb{V}[u_t] &= \begin{bmatrix}
        \mathbb{V}[u_{1t}] & \Cov[u_{1t}, u_{2t}] \\
        \cdot &  \mathbb{V}[u_{2t}] \\
    \end{bmatrix} \leftarrow \begin{bmatrix}
        \frac{1}{T} \sum_{t} \hat{u}_{it}^2 & \frac{1}{T} \sum_{t} \hat{u}_{1t} \hat{u}_{2t} \\
        \cdot & \frac{1}{T} \hat{u}_{2t} \\
    \end{bmatrix} = \hat{\Sigma} \\
    \hat{\Sigma} &= \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t \hat{u}_t^{\prime} = \frac{1}{T} \hat{U}^{\prime} \hat{U} = \frac{1}{T} \left(Y - X \hat{\Phi} \right)^{\prime} \left(Y - X \hat{\Phi} \right)
\end{align*}


\section{Estimation of Structural VARs}
\label{sec:estimation-structural-var}

We  consider the structural VAR(1) model in the form:
\begin{gather*}
    y_t = \Phi_0 + \Phi_1 y_{t-1} + \Phi_{\varepsilon} \varepsilon_t, \quad \varepsilon_t \sim WN(0, I)
\end{gather*}
We cannot identify $\Phi_\varepsilon$ from the data, as for a given $\Sigma$, there is infinite number of $\Phi_\varepsilon$ that can generate the same $\Sigma$.

We can further analyze the structural VAR model by decomposing:
\begin{gather*}
    \Phi_{\varepsilon} = \Sigma_{\tr} \Omega
\end{gather*}
where $\Sigma_{\tr}$ is the Cholesky decomposition of $\Sigma$, $\Sigma_{\tr} \Sigma_{\tr}^{\prime} = \Sigma$,
and $\Omega$ is any orthogonal matrix with $\Omega \Omega^{\prime} = I.$

\section{Point Restrictions}
\label{sec:point-restrictions}

\begin{eg}
    \

    Consider the following example:
    \begin{gather*}
        y_t = \begin{bmatrix}
             GDP_t \\
             IFL_t \\
             i_t \\
        \end{bmatrix}, \varepsilon_t = \begin{bmatrix}
             S_t \\
             d_t \\
             m_t \\
        \end{bmatrix}
    \end{gather*}
    so that
    \begin{gather*}
        \pd{GDP_t}{m_t} = \left[ \pd{y_{1t}}{\varepsilon_{3t}} \right] = \left[\Phi_1^h \Phi_\varepsilon \right]_{13} = 0
    \end{gather*}
\end{eg}