\section{Incidental Parameters Problem}

\subsection{Consistency}
Suppose we are estimating the following panel data regression:
\begin{align*}
  y_{it} &= \alpha +x_{it}^{\prime} \beta +u_{it}, \quad \mathbb{E}[u_{it} x_{it}] = 0, \quad \mathbb{V}[u_{it} | x_{it}] = \sigma^2
\end{align*}

Omitting the distinction between intercept and slope, we can write the model as:
\begin{align*}
  y_{it} &= \tilde{x}_{it}^{\prime} \tilde{\beta} + u_{it} \\
  \tilde{x}_{it} &= \begin{bmatrix}
    1 \\
    x_{it}
  \end{bmatrix} \\
  \tilde{\beta} &= \begin{bmatrix}
    \alpha \\
    \beta
  \end{bmatrix}
\end{align*}
where $i=1:n$, $T=1:t$.

Or, we can write the model as: 
\[ 
\underset{T\times 1}{y_i} = \underset{T \times K}{\tilde{X}_i} \underset{K \times 1}{\tilde{\beta}} + \underset{T \times 1}{u_i}
\]
Using OLS method to estimate $\tilde{\beta}$, we have:
\[
\underset{\tilde{\beta}}{\min} \sum_i \sum_t u_{it}^2 = \underset{\beta}{\min} \sum_i u_i^{\prime} u_i = \underset{\beta}{\min} (y_i - \tilde{X}_i \tilde{\beta})^{\prime} (y_i - \tilde{X}_i \tilde{\beta})
\]
The FOC of this equation is:
\begin{align*}
  \sum_i -\tilde{X}_i^{\prime} (y_i - \tilde{X}_i \tilde{\beta}) &= 0 \\
  \left(\sum_i \tilde{X}_i^{\prime} \tilde{X}_i \right) \tilde{\beta} &= \sum_i \tilde{X}_i^{\prime} y_i \\
  \hat{\tilde{\beta}} &= \left(\sum_i \tilde{X}_i^{\prime} \tilde{X}_i \right)^{-1} \sum_i \tilde{X}_i^{\prime} y_i \\
  &= \left(\sum_i \sum_t \tilde{x}_{it} \tilde{x}_{it}^{\prime} \right)^{-1} \left( \sum_i \sum_t \tilde{x}_{it} y_{it} \right) \\
  &= \tilde{\beta} + \left(\frac{1}{n} \sum_i \sum_t \tilde{x}_{it} \tilde{x}_{it}^{\prime} \right)^{-1} \left( \sum_i \sum_t \tilde{x}_{it} u_{it} \right) \\
  & \overset{p}{\rightarrow} \tilde{\beta} + \mathbb{E}\left[\sum_t \tilde{x}_{it} \tilde{x}_{it}^{\prime} \right] \mathbb{E}\left[\sum_t \tilde{x}_{it} u_{it} \right] \\
  &= \tilde{\beta}
\end{align*}
Hence $\hat{\beta_{OLS}}$ is consistent provided that $x_{it}$ and $u_{it}$ are contemperaneously uncorrelated,
as $\mathbb{E}[x_{it} u_{it}] = 0.$

\subsection{Asymptotic Normality}

From the analysis of consistency, we know that:
\[ 
\hat{\tilde{\beta}} = \left(\sum_i \tilde{X}_i^{\prime} \tilde{X}_i \right)^{-1} \sum_i \tilde{X}_i^{\prime} y_i
\]
Hence:
\begin{align*}
    \sqrt{n} (\hat{\tilde{\beta}} - \tilde{\beta}) &= \left(\frac{1}{n} \sum_i \tilde{X}_i^{\prime} \tilde{X}_i \right)^{-1} \left(\frac{1}{\sqrt{n} } \sum_i \tilde{X}_i^{\prime} u_i \right) \\
    & \overset{p}{\rightarrow}\mathbb{E}[\tilde{X}_i^{\prime} \tilde{X}_i]^{-1} \overset{d}{\rightarrow} \mathcal{N}\left(0, \mathbb{E}\left[\left(\tilde{X}_i^{\prime} u_i\right) \left(\tilde{X}_i^{\prime} u_i\right)^{\prime} \right] \right)\\
    & \overset{d}{\rightarrow} \mathcal{N} \left(0, \mathbb{E}\left[\tilde{X}_i^{\prime} \tilde{X}_i \right]^{-1} \mathbb{E}\left[\tilde{X}_i^{\prime} u_i u_i^{\prime} \tilde{X}_i \right] \mathbb{E}\left[\tilde{X}_i^{\prime} \tilde{X}_i \right] \right)
\end{align*}


The above model is honmogeneous, which is unattractive, as the data generating process would 
differ across $i$, with some units having a higher level of the outcome variable $y_{it} $
than others, regardless of covariates $x_{it}$(with a higher intercept $\alpha$) or a stronger effect
of some covariates $x_{it, k} $ on $y_{it}$ than others.

At the other extreme, we assume the fulle heterogenous estimation:
\[y_{it} = \alpha_i + x_{it}^{\prime} \beta + u_{it}, \quad \mathbb{E}[u_{it} x_{it}] = 0, \quad \mathbb{V}[u_{it} | x_{it}] = \sigma_i^2. \]

Under $T=1$, we run $y_i = \beta_0 + x_i^{\prime} \beta + v_i$, 
where $v_i = u_i + \underset{\tilde{\alpha}_i}{\underbrace{\alpha_i - \beta_0}}$
and $\mathbb{E}[v_i] = 0$.

Under $T>1$, we run:
\begin{align*}
    y_i &= x_i^{\prime} \beta  + \sum_{j=1}^{n} \alpha_j \mathbf{1}\{i=j\} + u_{it} \\
    &= \tilde{x}_{it}^{\prime} \tilde{\beta} + u_{it} \\
    \tilde{x}_{it} &= \begin{bmatrix}
      x_{it} \\
      \mathbf{1}\{i=1\} \\
      \mathbf{1}\{i=2\} \\
      \vdots \\
      \mathbf{1}\{i=n\}
    \end{bmatrix}, \quad
    \tilde{\beta} = \begin{bmatrix}
      \beta \\
      \alpha_1 \\
      \alpha_2 \\
      \vdots \\
      \alpha_n
    \end{bmatrix}
\end{align*}

In a similar way, we can write the regression as
\[y_{i}  = \tilde{X}_i \tilde{\beta}_i + u_i \]
with $\tilde{\beta}_i$ is specific for each $i$.
We have $n$ separate time series regressions, one for each unit $i$. 

Following the same analyzing process, we can get:
\[
\hat{\tilde{\beta}}_{i,OLS} = \Bigl( \sum_i \tilde{X}_i^{\prime} \tilde{X}_i \Bigr) \sum_i \tilde{X}_i^{\prime} y_i = \Bigl( \sum_t \tilde{x}_{it} \tilde{x}_{it}^{\prime} \Bigr)^{-1} \Bigl( \sum_t \tilde{x}_{it} y_{it} \Bigr),
\]
which obviously shows that $\hat{\tilde{\beta}}$ is consistent $\operatorname{\iff}$ $T \rightarrow\infty $.

\subsection{One-way error component model}
With the fully homogeneous specification unattractive and the fully heterogeneous specifi-
cation infeasible, researchers usually go for a compromise and let intercepts (and error term
variances) be unit-specific.

\begin{definition}[One-way error component model]
  \begin{equation}
    y_{it}  = \alpha_i + x_{it}^{\prime} + u_{it}, \quad \mathbb{E}[u_{it} x_{it}] = 0, \quad \mathbb{V}[u_{it} | x_{it}] = \sigma^2, \label{eq: basic model}
  \end{equation}
where $\alpha_i$ is an individual-speciﬁc effect, and $u_{it}$ are idiosyncratic(i.i.d.) errors.
\end{definition}

In any case, the equation above makes clear that $\alpha_i $ contains all factors that affect $y_{it}$, that are
not included in $x_{it}$ and that are ﬁxed over time (the time-varying factors are in $u_{it}$).

Suppose hte model is correctly specified, and we have a cross-sectional detaset available, i.e. $T=1$.
Then, we would estimate:
\[y_{it} = \beta_0 + x_{it}^{\prime} + v_i, \text{ for} t=1,\]
where $v_i = \alpha_i + u_{it} -\beta_0.$

If the unobserved heterogeneity $\alpha_i$ is correlated with the covariate $x_{it} $,
our standard OLS estimator is biased and inconsistent.

If we have a panel dataset, i.e. $T>1$, we can write the above model into a regression of $k+n$ regressors:
\[
y_{it} = x_{it}^{\prime} \beta + \sum_{j=1}^{n}\mathbf{1}\{i=j\} \alpha_j + u_{it} = x_{it}^{*^{\prime}} \beta^* + u_{it},
\]
where $x_{it}^* = \Bigl( x_{it}^{\prime}, \mathbf{1}\{i=1\}, \cdots, \mathbf{1}\{i=n\} \Bigr)^{\prime} $,
and $\beta^* = \Bigl( \beta^{\prime}, \alpha_1, \cdots, \alpha_n \Bigr)^{\prime}.$

This leads to the pooled OLS estimator for $\beta^*$:
\[
\hat{\beta}^* = \Bigl( \sum_i \sum_t x_{it}^* x_{it}^{*^{\prime}} \Bigr) \sum_i \sum_t x_{it}^* y_{it}. 
\]

HOwever, the estimator suffers from the so-called \textbf{IPP problem}, 
as the number of parameters increase with $n \rightarrow\infty $, 
the limit of $\frac{1}{n} \sum_i x_{it}^* x_{it}^{*^{\prime}}$ is not well-defined
and as a result, we can't establish consistency of $\hat{\beta}_{OLS}.$