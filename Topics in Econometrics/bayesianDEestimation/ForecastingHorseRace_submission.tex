\documentclass[12pt,a4paper]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\setstretch{1.25}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{placeins}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[natbibapa]{apacite}
\usepackage{fancyhdr}

\graphicspath{{results/figures/}}
\sisetup{group-separator = {,}, group-minimum-digits = 4, input-symbols = ()}

\newif\ifsubmission
% Standalone submission source: always compile submission branch.
\submissiontrue

% Helper: input generated tables but suppress their auto-generated notes blocks.
% (We keep all quantitative content sourced from `results/`, but we override notes
% in the manuscript to keep definitions and interpretation aligned with the audited code.)
\newcommand{\InputGeneratedTableNoNotes}[1]{%
    \begingroup%
    \renewenvironment{minipage}[1]{\setbox0\vbox\bgroup}{\egroup}%
    \input{#1}%
    \endgroup%
}

\begin{document}
% Title page (unnumbered)
\hypersetup{pageanchor=false}
\begin{titlepage}
    \centering
    \vspace*{1.5cm}
    {\Large Geneva Graduate Institute (IHEID)\par}
    \vspace{0.4cm}
    {\large Topics in Econometrics\par}
    \vspace{0.2cm}
    {\large Term Paper\par}
    \vspace{1.0cm}

    {\LARGE\bfseries The Incremental Predictive Power of Consumer Sentiment in Macroeconomic Forecasting\par}
    \vspace{0.25cm}
    {\large Evidence from a Hierarchical Bayesian VAR and Forecast-Revision Diagnostics\par}

    \vfill

    {\large Jingle Fu\par}
    \vspace{0.2cm}
    {\large Professor: Marko Mlikota\par}
\end{titlepage}
\hypersetup{pageanchor=true}

% Abstract page (roman numbering allowed before Introduction)
\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{plain}

\begin{abstract}
    Does consumer sentiment add predictive content for inflation and real activity once standard macro aggregates and financial prices are already included?
    I answer this question with a transparent horse race across nested information sets in a hierarchical Bayesian VAR, paired with a revision-based diagnostic of forecast updating following \citet{CG2015}.

    The information sets are nested by construction: \emph{Small} contains standard macro aggregates, \emph{Medium} adds a small set of financial prices, and \emph{Full} further adds survey-based consumer sentiment (see Section~\ref{sec:data} and \texttt{INTERNAL\_MAPPING.md} for the exact series mapping).
    The point-forecast evidence shows that richer information sets can improve accuracy, but sentiment's incremental contribution to forecast accuracy is limited once financial variables are already in the information set (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}).
    The revision diagnostic, by contrast, indicates systematic patterns in forecast updating, and the information set can shift these patterns even when point accuracy changes little (Table~\ref{tab:cg_regression}).
    Because the competing specifications are nested, I treat standard equal-accuracy tests as suggestive and use nested-model-robust adjustments as a robustness check \citep{ClarkMcCracken2001,ClarkWest2007} (Appendix Table~\ref{tab:clark_west}).
    Throughout, revision-regression coefficients are interpreted as a diagnostic of internal consistency in a regularized forecasting system (and can partly reflect prior-induced conservatism), not as structural evidence about economic agents.
\end{abstract}
\noindent\textit{Keywords:} Bayesian VAR; hierarchical shrinkage; forecasting; consumer sentiment; forecast revisions.\par

\clearpage
% Main text starts here (arabic numbering from Introduction)
\pagenumbering{arabic}
\setcounter{page}{1}

% --- Section 1: Introduction ---
\section{Introduction}

\noindent This paper studies a practical forecasting question: does consumer sentiment add incremental predictive content for inflation and real activity once conventional macro aggregates and financial prices are already included? I organize the answer around two objects that matter to forecasting practice: \emph{forecast accuracy} (how close point forecasts are to realizations) and \emph{forecast discipline} (whether the forecasting system revises in an internally coherent way, rather than exhibiting systematic updating patterns).

\paragraph{Contributions and headline evidence.}
\begin{itemize}[leftmargin=*]
    \item \textbf{Design: nested information sets in a hierarchical BVAR.} I run a horse race across nested information sets within a hierarchical Bayesian VAR that updates regularization strength within a hyperprior family rather than fixing it by hand \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}.
    \item \textbf{Accuracy versus discipline.} Sentiment's incremental contribution to point-forecast accuracy is limited once financial variables are included (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}), consistent with information overlap between sentiment and forward-looking prices \citep{BramLudvigson1998,Ludvigson2004}. At the same time, a revision-based diagnostic following \citet{CG2015} shows systematic updating patterns, and the information set can shift these patterns even when point accuracy moves little (Table~\ref{tab:cg_regression}).
    \item \textbf{Revision diagnostics as model diagnostics.} Applied to model-implied forecasts, the \citet{CG2015} regression is an updating diagnostic for a \emph{regularized} forecasting system: its coefficients summarize internal error--revision consistency and can partly reflect prior-induced conservatism rather than economic agents' behavior.
    \item \textbf{Inference discipline under nesting.} Because the specifications are nested, standard equal-accuracy tests can be distorted; I therefore emphasize magnitudes and stability and use nested-robust adjustments as robustness checks \citep{ClarkMcCracken2001,ClarkWest2007} (Appendix Table~\ref{tab:clark_west}).
\end{itemize}

\paragraph{Related literature.}
First, the revision diagnostic connects to work that uses forecast revisions to study expectation updating \citep{CG2015}; here, it is applied as a diagnostic for a model-based forecasting system rather than a structural claim about beliefs.
Second, evidence on whether confidence or sentiment contains incremental forecasting information is mixed once other indicators are included, motivating a conditional horse race \citep{CarrollFuhrerWilcox1994,BramLudvigson1998,Ludvigson2004}.
Third, the inflation-forecasting literature emphasizes that parsimonious benchmarks are often competitive and that forecasting relationships can shift, which motivates cautious interpretation of incremental gains \citep{AtkesonOhanian2001,StockWatson2007}.
Fourth, hierarchical BVAR shrinkage provides a disciplined way to compare information sets of different dimensions without ad hoc tuning \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}.

\paragraph{Roadmap.}
Section~\ref{sec:data} describes the nested information sets and the evaluation setup.
Section~\ref{sec:methods} presents the forecasting system, the role of hierarchical shrinkage, and the accuracy and revision diagnostics.
Section~\ref{sec:results} reports the evidence, and Section~\ref{sec:conclusion} concludes.

\paragraph{Empirical focus.}
The empirical goal is descriptive: quantify the incremental predictive content of sentiment conditional on macro aggregates and financial prices, and summarize forecast-updating patterns using the revision diagnostic. I avoid structural interpretations of revision-regression coefficients and treat formal comparisons under nesting cautiously.
% --- Section 2: Data ---
\section{Data}
\label{sec:data}

The dataset combines macro aggregates, a small set of widely used financial prices, and a survey-based measure of consumer sentiment. The information sets are nested to isolate incremental information content:
\begin{itemize}[leftmargin=*]
    \item \textbf{Small: macro aggregates.} Prices, real activity, labor-market slack, and the policy stance.
    \item \textbf{Medium: macro aggregates + financial prices.} Small plus a long-term interest rate, a broad equity price index, and an oil price series.
    \item \textbf{Full: macro aggregates + financial prices + sentiment.} Medium plus consumer sentiment.
\end{itemize}
The comparison between Medium and Full therefore targets whether sentiment contributes beyond information already summarized in market prices.

Following standard BVAR practice, the model is estimated in levels or log-levels \citep{Sims1980,GLP2015}. Forecasts are evaluated on a common growth-rate scale constructed from model-implied level forecasts, using the same transformation as the code pipeline. The output-to-manuscript mapping is audited in \texttt{INTERNAL\_MAPPING.md}.

% --- Section 3: Econometric Framework ---
\section{Empirical design}
\label{sec:methods}

\subsection{Forecasting system and nested information sets}
For each information set, I estimate a reduced-form VAR forecasting system and only change the information set. This isolates the incremental role of forward-looking prices and sentiment within a common estimation and prediction rule. The forecasting system is
\begin{equation}
    y_t = c + \sum_{\ell=1}^{p} B_{\ell} y_{t-\ell} + u_t,\qquad u_t \sim \mathcal{N}(0, \Sigma),
    \label{eq:varest}
\end{equation}
where $y_t$ collects the variables in the information set.
Because adding variables increases parameter uncertainty even when predictive content is present, I regularize the system with Minnesota-style shrinkage and learn the overall tightness from the data using the hierarchical prior-selection approach of \citet{GLP2015}, as implemented in \citet{KuschnigVashold2021}. Importantly, shrinkage is interpreted as statistical regularization of the forecasting system rather than as a proxy for economic frictions.

\subsection{Pseudo out-of-sample evaluation}
I evaluate performance in a recursive pseudo out-of-sample design with expanding estimation windows. At each forecast origin, the system is re-estimated using all data available up to that origin and then produces point forecasts at the horizons reported in the main accuracy table. This recursion mirrors a real-time workflow while remaining descriptive because it uses revised data rather than real-time vintages.

\subsection{Forecast accuracy and nested-model inference}
Forecast accuracy is summarized by RMSFE on the common evaluation scale described in Section~\ref{sec:data}. For target $i$ and horizon $h$,
\[
    \mathrm{RMSFE}_{i,h}=\left(P^{-1}\sum_{t \in \mathcal{T}} (y_{i,t+h}-\hat y_{i,t+h|t})^2\right)^{1/2}.
\]
I report RMSFEs (Table~\ref{tab:rmsfe}) and relative RMSFEs versus a parsimonious benchmark (Figure~\ref{fig:relrmsfe}). Because the information sets are nested, standard equal-accuracy tests can have nonstandard behavior \citep{ClarkMcCracken2001}; I therefore emphasize magnitudes and stability and report a nested-model-robust adjustment as a robustness check \citep{ClarkWest2007} (Appendix Table~\ref{tab:clark_west}).

\subsection{Forecast discipline: revision-based diagnostic}
To assess whether forecast updates are systematically related to subsequent forecast errors, I use the error-on-revision regression framework of \citet{CG2015} applied to model-implied forecasts:
\begin{equation}
    (z_{t,h} - \hat{z}_{t,h|t}^{(m)}) = \alpha_h + \beta_h r_{t,h}^{(m)} + \varepsilon_{t,h},
    \label{eq:cg}
\end{equation}
where $r_{t,h}^{(m)}$ is the revision to the forecast for the same target date made one period apart.
In this paper, the regression is used as a diagnostic of the forecasting system's updating rule: it measures whether revisions are followed by predictable errors, indicating systematic patterns in updating.
Because the forecasts are produced under shrinkage, such patterns can partly reflect prior-induced conservatism, misspecification, or instability rather than an economic mechanism.

% --- Section 4: Interpretation ---
\section{Results}
\label{sec:results}
\ifsubmission
    This section reports the core evidence through two complementary lenses: forecast \emph{accuracy} and forecast \emph{discipline}. Accuracy evaluates whether sentiment improves point forecasts once macro aggregates and financial prices are already in the information set. Discipline evaluates whether the information set changes systematic patterns in forecast updates, using the revision-based diagnostic in Section~\ref{sec:methods}.

    \paragraph{Main takeaways.}
    \begin{itemize}[leftmargin=*]
        \item \textbf{Accuracy: limited marginal value of sentiment conditional on prices.} Adding financial prices can improve point forecasts relative to the macro-only system, but the incremental contribution of sentiment beyond prices is limited (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}).
        \item \textbf{Discipline: information sets can change updating patterns.} The revision diagnostic shows systematic updating patterns, and the information set can shift these patterns even when point accuracy changes little (Table~\ref{tab:cg_regression}).
        \item \textbf{Nested comparisons: interpret tests cautiously.} Because the specifications are nested, I emphasize magnitudes and stability and use nested-model-robust adjustments as a robustness check (Appendix Table~\ref{tab:clark_west}).
    \end{itemize}

    \subsection{Forecast accuracy}
    Table~\ref{tab:rmsfe} reports RMSFEs by information set, and Figure~\ref{fig:relrmsfe} summarizes the same comparison in relative terms versus a parsimonious benchmark.
    \InputGeneratedTableNoNotes{results/latex_tables/tab_rmsfe.tex}
    {\footnotesize\textit{Notes:} The model labels correspond to nested information sets defined in Section~\ref{sec:data}. Small includes macro aggregates; Medium adds a small set of financial prices; Full further adds consumer sentiment. All evaluation-scale and pseudo out-of-sample implementation details follow the audited pipeline summarized in \texttt{INTERNAL\_MAPPING.md}. \par}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig2_relative_rmsfe.png}
        \caption{Relative forecast accuracy versus a parsimonious benchmark}
        \label{fig:relrmsfe}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure reports RMSFEs for each information set relative to a parsimonious benchmark, using the same evaluation scale as Table~\ref{tab:rmsfe}. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

    \subsection{Forecast discipline: revision-based diagnostic}
    Table~\ref{tab:cg_regression} reports error-on-revision coefficients from the \citet{CG2015} diagnostic applied to model-implied forecasts.
    Interpreted as a diagnostic of the forecasting system, the coefficients summarize whether revisions are followed by predictable errors, indicating systematic updating patterns.
    \InputGeneratedTableNoNotes{results/latex_tables/tab_cg_regression.tex}
    {\footnotesize\textit{Notes:} The regression follows \citet{CG2015} but is applied to model-implied forecasts, so coefficients are interpreted as a diagnostic of internal error--revision consistency in a regularized forecasting system, not as structural evidence about belief formation. Sign patterns can reflect regularization, misspecification, or instability, so the paper emphasizes qualitative shifts across information sets rather than a behavioral mechanism. \par}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig3_cg_coefficients.png}
        \caption{Revision diagnostic coefficients across information sets}
        \label{fig:cg_coeff}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: Estimated coefficients from the revision diagnostic. Coefficients summarize internal error--revision consistency in the model-based forecasting system. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

    \subsection{Regularization and model stability}
    Figure~\ref{fig:lambda} reports the time path of the learned shrinkage tightness parameter. The key message is methodological: hierarchical regularization adapts the forecasting system's effective complexity as information sets expand and as the data environment changes, which helps make horse-race comparisons less sensitive to ad hoc tuning choices.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig5_lambda_evolution.png}
        \caption{Evolution of learned shrinkage tightness}
        \label{fig:lambda}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure plots posterior means of $\lambda$ at each recursive forecast origin for each model specification. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

    \subsection{Economic interpretation and limitations}
    Taken together, the results support a single storyline. Financial prices can add incremental predictive content for point forecasts beyond the macro-only information set, while sentiment contributes limited incremental accuracy once prices are included (Table~\ref{tab:rmsfe}). For the revision diagnostic, the evidence indicates systematic updating patterns in model-implied forecasts and suggests that these patterns can shift across information sets (Table~\ref{tab:cg_regression}). Interpreting these shifts requires discipline: the regression is non-structural and the coefficients can be influenced by shrinkage, misspecification, or instability rather than an economic mechanism.


\else
    This section interprets the empirical outputs produced by the forecasting pipeline. All numerical results cited below correspond to the CSV tables in \texttt{results/tables/} and figures in \texttt{results/figures/}.

    \subsection{Forecast accuracy and the role of the information set}

    Table~\ref{tab:rmsfe_main} summarizes forecast accuracy for CPI inflation and industrial production growth across the three information sets, along with two benchmarks. Figure~\ref{fig:rmsfe_bar} visualizes the same RMSFEs, while Figure~\ref{fig:relrmsfe} reports the corresponding relative performance against the no-change benchmark.

    \paragraph{Inflation forecasts: Substantial benchmark improvements; limited incremental gains from expanded information sets.}
    All BVAR specifications decisively outperform the random-walk benchmark at every horizon. At $h=1$, the Medium model achieves the lowest RMSFE (2.982 percentage points), beating the benchmark by 27\% (relative RMSFE 0.735). The Small and Full models perform comparably (3.468 and 3.128, respectively), yielding relative RMSFEs of 0.854 and 0.771. At $h=3$, performance remains strong: RMSFEs range from 2.500 (Medium) to 2.643 (Small), translating to 19--24\% improvements over the benchmark. The most pronounced gains emerge at the twelve-month horizon: all specifications achieve relative RMSFEs below 0.57, representing 43--45\% reductions in forecast error. Notably, the Small model delivers the lowest h=12 RMSFE (1.305 percentage points), marginally outperforming Full (1.330) and Medium (1.349).

    \emph{Interpreting the absence of information-set gains at long horizons.} The Small model's superior long-horizon performance is economically revealing. At $h=12$, inflation dynamics are dominated by low-frequency movements in trend inflation and inflation expectations. The baseline specification---industrial production, CPI, unemployment, and the federal funds rate---already encodes the key determinants of these trends through the Phillips curve (unemployment-inflation linkage) and monetary policy stance (federal funds rate). Adding financial variables (Medium) introduces signals primarily about near-term cyclical pressures, which contribute little to twelve-month inflation forecasting beyond what monetary aggregates already capture. Including sentiment (Full) likewise fails to improve accuracy: while consumer inflation expectations are theoretically relevant for wage/price setting, the Michigan sentiment index appears to provide no marginal information beyond that embedded in realized unemployment and policy rates.

    This null result does \emph{not} imply sentiment is uninformative. Rather, it suggests that, in this design, expanding the information set changes revision dynamics more than it changes point-forecast RMSFE. In the CG diagnostic, moving from Small to Medium markedly reduces the short-horizon underreaction coefficient for inflation, while the additional step from Medium to Full does not further reduce short-horizon underreaction. At the twelve-month horizon, the Full model's coefficient is closer to zero than Medium's, consistent with richer information sets reducing long-horizon overreaction, though these differences are estimated with substantial uncertainty. The distinction between forecast accuracy and forecast \emph{discipline} is therefore central: soft information may alter internal updating patterns even when incremental RMSFE gains are small.

    \emph{Prior calibration and short-horizon performance.} The aggressive shrinkage ($\lambda$ mode 0.05) and accelerated lag decay ($\alpha=3$) jointly discipline short-horizon forecasts. At $h=1$, the prior forces heavy reliance on the random-walk component, guarding against overfitting to transient price shocks. The Medium model's dominance at this horizon (RMSFE 2.982 vs. Small 3.468) reflects financial variables' capacity to capture imminent demand pressures: stock returns and yield spreads encode market expectations about monetary policy and cyclical shifts, which materialize over monthly intervals. Adding sentiment degrades $h=1$ performance relative to Medium (Full RMSFE 3.128), consistent with sentiment containing low-frequency trend information that is less diagnostic of month-to-month fluctuations. At $h=3$, the information-set ranking compresses (Medium 2.500, Full 2.538, Small 2.643), indicating that distinctions among specifications diminish as the forecast horizon extends toward the range where all models rely primarily on prior-induced persistence.

    \paragraph{Industrial production forecasts: Financial variables help at short horizons; long-horizon challenges persist.}
    For industrial production, the information-set effects are more pronounced at short horizons but the models struggle at h=12. At $h=1$, the Medium model delivers the lowest RMSFE (7.315 percentage points), beating Small (7.649) by 4.4\% and outperforming the random-walk benchmark (8.012) by 8.7\% (relative RMSFE 0.913). At $h=3$, Medium's advantage is even larger: RMSFE 4.966 versus Small's 5.558 (11\% improvement) and the benchmark's 5.680 (relative RMSFE 0.874). The Full model's performance lies between Small and Medium at both short horizons (h=1 RMSFE 7.424, h=3 RMSFE 5.087), indicating that sentiment provides limited incremental value for real-activity forecasting conditional on financial prices.

    At the twelve-month horizon, all BVAR specifications \emph{underperform} the random-walk benchmark: relative RMSFEs range from 1.018 (Full) to 1.185 (Small), meaning the models' forecast errors are 2--19\% \emph{larger} than simply projecting zero growth. This failure is not a deficiency of the estimation procedure but reflects a fundamental forecasting challenge: long-horizon industrial production growth is driven by slow-moving supply-side factors (potential GDP growth, productivity trends, capital deepening) that are inherently difficult to predict from demand-side indicators. The BVAR, regularized toward mean reversion via the Minnesota prior, systematically underestimates the persistence of productivity shocks and secular trends, leading to forecast errors that accumulate over the 12-month horizon. Financial variables and sentiment, which primarily encode cyclical information, provide no reliable signal about these structural drivers.

    \emph{Implications for prior calibration.} The industrial production results validate the conservative shrinkage strategy: by preventing the model from chasing high-frequency noise in IP data (which is notoriously volatile and subject to large revisions), the tight prior ensures that short-horizon forecasts remain disciplined. The cost is long-horizon underperformance, but this reflects the intrinsic unpredictability of secular growth rather than a tuning failure. Alternative prior specifications (e.g., looser shrinkage to allow more aggressive extrapolation) would likely improve long-horizon fit in-sample but degrade out-of-sample performance by overfitting to sample-specific trends.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig1_rmsfe_comparison.png}
        \caption{Forecast performance by horizon (RMSFE; lower is better)}
        \label{fig:rmsfe_bar}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: Bars report RMSFEs on the evaluation scale for each BVAR information set and horizon. Values correspond to Table~\ref{tab:rmsfe_main}, Panel A, and are generated from \texttt{results/tables/}\allowbreak\texttt{rmsfe\_results.csv}.
        \end{minipage}
    \end{figure}

    \begin{table}[H]
        \centering
        \begin{threeparttable}
            \caption{Forecast accuracy across information sets}
            \label{tab:rmsfe_main}
            \begin{tabular}{ll S[table-format=1.3] S[table-format=1.3] S[table-format=1.3]}
                \toprule
                                &        & \multicolumn{1}{c}{$h=1$} & \multicolumn{1}{c}{$h=3$} & \multicolumn{1}{c}{$h=12$} \\
                \midrule
                \multicolumn{5}{l}{\textit{Panel A. RMSFE (percentage points, annualized)}}                                   \\
                Small           & CPI    & 3.468                     & 2.643                     & 1.305                      \\
                Medium          & CPI    & 2.982                     & 2.500                     & 1.349                      \\
                Full            & CPI    & 3.128                     & 2.538                     & 1.330                      \\
                \addlinespace
                Small           & INDPRO & 7.649                     & 5.558                     & 4.998                      \\
                Medium          & INDPRO & 7.315                     & 4.966                     & 4.371                      \\
                Full            & INDPRO & 7.424                     & 5.087                     & 4.387                      \\
                \addlinespace
                RW benchmark    & CPI    & 4.057                     & 3.267                     & 2.381                      \\
                AR(1) benchmark & CPI    & 3.226                     & 2.933                     & 1.535                      \\
                RW benchmark    & INDPRO & 8.012                     & 5.680                     & 4.294                      \\
                AR(1) benchmark & INDPRO & 8.117                     & 4.788                     & 6.678                      \\
                \addlinespace
                \multicolumn{5}{l}{\textit{Panel B. Relative RMSFE vs random-walk benchmark}}                                 \\
                Small           & CPI    & 0.854                     & 0.809                     & 0.548                      \\
                Medium          & CPI    & 0.735                     & 0.765                     & 0.567                      \\
                Full            & CPI    & 0.771                     & 0.777                     & 0.559                      \\
                \addlinespace
                Small           & INDPRO & 0.955                     & 0.979                     & 1.164                      \\
                Medium          & INDPRO & 0.913                     & 0.874                     & 1.018                      \\
                Full            & INDPRO & 0.927                     & 0.896                     & 1.022                      \\
                \bottomrule
            \end{tabular}
            \begin{tablenotes}[flushleft]
                \footnotesize
                \item Notes: Panel A reports RMSFEs computed from the expanding-window pseudo out-of-sample forecasts (\texttt{results/tables/}\allowbreak\texttt{rmsfe\_results.csv}) and benchmark RMSFEs (\texttt{results/tables/}\allowbreak\texttt{rw\_rmsfe\_benchmark.csv}, \texttt{results/tables/}\allowbreak\texttt{ar1\_rmsfe\_benchmark.csv}). The no-change benchmark corresponds to a random walk in levels (zero forecast on the cumulative-growth evaluation scale). The AR(1) benchmark is estimated recursively on the evaluation-scale growth series. Panel B reports RMSFEs relative to the no-change benchmark (\texttt{results/tables/}\allowbreak\texttt{relative\_rmsfe\_vs\_rw.csv}).
            \end{tablenotes}
        \end{threeparttable}
    \end{table}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig2_relative_rmsfe.png}
        \caption{Relative RMSFE versus no-change benchmark (random walk)}
        \label{fig:relrmsfe}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure plots RMSFE for each model divided by the RMSFE of the no-change benchmark at each horizon. Values below one indicate improvement over the benchmark. The plotted values correspond to \texttt{results/tables/}\allowbreak\texttt{relative\_rmsfe\_vs\_rw.csv}.
        \end{minipage}
    \end{figure}

    \subsection{Benchmarks, statistical uncertainty, and time variation}
    Because the information sets are nested (Small $\subset$ Medium $\subset$ Full), equal-accuracy tests based on loss differentials can be nonstandard under the null for nested model comparisons \cite{ClarkMcCracken2001}. I therefore interpret Diebold--Mariano tests primarily as descriptive checks and supplement them with Clark--West MSPE-adjusted tests for the nested comparisons \cite{ClarkWest2007} (Appendix Table~\ref{tab:clark_west}). In the main text, the emphasis is on the magnitude and stability of RMSFE differences rather than on sharp statistical dominance across closely related specifications.
    Formal comparisons of predictive accuracy use Diebold--Mariano tests on squared-error loss differentials with Newey--West standard errors. Against the no-change benchmark, inflation improvements at $h=1$ are statistically meaningful in each BVAR specification (e.g., the small model yields $t=-3.12$, $p=0.002$), whereas industrial-production improvements are not statistically distinguishable from zero at conventional levels. Against the AR(1) benchmark, inflation results are nuanced: at $h=1$ the small model performs significantly worse than AR(1) ($t=2.47$, $p=0.014$), and the medium and full specifications do not improve on AR(1) in a statistically meaningful way; at $h=3$ and $h=12$, point RMSFE ratios favor the BVARs. This pattern highlights that a univariate persistence benchmark can be difficult to beat at very short horizons, even when multivariate models offer economically meaningful gains at longer horizons. Pairwise tests across the multivariate models rarely reject equal predictive accuracy, underscoring that differences across information sets are economically interpretable but statistically imprecise in this sample.

    Rolling relative RMSFEs (Figure~\ref{fig:rolling_rw}) highlight time variation once a 60-month rolling window is available. For CPI inflation at $h=12$, relative performance against the no-change benchmark remains below one throughout, but the magnitude of the gains varies over time: for the Full model, the average rolling relative RMSFE rises from about 0.52 before 2013 to about 0.70 thereafter (still an improvement over the benchmark). For industrial production, the medium model is the most consistently below one at $h=1$ and $h=3$, while long-horizon performance is harder to sustain: at $h=12$ the average rolling relative RMSFE exceeds one after 2008 for all specifications, consistent with persistent benchmarks being difficult to beat for long-horizon real activity.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_rw.png}
        \caption{Rolling relative RMSFE versus no-change benchmark}
        \label{fig:rolling_rw}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.96\textwidth}
            \footnotesize
            Notes: Rolling-window relative RMSFEs (window length 60 months). Values below one indicate improvement over the no-change benchmark. The figure is generated from \texttt{results/tables/}\allowbreak\texttt{rolling\_relative\_rmsfe\_vs\_rw.csv}.
        \end{minipage}
    \end{figure}

    \subsection{Forecast-error decomposition and forecast-path diagnostics}
    Theil-type MSE decompositions (Figure~\ref{fig:error_decomp}) clarify what drives forecast errors across horizons. Decompose $\mathrm{MSE}=\mathbb{E}[(y-\hat y)^2]$ into a bias component (mean error), a variance component (dispersion mismatch), and a covariance component (imperfect co-movement), and report each as a share of total MSE. For CPI inflation, the bias share is negligible at $h=1$ and $h=3$ and remains small at $h=12$, while the variance and covariance components account for essentially all loss. Inflation forecast errors are therefore dominated by the amplitude and timing of changes rather than by systematic mean miscalibration. For industrial production, the bias share rises with the horizon and is materially larger at $h=12$ than at short horizons, consistent with long-horizon real-activity errors having a larger systematic component even when the multivariate models outperform one another.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.96\textwidth]{fig_error_decomposition.png}
        \caption{Forecast error decomposition (Theil MSE shares)}
        \label{fig:error_decomp}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.96\textwidth}
            \footnotesize
            Notes: Decomposition of mean squared forecast error into bias, variance, and covariance shares. Values correspond to \texttt{results/tables/}\allowbreak\texttt{error\_decomposition.csv}.
        \end{minipage}
    \end{figure}

    The forecast-path plots in Appendix~\ref{app:figures} provide a complementary view. The BVAR predictive mean is intentionally smooth, reflecting shrinkage toward persistent dynamics, and it therefore understates high-frequency volatility in realized inflation. Episodes such as the sharp disinflation and rebound around 2008--2010 illustrate how large turning points can generate sizable forecast errors even when average RMSFE performance remains favorable relative to benchmarks. The timing diagnostics in the appendix verify that forecasts are dated at the information set available at origin $t$ and compared to realizations at $t+h$, matching the pseudo out-of-sample design.

    \subsection{Forecast revisions and systematic expectation-updating patterns}

    Table~\ref{tab:cg_main} reports estimates of the forecast-error-on-revision regression (Equation~\ref{eq:cg}) for inflation and industrial production across the three information sets. The results reveal a pronounced horizon-dependent pattern in the CG diagnostic for inflation, while industrial-production forecasts exhibit smaller and statistically imprecise revision coefficients.

    \paragraph{Inflation: Underreaction at short horizons, overreaction at long horizons.}
    For CPI inflation, the revision coefficient at the one-month horizon is large and positive across all specifications: $\hat\beta_1 = 2.26$ in the Small model ($SE=1.19$, $p=0.060$), declining to $0.71$ in the Medium model ($SE=0.28$, $p=0.013$), and further to $0.93$ in the Full model ($SE=0.32$, $p=0.004$). Under the \citet{CG2015} interpretation, these positive coefficients indicate \emph{underreaction}: when the BVAR revises its inflation forecast upward at $t$, that revision moves in the correct direction on average but by insufficient magnitude to prevent a subsequent positive forecast error. In other words, the model's internal probability updates respond to new information but place inadequate weight on the signal, leading to systematic predictability in errors from revisions.

    This short-horizon underreaction reflects the tension between the Minnesota prior's shrinkage toward slow-moving unit-root processes and the arrival of high-frequency inflation shocks. The prior, calibrated with $\lambda$ mode at 0.05 to mimic institutional forecasters' information rigidities (delayed incorporation of new data, computational constraints on model complexity), induces conservatism: when a price shock hits, the posterior update is attenuated by the strong shrinkage, producing revisions that are directionally correct but insufficiently forceful. The decline in $\hat\beta_1$ from 2.26 (Small) to 0.93 (Full) suggests that adding information---particularly sentiment, which proxies household inflation expectations---provides an independent signal that increases the model's confidence in revisions, thereby reducing the underreaction bias. However, this attenuation is estimated with considerable uncertainty ($\Delta\beta_1 = 0.93 - 2.26 = -1.34$, $SE=1.24$, $p=0.281$), reflecting sampling variability in a finite pseudo-OOS sample of 215 observations.

    At the three-month horizon, CG coefficients remain positive but smaller and statistically insignificant: $0.69$ (Small, $p=0.387$), $0.56$ (Medium, $p=0.082$), and $0.69$ (Full, $p=0.066$). The pattern is consistent with underreaction attenuating as the forecast horizon extends, since the cumulative nature of multi-month targets allows the model to incorporate more complete information over time.

    At the twelve-month horizon, the sign reverses: CG coefficients are \emph{negative} across all specifications, indicating \emph{overreaction}. The Small model yields $\hat\beta_{12} = -0.52$ ($SE=0.32$, $p=0.109$), strengthening in magnitude to $-0.08$ in Medium ($p=0.684$) and $-0.03$ in Full ($p=0.897$). Negative $\beta_{12}$ means that upward forecast revisions are systematically followed by negative forecast errors, and vice versa---the hallmark of extrapolative forecasting or overfitting to low-frequency trends.

    \paragraph{Economic mechanisms underlying horizon-dependent biases.}
    The sign reversal from positive $\beta$ at $h=1$ to negative $\beta$ at $h=12$ reflects the interplay between prior-induced smoothness and trend persistence in the data. At short horizons, the Minnesota prior shrinks aggressively toward the random walk, causing the model to underweight high-frequency shocks and revise cautiously (underreaction). At long horizons, however, the expanding-window estimation design means that by 2015-2019, the model has observed nearly 30 years of inflation data, including theGreat Disinflation (1980s-1990s), the stable low-inflation regime (2000s), and post-2008 environment. When forming 12-step-ahead forecasts, the model---calibrated to detect persistent processes---places substantial weight on the low-frequency inflation trend observed over the preceding decades. If the model revises its 12-month forecast upward (e.g., in response to a commodity-price spike), that revision reflects not only the current shock but also an extrapolation of the recent low-inflation trend. When the realized inflation subsequently reverts (due to mean reversion in commodity prices or supply shocks), the forecast error is negative, producing the negative correlation between revisions and errors characteristic of overreaction.

    Adding sentiment does \emph{not} eliminate the long-horizon overreaction; in fact, the Full model's $\hat\beta_{12}$ is closer to zero ($-0.03$) than the Small model's ($-0.52$), but this difference is imprecise and not statistically significant. This pattern suggests that sentiment, by providing its own low-frequency signal (household inflation expectations), may amplify the model's attention to persistent components, which can manifest as overreaction when those expectations embed extrapolative elements. Importantly, this is not necessarily a flaw: if sentiment genuinely reflects households' inflation beliefs and those beliefs influence wage/price setting, the model \emph{should} incorporate them even if doing so sometimes leads to forecast errors when those beliefs prove overly pessimistic or optimistic. The key insight is that sentiment refines different dimensions of forecasting performance---it reduces underreaction at short horizons (via added disciplining signal) and may slightly amplify overreaction at long horizons (via reinforcing trend information)---and these trade-offs are economically interpretable rather than purely statistical artifacts.

    \paragraph{Magnitude and uncertainty of changes in short-horizon underreaction.}
    The difference $\Delta\beta_1$ reported in the project outputs compares the Full and Small specifications, i.e., the combined effect of expanding the information set from core macro variables to the full set that includes both financial variables and sentiment. The point estimate is negative, indicating that the short-horizon inflation underreaction coefficient is smaller in the Full specification than in the Small specification, but the estimate is imprecise (the associated standard error is large and the null cannot be rejected at conventional levels). This uncertainty highlights a general limitation of finite pseudo out-of-sample samples: revision-based diagnostics can detect large, stable biases, but are less powerful for isolating incremental changes across closely related specifications. When interpreting incremental effects of sentiment specifically, the Medium vs.\ Full comparison is the most relevant nested step and suggests only modest changes in short-horizon revision behavior.

    \paragraph{Industrial production: Absence of detectable revision biases.}
    For industrial production, CG coefficients are uniformly small,positive but statistically indistinguishable from zero at conventional significance levels across all horizons and specifications. For example, in the Small model at $h=1$, $\hat\beta=0.72$ ($p=0.202$); in the Full model, $\hat\beta=0.11$ ($p=0.756$). At $h=12$, coefficients range from $0.14$ (Small) to $0.22$ (Full), with $p$-values exceeding 0.6. This absence of systematic bias could reflect two mechanisms. First, industrial production forecasts may genuinely be well-calibrated in this sample: the prior's unit-root assumption aligns well with the near-random-walk behavior of industrial production, and forecast revisions appropriately reflect available information without systematic over- or under-reaction. Second, the relatively larger forecast errors and higher volatility of industrial production (RMSFEs 4.4--7.6 percentage points across horizons, compared to 1.3--3.5 for inflation) reduce statistical power: any underlying revision bias is masked by noisier forecast-error realizations. Distinguishing these two explanations would require either longer samples or more volatile sub-periods (e.g., recession-specific analysis), which we defer to future work.

    \begin{table}[H]
        \centering
        \begin{threeparttable}
            \caption{Forecast error on forecast revision (CG regression)}
            \label{tab:cg_main}
            \begin{tabular}{llc S[table-format=-1.3] S[table-format=1.3] S[table-format=-1.2] S[table-format=1.3] c}
                \toprule
                Model  & Target & Horizon & \multicolumn{1}{c}{$\hat\beta_h$} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{$t$} & \multicolumn{1}{c}{$p$} & $N$ \\
                \midrule
                Small  & CPI    & $h=1$   & 2.261                             & 1.194                  & 1.89                    & 0.060                   & 215 \\
                Medium & CPI    & $h=1$   & 0.709                             & 0.284                  & 2.50                    & 0.013                   & 215 \\
                Full   & CPI    & $h=1$   & 0.926                             & 0.319                  & 2.90                    & 0.004                   & 215 \\
                \addlinespace
                Small  & CPI    & $h=3$   & 0.692                             & 0.799                  & 0.87                    & 0.387                   & 215 \\
                Medium & CPI    & $h=3$   & 0.560                             & 0.320                  & 1.75                    & 0.082                   & 215 \\
                Full   & CPI    & $h=3$   & 0.689                             & 0.373                  & 1.85                    & 0.066                   & 215 \\
                \addlinespace
                Small  & CPI    & $h=12$  & -0.518                            & 0.321                  & -1.61                   & 0.109                   & 215 \\
                Medium & CPI    & $h=12$  & -0.084                            & 0.207                  & -0.41                   & 0.684                   & 215 \\
                Full   & CPI    & $h=12$  & -0.027                            & 0.211                  & -0.13                   & 0.897                   & 215 \\
                \addlinespace
                Small  & INDPRO & $h=1$   & 0.718                             & 0.561                  & 1.28                    & 0.202                   & 215 \\
                Medium & INDPRO & $h=1$   & 0.266                             & 0.492                  & 0.54                    & 0.589                   & 215 \\
                Full   & INDPRO & $h=1$   & 0.108                             & 0.347                  & 0.31                    & 0.756                   & 215 \\
                \addlinespace
                Small  & INDPRO & $h=3$   & 0.892                             & 0.482                  & 1.85                    & 0.065                   & 215 \\
                Medium & INDPRO & $h=3$   & 0.598                             & 0.364                  & 1.64                    & 0.101                   & 215 \\
                Full   & INDPRO & $h=3$   & 0.189                             & 0.363                  & 0.52                    & 0.603                   & 215 \\
                \addlinespace
                Small  & INDPRO & $h=12$  & 0.145                             & 0.442                  & 0.33                    & 0.744                   & 215 \\
                Medium & INDPRO & $h=12$  & 0.318                             & 0.529                  & 0.60                    & 0.548                   & 215 \\
                Full   & INDPRO & $h=12$  & 0.224                             & 0.492                  & 0.45                    & 0.650                   & 215 \\
                \bottomrule
            \end{tabular}
            \begin{tablenotes}[flushleft]
                \footnotesize
                \item Notes: Coefficients from regressing forecast errors on forecast revisions, $FE_{t,h} = \alpha_h + \beta_h \times FR_{t,h} + \varepsilon_{t,h}$, where both forecast errors and revisions are constructed on the evaluation scale (annualized cumulative growth rates). Standard errors are Newey--West HAC with lag truncation parameter $h$. Under rational expectations, $\beta_h=0$. Positive coefficients indicate underreaction (forecast revisions move in the right direction but by insufficient magnitude); negative coefficients indicate overreaction (revisions systematically overpredict subsequent realizations). Values correspond to \texttt{results/tables/cg\_regression\_results.csv}. Sample: 215 forecast origins, 2001M1--2019M11.
            \end{tablenotes}
        \end{threeparttable}
    \end{table}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig3_cg_coefficients.png}
        \caption{CG regression coefficients with 95\% confidence intervals}
        \label{fig:cg_coeffs}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure plots $\hat\beta_h$ from Table~\ref{tab:cg_main} with normal-approximation 95\% confidence intervals based on Newey--West standard errors. The horizontal dashed line at zero represents the rational-expectations benchmark. Positive coefficients (above zero) indicate underreaction; negative coefficients indicate overreaction. Source: \texttt{results/tables/cg\_regression\_results.csv}.
        \end{minipage}
    \end{figure}

    To quantify the incremental effect of sentiment on revision patterns, define $\Delta\beta_h = \beta_h^{\textup{Full}} - \beta_h^{\textup{Small}}$. Table~\ref{tab:delta_beta} reports these differences along with standard errors (computed via the variance formula for linear combinations of correlated estimates). Figure~\ref{fig:delta_beta} visualizes the same differences with uncertainty bands.

    For CPI at $h=1$, $\Delta\beta_1 = -1.34$ ($SE=1.24$, $p=0.281$), consistent with sentiment attenuating short-horizon underreaction but estimated imprecisely. At $h=12$, $\Delta\beta_{12} = 0.49$ ($SE=0.38$, $p=0.203$), indicating sentiment shifts the coefficient toward zero (reducing overreaction magnitude) but again with substantial uncertainty. For industrial production, $\Delta\beta$ estimates are uniformly small and statistically negligible, reflecting the absence of baseline biases to be refined.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig4_delta_beta_overreaction.png}
        \caption{Incremental effect of sentiment on CG coefficients: $\Delta\beta_h = \beta_h^{\textup{Full}} - \beta_h^{\textup{Small}}$}
        \label{fig:delta_beta}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure reports $\Delta\beta_h$ with 95\% confidence intervals based on HAC-robust standard errors for the difference. Negative values indicate sentiment reduces $\beta_h$ (e.g., attenuating underreaction at $h=1$); positive values indicate sentiment increases $\beta_h$ (e.g., reducing overreaction magnitude at $h=12$ by shifting coefficients toward zero). The wide confidence bands reflect sampling uncertainty in finite pseudo-OOS samples. Source: \texttt{results/tables/delta\_beta\_overreaction\_test.csv}.
        \end{minipage}
    \end{figure}

    \subsection{Hyperparameter adaptation and data-driven regularization}

    A key virtue of the hierarchical prior approach is that it makes the shrinkage intensity $\lambda$ endogenous to model size, allowing us to observe how the data-generating process adjusts regularization as the information set expands. Table~\ref{tab:lambda_evolution} and Figure~\ref{fig:lambda} document this variation.

    \begin{table}[H]
        \centering
        \begin{threeparttable}
            \caption{Posterior mean of shrinkage parameter $\lambda$ by model and forecast origin (selected origins)}
            \label{tab:lambda_evolution}
            \begin{tabular}{lSSS}
                \toprule
                Period                       & \multicolumn{1}{c}{Small} & \multicolumn{1}{c}{Medium} & \multicolumn{1}{c}{Full} \\
                \midrule
                2001--2005 (average)         & 0.669                     & 0.597                      & 0.368                    \\
                2006--2008 (pre-crisis)      & 0.713                     & 0.643                      & 0.430                    \\
                2008--2010 (Great Recession) & 0.874                     & 0.754                      & 0.489                    \\
                2011--2015 (recovery)        & 0.976                     & 0.787                      & 0.503                    \\
                2016--2019 (late sample)     & 1.135                     & 0.816                      & 0.541                    \\
                Overall average              & 0.881                     & 0.721                      & 0.464                    \\
                \bottomrule
            \end{tabular}
            \begin{tablenotes}[flushleft]
                \footnotesize
                \item Notes: Values are posterior means of $\lambda$ from the hierarchical MCMC, averaged over forecast origins in each subperiod. Source: \texttt{results/forecasts/hyperparameters\_evolution.csv}.
            \end{tablenotes}
        \end{threeparttable}
    \end{table}

    \emph{Interpretation of model-size dependence of shrinkage.} The systematic pattern is striking: as the information set grows from Small (4 variables) to Medium (6 variables) to Full (7 variables), the posterior-mean $\lambda$ declines from 0.881 to 0.721 to 0.464. Because smaller $\lambda$ corresponds to tighter Minnesota shrinkage, this pattern implies that the hierarchical procedure automatically tightens the prior as the model becomes more parameter-rich, mitigating overfitting risk and improving comparability across specifications.

    The time variation is also informative. Posterior-mean $\lambda$ rises during the 2008--2010 crisis period and increases further in the late sample, consistent with the data favoring looser shrinkage when the linear VAR requires additional flexibility (either because volatility is elevated or because fit deteriorates under persistent regime changes). The key point is not a specific ``crisis spike'' date, but that the hierarchical procedure makes regularization time-varying and transparent, rather than fixed by assumption.

    \paragraph{Implications for forecast discipline.} These patterns have practical implications. Using a fixed $\lambda$ would mechanically impose the same tightness across specifications, despite very different parameter counts; hierarchical selection instead adjusts tightness by model size and over time, making comparisons across information sets less sensitive to arbitrary prior choices.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig5_lambda_evolution.png}
        \caption{Evolution of hierarchical tightness parameter $\lambda$ over forecast origins}
        \label{fig:lambda}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure plots posterior means of $\lambda$ at each recursive forecast origin from 2001M1 to 2019M11 for each model specification. $\lambda$ is elevated during the 2008--2010 crisis window and increases further in the late sample. Source: \texttt{results/forecasts/hyperparameters\_evolution.csv}.
        \end{minipage}
    \end{figure}

    \subsection{Robustness}
    Two robustness exercises vary (i) the lag length from $p=12$ to $p=6$ and (ii) the initial training window endpoint from 2000M12 to 1995M12, keeping the rest of the design unchanged. The qualitative implications are stable. The medium specification remains the strongest performer for industrial production at short horizons, and the full specification remains competitive for inflation. The alternative initial-window design yields the lowest twelve-month inflation RMSFE for the full model (1.286). Appendix Table~\ref{tab:robustness} and Appendix Figure~\ref{fig:robustness_rw} summarize these results.
\fi

\ifsubmission
    % (Omitted in submission build; limitations summarized in the conclusion.)
\else
    \section{Limitations and future research directions}

    \subsection{Data and identification boundaries}

    Our analysis uses pseudo out-of-sample forecasts constructed from final-vintage macroeconomic data, not real-time vintages that forecasters would have actually observed. This choice simplifies the analysis and focuses attention on the information content of various data sources, but it sidesteps the practical challenge of nowcasting and data revision that practitioners face. A natural extension would be to re-implement this analysis using FRED-RTDF real-time data, which would reveal whether sentiment's predictive value survives the revision process---i.e., whether sentiment indices themselves are robust to later revision.

    The paper is deliberately descriptive and does not attempt to identify causal relationships between sentiment and macro outcomes. Consumer sentiment and macroeconomic conditions are mutually endogenous: households' sentiment responds to current conditions (employment, inflation expectations, asset prices), and in turn, sentiment-driven changes in consumption and savings affect output and inflation. A structural VAR exercise (estimating causal impulse responses via sign or zero restrictions) is beyond the paper's scope, but it would be a valuable complement to clarify the direction of causality and the quantitative magnitude of sentiment's causal effect.

    \subsection{Model specification and functional form}

    The paper estimates a linear BVAR on all three information sets. Inflation and sentiment may be related through nonlinear channels: for instance, sentiment's predictive content might be stronger during crisis periods (high volatility, low sentiment) than during calm periods. A time-varying parameter VAR (TV-BVAR) or a model with regime-switching could capture this richer dynamic. Similarly, we do not explore whether sentiment is better measured by decomposing the Michigan index into sub-components (current vs. expected conditions) or by combining sentiment with alternative confidence measures (e.g., the Conference Board consumer confidence index).

    The evaluation-scale transformation (cumulative growth over $h$ periods from a fixed origin) is standard for forecast evaluation but may mask phenomena visible at other horizons. For instance, one-period-ahead growth-rate forecasts (as opposed to cumulative $h$-period changes) might reveal different roles for sentiment.

    \subsection{Limitations of the CG diagnostic for model-based forecasts}

    The Coibion-Gorodnichenko regression was originally developed to diagnose biases in survey expectations, where $\beta>0$ can be interpreted as information rigidity or rational inattention by households. When applied to a VAR forecasting model, the interpretation is less direct: $\beta$ measures the model's internal consistency in updating, not a structural behavioral phenomenon. A $\beta=2.4$ coefficient at $h=1$ for the small model means that the model tends to under-weight forecast revisions relative to the magnitude needed to eliminate subsequent errors, but this may reflect not an economic irrationality but a prior specification (the Minnesota prior may be too tight at short horizons) or a genuinely persistent signal that takes time to be incorporated.

    \subsection{Concrete proposals for extension}

    \begin{enumerate}
        \item \textbf{Real-time data and nowcasting.} Repeat the analysis using FRED-RTDF real-time data vintages at forecast origin $t$, incorporating realistic delays and revisions. Assess whether sentiment's predictive value is diminished by data uncertainty.

        \item \textbf{Time-varying and nonlinear structures.} Extend to TV-BVAR or Markov-switching BVAR to test whether sentiment's role varies across regimes (e.g., stronger during crisis periods or high-uncertainty environments).

        \item \textbf{Sentiment decomposition.} Decompose the Michigan sentiment index into its major sub-components (current conditions vs. expectations) and evaluate their independent predictive contributions. Explore whether the expectations sub-component better predicts long-horizon inflation.

        \item \textbf{Multivariate sentiment measures.} Combine the Michigan index with other sentiment indicators (Conference Board, stock market-based measures, news-based indices) and evaluate whether a factor model of sentiment improves predictions.

        \item \textbf{Structural identification.} Estimate sign-restricted VAR IRFs to identify the causal response of inflation and production to a structural sentiment shock, holding constant the responses to other shocks.

        \item \textbf{Comparative evaluation against production models.} Benchmark the BVAR's forecasts against professional forecasts from the Survey of Professional Forecasters (SPF) and other real-world prediction systems to assess practical competitive advantage.
    \end{enumerate}

\fi

\section{Conclusion}
\label{sec:conclusion}

\ifsubmission
    I study whether sentiment adds incremental predictive content in a hierarchical BVAR once standard macro aggregates and financial prices are already included.
    The results suggest a two-part pattern: sentiment has limited incremental value for point-forecast accuracy conditional on prices (Table~\ref{tab:rmsfe}), consistent with information overlap where forward-looking asset prices may already incorporate signals also present in sentiment indices (Section~\ref{sec:mechanism}).
    The revision diagnostic indicates that information sets may shift systematic patterns in forecast updating (Table~\ref{tab:cg_regression}), though these estimates are imprecise.
    When applied to model-implied forecasts from a regularized BVAR, revision-regression coefficients reflect the forecasting system's statistical updating rule under hierarchical shrinkage and can partly reflect prior-induced conservatism, not economic agents' beliefs or information processing.
    Because the specifications are nested and the exercise is descriptive, I emphasize magnitudes and stability over sharp statistical dominance claims and report nested-robust adjustments as robustness checks (Appendix Table~\ref{tab:clark_west}).

    This comparison provides no causal identification: sentiment's limited incremental RMSFE may reflect information aggregation by market participants, model misspecification, or unstable forecasting relationships.
    Future work priorities include real-time data vintages to address revision bias, alternative sentiment measures to test proxy sensitivity, and density forecast evaluation to assess whether sentiment contributes to uncertainty quantification even when point RMSFE gains are limited.
\else
    This paper evaluates the incremental role of consumer sentiment in macro forecasting within a transparent, nested-information-set BVAR horse race, and complements accuracy comparisons with a forecast-revision diagnostic following \cite{CG2015}. The evidence is consistent with a disciplined message: adding financial variables helps some short-horizon forecasts (especially for industrial production), but sentiment adds little incremental improvement in point-forecast RMSFE once financial variables are included; for inflation at long horizons, the baseline macro specification performs best in this sample.

    The revision diagnostic shows that inflation forecasts exhibit short-horizon underreaction and long-horizon coefficients near zero or slightly negative, and that richer information sets move long-horizon coefficients toward the rational-expectations benchmark. Because the models are nested, I treat standard equal-accuracy tests as suggestive and report Clark--West MSPE-adjusted tests for nested comparisons \cite{ClarkWest2007}, emphasizing magnitudes and stability rather than sharp claims about statistical dominance \cite{ClarkMcCracken2001}. The main limitation is interpretational: both RMSFE differences and CG coefficients are descriptive summaries of forecast performance rather than causal effects of sentiment. Future work using real-time vintages, broader sentiment measures, and structural identification would sharpen the economic interpretation.
\fi

\label{LastMainTextPage}

\clearpage
\bibliographystyle{apacite}
\bibliography{ref}

\clearpage
\appendix
\section{Additional figures and robustness}\label{app:figures}

\paragraph{Nested-model forecast accuracy: Clark--West tests.}
Table~\ref{tab:clark_west} reports Clark--West MSPE-adjusted tests for nested model comparisons (Small vs.\ Medium; Medium vs.\ Full). This robustness addresses the nonstandard behavior of standard equal-accuracy tests under nesting.

\InputGeneratedTableNoNotes{results/latex_tables/clark_west_tests.tex}
{\ footnotesize\textit{Notes:} The Clark--West test adjusts mean squared prediction error comparisons to account for nested model structures \citep{ClarkWest2007}. Under the null hypothesis that the smaller (nested) model is adequate, the test statistic has a standard normal distribution. Because specifications are nested and forecasting relationships may be unstable, results are interpreted as suggestive rather than definitive. Source file: see \texttt{INTERNAL\_MAPPING.md}. \par}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_ar1.png}
    \caption{Rolling relative RMSFE versus an autoregressive benchmark}
    \label{fig:rolling_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Rolling-window relative RMSFEs. Lower values indicate smaller forecast errors relative to the benchmark. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_rw.png}
    \caption{Robustness: relative RMSFE versus no-change benchmark}
    \label{fig:robustness_rw}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under alternative implementation choices, benchmarked to the same reference forecast. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_ar1.png}
    \caption{Robustness: relative RMSFE versus an autoregressive benchmark}
    \label{fig:robustness_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under an alternative implementation choice, benchmarked to the autoregressive reference forecast. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6a_forecast_vs_actual_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6b_forecast_vs_actual_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6c_forecast_vs_actual_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{CPI inflation: forecast versus realized (multiple horizons)}
    \label{fig:cpi_forecast_paths}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Each panel plots the model-implied predictive mean and the realized target on the evaluation scale. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6d_timing_diagnostic_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6e_timing_diagnostic_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6f_timing_diagnostic_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Forecast timing diagnostic (multiple horizons)}
    \label{fig:timing_diag}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Realizations are dated at the target date; forecasts are dated at the origin date. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7a_cg_scatter_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7b_cg_scatter_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7c_cg_scatter_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Revision diagnostic scatter (multiple horizons)}
    \label{fig:cg_scatter}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Scatter of forecast errors against forecast revisions for CPI inflation in the baseline design. The fitted line corresponds to the \citet{CG2015} regression. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\end{document}
