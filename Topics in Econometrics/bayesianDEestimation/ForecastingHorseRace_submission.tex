\documentclass[12pt,a4paper]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\setstretch{1.25}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{placeins}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[natbibapa]{apacite}
\usepackage{fancyhdr}

\graphicspath{{results/figures/}}
\sisetup{group-separator = {,}, group-minimum-digits = 4, input-symbols = ()}

\newif\ifsubmission
% Standalone submission source: always compile submission branch.
\submissiontrue

% Helper: input generated tables but suppress their auto-generated notes blocks.
% (We keep all quantitative content sourced from `results/`, but we override notes
% in the manuscript to keep definitions and interpretation aligned with the audited code.)
\newcommand{\InputGeneratedTableNoNotes}[1]{%
    \begingroup%
    \renewenvironment{minipage}[1]{\setbox0\vbox\bgroup}{\egroup}%
    \input{#1}%
    \endgroup%
}

\begin{document}
% Title page (unnumbered)
\hypersetup{pageanchor=false}
\begin{titlepage}
    \centering
    \vspace*{1.5cm}
    {\Large Geneva Graduate Institute (IHEID)\par}
    \vspace{0.4cm}
    {\large Topics in Econometrics\par}
    \vspace{0.2cm}
    {\large Term Paper\par}
    \vspace{1.0cm}

    {\LARGE\bfseries The Incremental Predictive Power of Consumer Sentiment in Macroeconomic Forecasting\par}
    \vspace{0.25cm}
    {\large Evidence from a Hierarchical Bayesian VAR and Forecast-Revision Diagnostics\par}

    \vfill

    {\large Jingle Fu\par}
    \vspace{0.2cm}
    {\large Professor: Marko Mlikota\par}
\end{titlepage}
\hypersetup{pageanchor=true}

% Abstract page (roman numbering allowed before Introduction)
\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{plain}

\begin{abstract}
    Does consumer sentiment add predictive content for inflation and real activity once standard macro aggregates and financial prices are already included?
    I answer this question with a horse race across nested information sets in a hierarchical Bayesian VAR in a recursive pseudo out-of-sample design and a revision-based diagnostic of forecast updating following \citet{CG2015}.
    The point-forecast evidence indicates limited incremental accuracy from adding sentiment once financial prices are included (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}).
    The revision diagnostic shows that information sets can shift systematic updating patterns even when point accuracy changes little (Table~\ref{tab:cg_regression}).
    Throughout, revision-regression coefficients are interpreted as diagnostics of a regularized forecasting system rather than causal evidence about beliefs.
\end{abstract}
\noindent\textit{Keywords:} Bayesian VAR; hierarchical shrinkage; forecasting; consumer sentiment; forecast revisions.\par

\clearpage
% Main text starts here (arabic numbering from Introduction)
\pagenumbering{arabic}
\setcounter{page}{1}

% --- Section 1: Introduction ---
\section{Introduction}

\noindent This paper studies a practical forecasting question: does consumer sentiment add incremental predictive content for inflation and real activity once conventional macro aggregates and financial prices are already included? I organize the answer around two objects that matter to forecasting practice: \emph{forecast accuracy} (how close point forecasts are to realizations) and \emph{forecast discipline} (whether the forecasting system revises in an internally coherent way, rather than exhibiting systematic updating patterns).

\paragraph{Contributions and headline evidence.}
The paper makes two contributions. First, it provides a transparent comparison across nested information sets in a hierarchical BVAR that learns overall shrinkage, allowing dimensional changes without ad hoc tuning \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}. Second, it separates forecast accuracy from forecast-discipline diagnostics: sentiment adds limited incremental accuracy beyond financial prices (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}), while revision diagnostics reveal systematic updating patterns that shift with the information set (Table~\ref{tab:cg_regression}). Because the specifications are nested, equal-accuracy tests are treated as suggestive and nested-robust adjustments are reported in the appendix \citep{ClarkMcCracken2001,ClarkWest2007}. Applied to model-implied forecasts, the \citet{CG2015} regression is a diagnostic for a regularized forecasting system rather than a behavioral claim.

\paragraph{Related literature.}
The revision diagnostic builds on \citet{CG2015} and relates to work on belief distortions and diagnostic expectations that emphasizes systematic underreaction and overreaction patterns in expectations; here, the regression is used as a model diagnostic rather than a structural test \citep{BordaloGennaioliShleifer2018,BordaloGennaioliShleifer2020JEP}. Evidence on whether confidence or sentiment contains incremental forecasting information is mixed once other indicators are included, and real-time evaluations often find limited or unstable gains \citep{CarrollFuhrerWilcox1994,BramLudvigson1998,Ludvigson2004,Croushore2005}. The inflation-forecasting literature emphasizes that parsimonious benchmarks can be difficult to beat and that forecasting relationships shift, motivating cautious interpretation of small differences \citep{AtkesonOhanian2001,StockWatson2007}. Hierarchical BVAR shrinkage provides a disciplined way to compare information sets of different dimensions without ad hoc tuning, which is central to the design here \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}.

\paragraph{Roadmap.}
Section~\ref{sec:data} describes the nested information sets and the evaluation setup.
Section~\ref{sec:methods} presents the forecasting system, the role of hierarchical shrinkage, and the accuracy and revision diagnostics.
Section~\ref{sec:results} reports the evidence, and Section~\ref{sec:conclusion} concludes.

\paragraph{Empirical focus.}
The empirical goal is descriptive: quantify the incremental predictive content of sentiment conditional on macro aggregates and financial prices, and summarize forecast-updating patterns using the revision diagnostic. I avoid structural interpretations of revision-regression coefficients and treat formal comparisons under nesting cautiously. This framing separates incremental information content from any belief-distortion interpretation.
% --- Section 2: Data ---
\section{Data}
\label{sec:data}
This section defines the nested information sets and the evaluation scale used in the forecasting comparison.
The dataset combines macro aggregates, a set of widely used financial prices, and a survey-based measure of consumer sentiment. The information sets are nested to isolate incremental information content: \emph{Small} contains macro aggregates, \emph{Medium} adds financial prices, and \emph{Full} further adds consumer sentiment.
The comparison between Medium and Full therefore targets whether sentiment contributes beyond information already summarized in market prices.

Following standard BVAR practice, the model is estimated in levels or log-levels \citep{Sims1980,GLP2015}. Forecasts are evaluated on a common growth-rate scale constructed from model-implied level forecasts, using the same transformation as the code pipeline. The output-to-manuscript mapping is audited in \texttt{INTERNAL\_MAPPING.md}. This structure isolates the incremental role of sentiment while keeping the evaluation scale consistent across information sets.

% --- Section 3: Econometric Framework ---
\section{Empirical design}
\label{sec:methods}
This section describes the forecasting system, the pseudo out-of-sample design, and the diagnostics used to compare nested information sets under hierarchical shrinkage.

\subsection{Forecasting system and nested information sets}
For each information set, I estimate a reduced-form VAR forecasting system and only change the information set. This isolates the incremental role of forward-looking prices and sentiment within a common estimation and prediction rule. The forecasting system is
\begin{equation}
    y_t = c + \sum_{\ell=1}^{p} B_{\ell} y_{t-\ell} + u_t,\qquad u_t \sim \mathcal{N}(0, \Sigma),
    \label{eq:varest}
\end{equation}
where $y_t$ collects the variables in the information set.
Because adding variables increases parameter uncertainty even when predictive content is present, I regularize the system with Minnesota-style shrinkage and learn the overall tightness from the data using the hierarchical prior-selection approach of \citet{GLP2015}, as implemented in \citet{KuschnigVashold2021}. Importantly, shrinkage is interpreted as statistical regularization of the forecasting system rather than as a proxy for economic frictions.

\subsection{Pseudo out-of-sample evaluation}
I evaluate performance in a recursive pseudo out-of-sample design with expanding estimation windows. At each forecast origin, the system is re-estimated using all data available up to that origin and then produces point forecasts at the horizons reported in the main accuracy table. This recursion mirrors a real-time workflow while remaining descriptive because it uses revised data rather than real-time vintages.

\subsection{Forecast accuracy and nested-model inference}
Forecast accuracy is summarized by RMSFE on the common evaluation scale described in Section~\ref{sec:data}. For target $i$ and horizon $h$,
\[
    \mathrm{RMSFE}_{i,h}=\left(P^{-1}\sum_{t \in \mathcal{T}} (y_{i,t+h}-\hat y_{i,t+h|t})^2\right)^{1/2}.
\]
I report RMSFEs (Table~\ref{tab:rmsfe}) and relative RMSFEs versus a parsimonious benchmark (Figure~\ref{fig:relrmsfe}). Because the information sets are nested, standard equal-accuracy tests can have nonstandard behavior \citep{ClarkMcCracken2001}; I therefore emphasize magnitudes and stability and report a nested-model-robust adjustment as a robustness check \citep{ClarkWest2007} (Appendix Table~\ref{tab:clark_west}).

\subsection{Forecast discipline: revision-based diagnostic}
To assess whether forecast updates are systematically related to subsequent forecast errors, I use the error-on-revision regression framework of \citet{CG2015} applied to model-implied forecasts:
\begin{equation}
    (z_{t,h} - \hat{z}_{t,h|t}^{(m)}) = \alpha_h + \beta_h r_{t,h}^{(m)} + \varepsilon_{t,h},
    \label{eq:cg}
\end{equation}
where $r_{t,h}^{(m)}$ is the revision to the forecast for the same target date made one period apart.
In this paper, the regression is used as a diagnostic of the forecasting system's updating rule: it measures whether revisions are followed by predictable errors, indicating systematic patterns in updating.
Because the forecasts are produced under shrinkage, such patterns can partly reflect prior-induced conservatism, misspecification, or instability rather than an economic mechanism.

\subsection{Implementation map and audit trail}
All quantitative evidence in the paper is drawn from existing outputs under \texttt{results/} and is mapped to manuscript tables and figures in \texttt{INTERNAL\_MAPPING.md}. The mapping also records where the hierarchical shrinkage settings and hyperparameter extraction are defined in code, ensuring that the narrative aligns with the audited pipeline without rerunning estimation.

% --- Section 4: Interpretation ---
\section{Results}
\label{sec:results}
\ifsubmission
    This section synthesizes evidence from forecast accuracy and revision diagnostics, then interprets the patterns and limitations within a signal-extraction framework.
    The accuracy results show that adding financial prices can improve point forecasts relative to a macro-only information set, while adding sentiment on top of prices yields limited incremental gains (Table~\ref{tab:rmsfe}; Figure~\ref{fig:relrmsfe}).
    The revision diagnostic indicates systematic updating patterns that shift with the information set even when point accuracy changes little (Table~\ref{tab:cg_regression}; Figure~\ref{fig:cg_coeff}).
    Because the specifications are nested, these comparisons are read as descriptive magnitudes, with nested-robust checks reported in the appendix (Table~\ref{tab:clark_west}).

\subsection{Forecast accuracy}
    Table~\ref{tab:rmsfe} reports RMSFEs by information set, and Figure~\ref{fig:relrmsfe} summarizes the same comparison in relative terms versus a parsimonious benchmark.
    The comparison between Medium and Full isolates sentiment's incremental contribution conditional on prices, while the comparison between Small and Medium captures the incremental content of financial prices. The evidence indicates limited incremental gains from sentiment once prices are included, consistent with information overlap across forward-looking indicators.
    \InputGeneratedTableNoNotes{results/latex_tables/tab_rmsfe.tex}
    {\footnotesize\textit{Notes:} The model labels correspond to nested information sets defined in Section~\ref{sec:data}. Small includes macro aggregates; Medium adds a small set of financial prices; Full further adds consumer sentiment. All evaluation-scale and pseudo out-of-sample implementation details follow the audited pipeline summarized in \texttt{INTERNAL\_MAPPING.md}. \par}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig2_relative_rmsfe.png}
        \caption{Relative forecast accuracy versus a parsimonious benchmark}
        \label{fig:relrmsfe}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure reports RMSFEs for each information set relative to a parsimonious benchmark, using the same evaluation scale as Table~\ref{tab:rmsfe}. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

\subsection{Forecast discipline: revision-based diagnostic}
    Table~\ref{tab:cg_regression} reports error-on-revision coefficients from the \citet{CG2015} diagnostic applied to model-implied forecasts, and Figure~\ref{fig:cg_coeff} visualizes the same patterns.
    For inflation, the coefficients display horizon-dependent sign patterns consistent with underreaction at shorter horizons and overreaction at longer horizons; for real activity, coefficients are closer to zero and imprecise.
    Interpreted as a diagnostic of the forecasting system, the coefficients summarize whether revisions are followed by predictable errors, indicating systematic updating patterns.
    \InputGeneratedTableNoNotes{results/latex_tables/tab_cg_regression.tex}
    {\footnotesize\textit{Notes:} The regression follows \citet{CG2015} but is applied to model-implied forecasts, so coefficients are interpreted as a diagnostic of internal error--revision consistency in a regularized forecasting system, not as structural evidence about belief formation. Sign patterns can be read as underreaction or overreaction patterns and can reflect regularization, misspecification, or instability, so the paper emphasizes qualitative shifts across information sets rather than a behavioral mechanism. \par}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig3_cg_coefficients.png}
        \caption{Revision diagnostic coefficients across information sets}
        \label{fig:cg_coeff}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: Estimated coefficients from the revision diagnostic. Coefficients summarize internal error--revision consistency in the model-based forecasting system and are interpreted as updating patterns rather than behavioral claims. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

\subsection{Regularization and model stability}
    Figure~\ref{fig:lambda} reports the evolution of the learned shrinkage tightness parameter. The key message is methodological: hierarchical regularization adapts the forecasting system's effective complexity as information sets expand and as the data environment changes, which helps make horse-race comparisons less sensitive to ad hoc tuning choices.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.92\textwidth]{fig5_lambda_evolution.png}
        \caption{Evolution of learned shrinkage tightness}
        \label{fig:lambda}
        \captionsetup{font=small}
        \vspace{-0.2cm}
        \begin{minipage}{0.92\textwidth}
            \footnotesize
            Notes: The figure plots posterior means of $\lambda$ at each recursive forecast origin for each model specification. See \texttt{INTERNAL\_MAPPING.md} for source files.
        \end{minipage}
    \end{figure}

\subsection{Economic interpretation and mechanisms}
\label{sec:mechanism}
    A simple information-aggregation and signal-extraction view helps interpret why sentiment adds limited incremental accuracy once prices are included. Financial prices aggregate dispersed signals and react quickly to news about growth and inflation, so they already embed forward-looking content. Survey sentiment can add information about household plans and expectations, but it is also noisy, subject to framing, and slower-moving. In a linear forecasting system that already includes macro aggregates and prices, the marginal signal from sentiment is therefore small or state dependent, and its incremental contribution to point-forecast accuracy can be limited.

    Long-horizon inflation forecasts are dominated by slow-moving trends and persistence, making parsimonious benchmarks competitive. Hierarchical shrinkage encourages persistence and can dampen short-run updates, so revision diagnostics may display underreaction or overreaction patterns that reflect regularization, misspecification, or structural change rather than beliefs. This mechanism view is interpretive rather than identified: the regression is a diagnostic of the forecasting system, and the evidence is consistent with sentiment affecting the pattern of revisions more than average point accuracy.

\subsection{Limitations}
    The revision regression is a diagnostic tool, and its patterns can arise from shrinkage, misspecification, or structural change. The nested design limits power for incremental comparisons, and alternative sentiment measures, real-time data vintages, or density-forecast evaluation could alter the conclusions. Overall, the evidence supports a single narrative: sentiment adds limited incremental point-forecast gains beyond prices, while revision diagnostics reveal systematic updating patterns that vary with the information set.

\fi
\section{Conclusion}
\label{sec:conclusion}

This paper evaluates whether consumer sentiment adds incremental predictive content in a hierarchical BVAR with nested information sets and a revision-based diagnostic.
The evidence indicates limited incremental point-forecast gains from sentiment once financial prices are included, while revision diagnostics show systematic updating patterns that vary with the information set (Table~\ref{tab:rmsfe}; Table~\ref{tab:cg_regression}).
These findings are descriptive and consistent with information overlap, regularization effects, or changing environments rather than causal claims.
Future work can assess state dependence, alternative sentiment measures, real-time data vintages, and density-forecast evaluation.

\label{LastMainTextPage}

\clearpage
\bibliographystyle{apacite}
\bibliography{ref}

\clearpage
\appendix
\section{Additional figures and robustness}\label{app:figures}

\paragraph{Nested-model forecast accuracy: Clark--West tests.}
Table~\ref{tab:clark_west} reports Clark--West MSPE-adjusted tests for nested model comparisons (Small vs.\ Medium; Medium vs.\ Full). This robustness addresses the nonstandard behavior of standard equal-accuracy tests under nesting.

\InputGeneratedTableNoNotes{results/latex_tables/clark_west_tests.tex}
{\footnotesize\textit{Notes:} The Clark--West test adjusts mean squared prediction error comparisons to account for nested model structures \citep{ClarkWest2007}. Under the null hypothesis that the smaller (nested) model is adequate, the test statistic has a standard normal distribution. Because specifications are nested and forecasting relationships may be unstable, results are interpreted as suggestive rather than definitive. Source file: see \texttt{INTERNAL\_MAPPING.md}. \par}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_ar1.png}
    \caption{Rolling relative RMSFE versus an autoregressive benchmark}
    \label{fig:rolling_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Rolling-window relative RMSFEs. Lower values indicate smaller forecast errors relative to the benchmark. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_rw.png}
    \caption{Robustness: relative RMSFE versus no-change benchmark}
    \label{fig:robustness_rw}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under alternative implementation choices, benchmarked to the same reference forecast. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_ar1.png}
    \caption{Robustness: relative RMSFE versus an autoregressive benchmark}
    \label{fig:robustness_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under an alternative implementation choice, benchmarked to the autoregressive reference forecast. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6a_forecast_vs_actual_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6b_forecast_vs_actual_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6c_forecast_vs_actual_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{CPI inflation: forecast versus realized (multiple horizons)}
    \label{fig:cpi_forecast_paths}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Each panel plots the model-implied predictive mean and the realized target on the evaluation scale. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6d_timing_diagnostic_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6e_timing_diagnostic_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6f_timing_diagnostic_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Forecast timing diagnostic (multiple horizons)}
    \label{fig:timing_diag}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Realizations are dated at the target date; forecasts are dated at the origin date. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7a_cg_scatter_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7b_cg_scatter_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7c_cg_scatter_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Revision diagnostic scatter (multiple horizons)}
    \label{fig:cg_scatter}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Scatter of forecast errors against forecast revisions for CPI inflation in the baseline design. The fitted line corresponds to the \citet{CG2015} regression. See \texttt{INTERNAL\_MAPPING.md} for source files.
    \end{minipage}
\end{figure}

\end{document}
