\documentclass[12pt,a4paper]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\setstretch{1.25}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{placeins}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[natbibapa]{apacite}
\usepackage{fancyhdr}

\graphicspath{{results/figures/}}
\sisetup{group-separator = {,}, group-minimum-digits = 4, input-symbols = ()}
\captionsetup{font=small, labelfont=bf, justification=justified, singlelinecheck=false}

\newif\ifsubmission
\submissiontrue

\newcommand{\TableNotes}[1]{%
    \begingroup%
    \captionsetup{font=footnotesize}%
    \caption*{Notes: #1}%
    \endgroup%
}
\newcommand{\FigNotes}[1]{%
    \begingroup%
    \captionsetup{font=footnotesize}%
    \caption*{Notes: #1}%
    \endgroup%
}
\newcommand{\InputGeneratedTableWithNotes}[2]{%
    \begingroup%
    \renewenvironment{minipage}[1]{\setbox0\vbox\bgroup}{\egroup}%
    \let\oldendtable\endtable%
    \renewcommand{\endtable}{\TableNotes{#2}\oldendtable}%
    \input{#1}%
    \endgroup%
}

\begin{document}
\hypersetup{pageanchor=false}
\begin{titlepage}
    \centering
    \vspace*{1.5cm}
    {\Large Geneva Graduate Institute (IHEID)\par}
    \vspace{0.4cm}
    {\large Topics in Econometrics (EI137)\par}
    \vspace{0.2cm}
    {\large Term Paper\par}
    \vspace{1.0cm}

    {\LARGE\bfseries Forecasting Horse Races and ``Belief Distortions'':\par}
    \vspace{0.25cm}
    {\large Hierarchical Bayesian VAR Study with Sentiment Signals\par}

    \vfill

    {\large Jingle Fu\par}
    \vspace{0.2cm}
    {\large Professor: Marko Mlikota\par}
\end{titlepage}
\hypersetup{pageanchor=true}

\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{plain}

\begin{abstract}
    Does consumer sentiment improve forecasts of inflation and real activity beyond the information in macro aggregates and financial prices?
    I address this question using a hierarchical Bayesian VAR with nested information sets, recursive pseudo out-of-sample forecasting, and a revision-based diagnostic following \citet{CG2015}.
    Point forecasts indicate marginal accuracy gains at best once financial prices are included.
    Revision diagnostics reveal updating patterns that vary with the information set, characterizing internal forecast discipline.
    The estimated coefficients and shrinkage patterns reflect conditional predictive relationships within a regularized system.
\end{abstract}
\noindent\textit{Keywords:} Bayesian VAR; hierarchical shrinkage; forecasting; consumer sentiment; forecast revisions.\par

\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}

How much does consumer sentiment improve forecasts of inflation and output after accounting for standard macro aggregates and financial market prices?
I distinguish forecast accuracy from forecast discipline to separate added information from the model's updating behavior under regularization.

I compare nested information sets within a hierarchical BVAR with data-driven shrinkage. This design accommodates changes in dimensionality
without ad hoc tuning \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}. I evaluate both point-forecast accuracy and forecast discipline. The
evidence offers limited support for sentiment's incremental value once financial prices are included, while revision diagnostics reveal updating patterns
that vary with the information set. Given the nested structure, I emphasize relative forecasting efficiency and stability across horizons; nested-robust
Clark-West adjustments appear in the appendix \citep{ClarkMcCracken2001,ClarkWest2007}. I implement the \citet{CG2015} regression on model-implied
forecasts to diagnose the system's updating rule.

This revision diagnostic builds on \citet{CG2015} and connects to work on expectations updating, including diagnostic-expectations models \citep{BordaloGennaioliShleifer2018,BordaloGennaioliShleifer2020JEP}. Evidence on whether confidence or
sentiment adds incremental forecasting information is mixed once other indicators are included, and real-time evaluations often find gains that are
limited or unstable \citep{CarrollFuhrerWilcox1994,BramLudvigson1998,Ludvigson2004,Croushore2005}. The inflation-forecasting literature emphasizes that
parsimonious benchmarks are difficult to beat and that relationships shift over time, which counsels caution in interpreting small differences
\citep{AtkesonOhanian2001,StockWatson2007}. Hierarchical BVAR shrinkage provides a disciplined way to compare information sets of different sizes without
ad hoc tuning, which is central to the design here \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}.

A signal-extraction framework clarifies sentiment's limited predictive role. Financial prices aggregate dispersed information efficiently and may act
as a sufficient statistic for future macroeconomic fundamentals. Consumer sentiment surveys, by contrast, contain idiosyncratic noise and measurement
error. The analysis tests whether, conditional on efficient price discovery, the marginal signal in noisy survey measures is statistically significant for
point-forecast performance.

\section{Data}
\label{sec:data}

The dataset is monthly and spans 1985M1--2019M12, using revised data from FRED and Yahoo Finance. Variables are chosen to capture the macro state (output, inflation, unemployment), the policy stance (short rate), forward-looking market prices (term yield, equity prices, oil), and survey-based sentiment. The information sets are nested to isolate incremental information content. The `Small' set includes core macro variables: Industrial Production (INDPRO), Consumer Price Index (CPIAUCSL), Unemployment Rate (UNRATE), and the Federal Funds Rate (FEDFUNDS). The `Medium' set adds financial prices: the 10-Year Treasury Yield (GS10), the S\&P 500 Index (SP500), and WTI crude oil prices (DCOILWTICO). The `Full' set adds the University of Michigan Consumer Sentiment Index (UMCSENT). The Medium-to-Full comparison therefore tests whether sentiment contributes beyond information already summarized in market prices.

Figure~\ref{fig:key_series} plots three core series to illustrate their co-movement prior to the BVAR analysis.
Table~\ref{tab:summary_stats} presents summary statistics for the key variables.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig0_key_series.png}
    \caption{Key series (standardized): CPI, S\&P 500, and consumer sentiment}
    \label{fig:key_series}
    \FigNotes{Each series is standardized to mean zero and unit variance over the estimation sample to emphasize co-movement and regime shifts.}
\end{figure}

\InputGeneratedTableWithNotes{results/latex_tables/tab_summary_stats.tex}{Summary statistics computed from the estimation sample in the full information set. The correlation column reports pairwise correlations with UMCSENT.}

Following standard BVAR practice, the model is estimated in levels or log-levels \citep{GLP2015}. Forecasts are evaluated on a common growth-rate scale constructed from model-implied level forecasts, using the same transformation throughout the forecasting system. This structure isolates the incremental role of sentiment while keeping the evaluation scale consistent across information sets.
For log-level series (CPIAUCSL and INDPRO), the evaluation scale is the cumulative annualized growth rate from the forecast origin,
\[
    g_{t,h}=\frac{1200}{h}\left(\ell_{t+h}-\ell_{t}\right), \qquad h\in\{1,3,12\},
\]
yielding annualized rates of $1200(\ell_{t+1}-\ell_t)$ for $h=1$, $400(\ell_{t+3}-\ell_t)$ for $h=3$, and $100(\ell_{t+12}-\ell_t)$ for $h=12$; levels are handled as cumulative changes per period.

\section{Empirical design}
\label{sec:methods}

\subsection{Forecasting system and notation}\label{sec:forecasting_system}
I estimate the same reduced-form VAR specification for each information set, varying only the composition of $y_t$.\footnote{All quantitative evidence is generated by the hierarchical BVAR system described in this section; replication materials are summarized in the appendix.} Let $y_t$ denote the $n \times 1$ vector of endogenous variables observed at time $t$. For the Small information set, $y_t$ stacks the macro variables (INDPRO, CPIAUCSL, UNRATE, FEDFUNDS). For the Medium set, $y_t$ augments the macro block with financial prices (GS10, SP500, DCOILWTICO). For the Full set, $y_t$ further adds UMCSENT. The order-$p$ reduced-form VAR is
\begin{equation}
    y_t = c + \sum_{\ell=1}^{p} B_{\ell} y_{t-\ell} + u_t,\qquad u_t \sim \mathcal{N}(0, \Sigma),
    \label{eq:varest}
\end{equation}
where $c$ is an $n \times 1$ intercept, $B_{\ell}$ are $n \times n$ coefficient matrices, and $\Sigma$ is the reduced-form covariance matrix. In the empirical implementation, $p=12$ to match the monthly data frequency.

Stacking observations over $t=1,\dots,T$, define the regressor matrix $X$ with an intercept and lagged $y_t$ terms and the coefficient matrix $\Phi = [c, B_1, \dots, B_p]'$. The reduced-form system can be written as
\begin{equation}
    Y = X\Phi + U,\qquad \mathrm{vec}(U) \sim \mathcal{N}(0, I_T \otimes \Sigma),
    \label{eq:var_stack}
\end{equation}
implying a Gaussian likelihood for $(\Phi,\Sigma)$ with joint estimation of all equations.

\subsection{Hierarchical Minnesota prior}\label{sec:prior}
The BVAR is regularized via a Minnesota prior that shrinks the system toward a parsimonious, univariate benchmark.
For persistent level variables, the prior mean on the first own lag is set to one and all other coefficients are centered at zero;
for stationary variables, all lag coefficients are centered at zero. The prior on the coefficient matrix takes the standard conjugate form
\begin{equation}
    \mathrm{vec}(\Phi)\mid \Sigma,\lambda \sim \mathcal{N}\!\left(\mathrm{vec}(\Phi_0),\ \Sigma \otimes \Omega(\lambda)\right),
    \label{eq:mn_vec}
\end{equation}
where $\Phi_0$ encodes the prior means and $\Omega(\lambda)$ is a diagonal tightness matrix.
For the coefficient on variable $j$ at lag $\ell$ in equation $i$, the Minnesota prior variance is
\begin{equation}
    \Omega_{ij,\ell}(\lambda) = \left(\frac{\lambda^2}{\ell^{2\alpha}}\right)\left(\frac{\sigma_i^2}{\sigma_j^2}\right)\psi_{ij},
    \label{eq:mn_var}
\end{equation}
with $\sigma_i^2$ denoting a scale estimate for equation $i$, $\alpha$ governing lag decay,
and $\psi_{ij}$ applying additional shrinkage to cross-variable lags (normalized to one for own lags in the standard Minnesota design).
This structure delivers stronger shrinkage on long lags and cross-variable effects while preserving flexibility for own-lag persistence.

Two additional priors stabilize the system.
A sum-of-coefficients prior imposes near-unit-root dynamics in levels,
while a dummy-initial-observation prior anchors the system to initial conditions \citep{GLP2015,BanburaGLP2010,KuschnigVashold2021}.
These components are implemented alongside the Minnesota prior and are treated as part of the regularization mechanism that stabilizes the system.

The hierarchical approach learns the appropriate degree of shrinkage from the data rather than relying on pre-tuned hyperparameters.
The global tightness $\lambda$ and lag-decay $\alpha$ are assigned Gamma hyperpriors with bounded support, consistent with the BVAR implementation.
For example,
\begin{equation}
    \lambda \sim \mathrm{Gamma}(a_\lambda,b_\lambda)\ \mathbb{I}(\lambda_{\min}\leq \lambda \leq \lambda_{\max}),
    \label{eq:lambda_hyper}
\end{equation}
with analogous structure for $\alpha$. The hyperparameters are updated via Metropolis--Hastings steps within the posterior sampler,
so shrinkage adapts to the data rather than being fixed ex ante.

\begin{table}[H]
    \centering
    \caption{Implementation settings and hyperparameter values}
    \label{tab:implementation}
    \small
    \begin{tabular}{lcl}
        \toprule
        Parameter                       & Symbol             & Specification                                                                        \\
        \midrule
        MCMC draws                      & $S$                & $S = 10,\!000$ (post-burn-in)                                                        \\
        Burn-in period                  & $B$                & $B = 5,\!000$                                                                        \\
        Minnesota tightness             & $\lambda$          & $\lambda \sim \text{Gamma}(\text{mode}=0.05, \text{sd}=0.2)$; support $[0.001, 2.0]$ \\
        Lag decay                       & $\alpha$           & $\alpha \sim \text{Gamma}(\text{mode}=3.0, \text{sd}=0.25)$; support $[1, 3]$        \\
        Cross-variable shrinkage        & $\psi$             & Automatic determination (BVAR package default)                                       \\
        Sum-of-coefficients prior       & $\mu_{\text{SOC}}$ & $\text{Gamma}(\text{mode}=1, \text{sd}=1)$; support $[0.01, 50]$                     \\
        Dummy-initial-observation prior & $\mu_{\text{DIO}}$ & $\text{Gamma}(\text{mode}=1, \text{sd}=1)$; support $[0.01, 50]$                     \\
        Hyperparameter sampler          &                    & Metropolis--Hastings within Gibbs sampler                                            \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Marginal likelihood and hierarchical selection}\label{sec:ml}
Following Giannone--Lenza--Primiceri, I treat $\lambda$ as a hyperparameter selected by the data through the marginal data density (MDD),
\[
    p(Y\mid \lambda) = \int p(Y\mid \Phi,\Sigma)\,p(\Phi,\Sigma\mid \lambda)\,d\Phi\,d\Sigma,
\]
and use the \texttt{BVAR} package implementation to obtain $\hat\lambda$ (and other prior hyperparameters) via hierarchical prior selection.
Conjugacy implies that, conditional on $\lambda$, the posterior is Normal--Inverse-Wishart,
which yields closed-form posterior moments and a tractable posterior predictive distribution for $y_{t+h}$.

We first give $\lambda$ a Gamma hyperprior, $\lambda \sim \mathcal{G}(a,b)$, and based on data $Y$, we compute the best posterior mode $\hat\lambda$ by maximizing $p(Y\mid \lambda)p(\lambda)$ over a grid of $\lambda$ values.
Then, conditional on $\hat\lambda$, we obtain the posterior of $(\Phi,\Sigma)$ and the predictive distribution of $y_{t+h}$.

\subsection{Convergence and diagnostics}
I assess convergence through visual inspection of hyperparameter trace behavior and by verifying stability of posterior mean shrinkage across origins,
as summarized in Figure~\ref{fig:lambda}. The hierarchical prior mitigates overfitting as dimensionality changes,
and the evidence is organized around trace stability and the evolution of posterior mean shrinkage;
formal effective-sample-size or Geweke diagnostics are reserved for supplementary robustness work.

\subsection{Pseudo out-of-sample evaluation}
I evaluate performance using recursive pseudo out-of-sample forecasts with expanding windows.
The initial window runs from 1985M1 to 2000M12, so the first forecast origin is 2001M1.
Origins advance monthly through 2019M11, yielding one-step evaluation targets through 2019M12;
longer-horizon RMSFEs are computed over the available non-missing targets implied by this alignment.
At each origin, the system is re-estimated with data available up to that date and then produces point forecasts at the horizons reported in the main accuracy table.
This recursion mirrors a real-time workflow and evaluates performance using revised data,
offering evidence on forecasting efficiency under final-vintage information.

\subsection{Forecast accuracy and nested-model inference}
I summarize forecast accuracy by RMSFE on the common evaluation scale described in Section~\ref{sec:data}. For target $i$ and horizon $h$,
\[
    \mathrm{RMSFE}_{i,h}=\left(P^{-1}\sum_{t \in \mathcal{T}} (y_{i,t+h}-\hat y_{i,t+h|t})^2\right)^{1/2}.
\]
Given nesting, I emphasize magnitudes and stability and report a nested-model-robust Clark--West adjustment (Appendix Table~\ref{tab:clark_west}).

\subsection{Forecast discipline: revision-based diagnostic}
I assess the systematic relationship between forecast updates and subsequent errors using the error-on-revision regression of \citet{CG2015} applied to model-implied forecasts:
\begin{equation}
    (z_{t,h} - \hat{z}_{t,h|t}^{(m)}) = \alpha_h + \beta_h r_{t,h}^{(m)} + \varepsilon_{t,h},
    \label{eq:cg}
\end{equation}
where $r_{t,h}^{(m)}$ is the revision to the forecast for the same target date made one period apart.
In this paper, the regression serves as a diagnostic of the forecasting system's updating rule:
it measures whether revisions are followed by predictable errors, indicating systematic patterns in updating.
Because the forecasts are produced under shrinkage, such patterns reflect prior-induced conservatism, misspecification,
or instability as components of the updating mechanism.

\section{Results}
\label{sec:results}

\subsection{Forecast accuracy}
Table~\ref{tab:rmsfe} presents RMSFEs by information set,
and Figure~\ref{fig:relrmsfe} summarizes the comparison in relative terms versus an AR(1) benchmark on the evaluation scale.
The Medium-to-Full comparison isolates sentiment's incremental value conditional on financial prices,
while the Small-to-Medium comparison captures the incremental content of financial prices.
Adding sentiment to a price-augmented model yields statistically insignificant gains that are unstable across horizons.
This is consistent with substantial information overlap between sentiment and forward-looking market prices.

Price-based indicators convey cleaner signals about latent fundamentals than survey sentiment.
Once prices are included, the remaining independent signal in sentiment is
small relative to measurement noise, which explains why RMSFE gains are modest even when sentiment correlates with the target.
In the hierarchical BVAR, this overlap manifests as posterior shrinkage of
sentiment coefficients toward zero when the data provide little support for extra explanatory power.

Redundant predictors increase dimensionality without improving accuracy,
raise estimation variance, and necessitate stronger regularization to maintain
stability. The hierarchical prior addresses this by tightening as the information set expands and by regularizing more aggressively when incremental signal is weak.

The improvement from the Small to the Medium model likely reflects information aggregation:
financial prices embed forward-looking signals that macro aggregates alone do not fully capture.
When adding prices improves accuracy, the pattern is consistent with price discovery adding marginal predictive content in a linear system,
though the size and stability of the gains differ across targets and horizons.

The Medium-to-Full comparison is a signal-extraction exercise. If sentiment surveys are noisy or collinear with price-based signals,
their marginal contribution to point-forecast accuracy can be small even when sentiment contains information about future fundamentals.

\InputGeneratedTableWithNotes{results/latex_tables/tab_rmsfe.tex}{RMSFEs are computed from recursive pseudo out-of-sample forecasts on the common evaluation scale described in Section~\ref{sec:data}. Information sets are nested, so interpretation emphasizes incremental information content in a regularized forecasting system.}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig2_relative_rmsfe.png}
    \caption{Relative forecast accuracy versus AR(1) (no-change) benchmark}
    \label{fig:relrmsfe}
    \FigNotes{Relative RMSFEs versus AR(1) benchmark in growth rates (no-change forecast equals zero on the evaluation scale).}
\end{figure}

\subsection{Forecast discipline: revision-based diagnostic}
Table~\ref{tab:cg_regression} presents error-on-revision coefficients from the \citet{CG2015} diagnostic applied to model-implied forecasts, and Figure~\ref{fig:cg_coeff} visualizes the same patterns.
For inflation, the coefficients change sign across horizons; for real activity, estimates are imprecise and the revision--error association is unstable across horizons.
Within this diagnostic, $\beta_h=0$ serves as the efficient-updating benchmark, while statistically detectable deviations indicate that revisions are predictably related to subsequent errors, consistent with underreaction when $\beta_h>0$ and overreaction when $\beta_h<0$. This evidence characterizes how the forecasting system uses its own history in updating.

A positive $\beta_h$ indicates conservative updating: revisions incorporate new information only partially, so errors are predictable in the direction of the revision. In a hierarchical BVAR, tighter overall shrinkage pulls coefficients toward persistence and can induce such inertia, especially when the data provide limited independent signal. Conversely, negative $\beta_h$ can arise if the system overreacts to transient signals in the information set. The variation in signs and magnitudes across information sets is therefore consistent with the interaction between data-driven shrinkage and the informational redundancy of the predictors.

The diagnostic characterizes the internal consistency of the updating rule and summarizes how revisions align with subsequent errors. Because forecasts are produced under hierarchical shrinkage, the revision--error association can reflect conservative updating, model misspecification, or shifting data relationships. The analysis therefore emphasizes how the patterns move with the information set and the regularization environment.

\InputGeneratedTableWithNotes{results/latex_tables/tab_cg_regression.tex}{Error-on-revision regression following \citet{CG2015} applied to model-implied forecasts. Within this diagnostic, $\beta_h=0$ is the efficient-updating benchmark; departures from zero imply underreaction or overreaction in the model's updating rule.}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig3_cg_coefficients.png}
    \caption{Revision diagnostic coefficients across information sets}
    \label{fig:cg_coeff}
    \FigNotes{Revision diagnostic coefficients from model-implied forecasts; interpreted as internal updating patterns in the regularized system.}
\end{figure}

\subsection{Regularization and model stability}
Figure~\ref{fig:lambda} presents the evolution of the posterior mean of the hierarchical shrinkage tightness parameter. The key message is methodological: hierarchical regularization adapts the forecasting system's effective complexity as information sets expand and as the data environment changes, which helps make horse-race comparisons less sensitive to ad hoc tuning choices.

The tightness parameter is a statistical object governing the strength of prior shrinkage. Changes in its posterior mean reflect how strongly the data support deviations from the prior and keep the interpretation of regularization anchored in statistical discipline.

By letting the prior adapt to information set size and data fit, hierarchical shrinkage improves comparability across models. It limits the risk that larger information sets appear to perform well simply because they overfit in-sample variation, which is especially important in nested comparisons.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig5_lambda_evolution.png}
    \caption{Evolution of hierarchical shrinkage tightness}
    \label{fig:lambda}
    \FigNotes{Posterior mean of hierarchical shrinkage tightness over recursive forecast origins; reflects statistical regularization in the forecasting system.}
\end{figure}

\subsection{Economic interpretation and mechanisms}
\label{sec:mechanism}
Sentiment's limited contribution aligns with a signal-extraction framework:
financial prices appear to aggregate dispersed information efficiently,
leaving sentiment with little independent signal.
If financial prices efficiently aggregate dispersed information,
they can act as a sufficient statistic for forward-looking fundamentals within a linear forecasting system.
By contrast, survey sentiment reflects dispersed signals filtered through household information-processing costs and measurement error.
Conditional on prices, the marginal signal in sentiment is therefore difficult to distinguish from noise in point-forecast performance.

Consider a latent state $s_t$ driving the macro target $y_t$, with prices $p_t$ and sentiment $m_t$ as noisy measurements:
\[
    y_t = s_t + \varepsilon_t, \qquad p_t = s_t + \nu_t, \qquad m_t = s_t + \eta_t.
\]
The critical quantity is each observable's signal-to-noise ratio (SNR).
If price-based indicators have a higher SNR than sentiment,
conditioning on $p_t$ captures most of the forecast-relevant variation in $s_t$.
The optimal linear weight on $m_t$ given $p_t$ is then small,
and the residual contribution of sentiment largely reflects measurement noise.

This signal-extraction logic carries over to the hierarchical BVAR.
When the data show that $m_t$ provides little incremental explanatory power once $p_t$ is included,
the posterior for sentiment coefficients concentrates near zero,
with Minnesota prior cross-variable shrinkage reinforcing that result.
Forecasts can therefore show little improvement in RMSFE from adding sentiment even when
sentiment is correlated with the target, because prices already subsume that information.

This interpretation is conditional on the information set and linear specification used here.
Alternative financial variables, nonlinear dynamics, or distributional forecasting objectives could change the
marginal value of sentiment, so the results should be read as model- and sample-specific.
Revision diagnostics can still vary with the information set even when average point accuracy changes little,
because revisions reflect the system's updating rule and the marginal predictive content of new signals.

Long-horizon inflation forecasts are dominated by slow-moving trends and persistence, making parsimonious benchmarks competitive.
Hierarchical shrinkage encourages persistence and can dampen short-run updates,
so revision diagnostics may display systematic updating patterns that reflect regularization, misspecification, or structural change.
This mechanism view is interpretive and anchored in the forecasting system:
the regression is a diagnostic of the updating rule,
and the evidence is consistent with sentiment affecting the pattern of revisions more than average point accuracy.

\subsection{Limitations}
Revision patterns may reflect shrinkage, misspecification, or structural change.
The nested design limits power for incremental comparisons, and alternative sentiment measures, real-time data vintages,
or density-forecast evaluation could alter the conclusions.
Overall, the evidence indicates limited and unstable incremental support for sentiment in point-forecast accuracy and revision diagnostics that vary with the information set,
framed as conditional evidence on relative forecasting efficiency.

Evaluation is performed on revised data,
with real-time vintage differences left for future work.
In operational forecasting, macro data arrive with publication lags and are subject to revisions,
which can change the relative informational value of survey-based measures.
In such settings, sentiment indicators may become more useful as timely signals,
so the relative ranking across information sets may differ from the revised-data evidence reported here.

\section{Conclusion}
\label{sec:conclusion}

My central finding establishes a clear hierarchy for linear point forecasting:
financial prices capture the bulk of forward-looking information beyond core macro aggregates.
Consumer sentiment, by contrast, provides only marginal and unstable incremental gains conditional on financial prices.
This pattern is consistent with a signal-extraction view and with posterior shrinkage of redundant predictors,
and it persists under nested-robust checks in Appendix Table~\ref{tab:clark_west},
which reinforce disciplined interpretation of small differences.

Revision diagnostics complement this finding.
When $\beta_h$ departs from zero, the system's revisions are predictably related to subsequent errors,
indicating conservative updating or overreaction relative to an efficient-updating benchmark within the model.
This frames the evidence in terms of the forecasting rule and its updating mechanism.

These results suggest positioning sentiment surveys as contextual indicators and as inputs for monitoring forecast revisions in price-augmented linear systems,
framed as forecasting guidance.

Future research should explore state dependence, alternative sentiment measures, real-time data vintages, and density-forecast evaluation.
These extensions would clarify whether the limited incremental role of sentiment for point forecasts is robust to different data environments and forecasting objectives.

\label{LastMainTextPage}

\clearpage
\bibliographystyle{apacite}
\bibliography{ref}

\clearpage
\appendix
\section{Data definitions}\label{app:data_definitions}

\begin{table}[H]
    \centering
    \small
    \caption{Information sets and data definitions}
    \label{tab:data_definitions}
    \begin{tabular}{@{}ll>{\raggedright\arraybackslash}p{3.2cm}>{\raggedright\arraybackslash}p{3.2cm}l@{}}
        \toprule
        Set    & Variable   & Source                      & Transformation & Frequency \\
        \midrule
        Small  & INDPRO     & FRED                        & log level      & Monthly   \\
        Small  & CPIAUCSL   & FRED                        & log level      & Monthly   \\
        Small  & UNRATE     & FRED                        & level          & Monthly   \\
        Small  & FEDFUNDS   & FRED                        & level          & Monthly   \\
        Medium & GS10       & FRED                        & level          & Monthly   \\
        Medium & SP500      & Yahoo Finance               & log level      & Monthly   \\
        Medium & DCOILWTICO & FRED                        & log level      & Monthly   \\
        Full   & UMCSENT    & FRED (U.\ Michigan Surveys) & level          & Monthly   \\
        \bottomrule
    \end{tabular}
    \TableNotes{Sample period 1985M1--2019M12 for all series. Transformations refer to the level used in estimation; evaluation uses a common growth-rate scale as described in Section~\ref{sec:data}. Sources: FRED (Federal Reserve Bank of St.\ Louis) for macro and financial series, Yahoo Finance for the S\&P 500 index.}
\end{table}

\section{Additional figures and robustness}\label{app:figures}

\paragraph{Nested-model forecast accuracy: Clark--West tests.}
Table~\ref{tab:clark_west} reports Clark--West MSPE-adjusted tests for nested model comparisons (Small vs.\ Medium; Medium vs.\ Full). This robustness addresses the nonstandard behavior of standard equal-accuracy tests under nesting.

\input{results/latex_tables/clark_west_tests.tex}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_ar1.png}
    \caption{Rolling relative RMSFE versus an AR(1) benchmark}
    \label{fig:rolling_ar1}
    \FigNotes{Rolling relative RMSFEs versus an AR(1) benchmark estimated on the evaluation-scale growth rates. .}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_rw.png}
    \caption{Robustness: relative RMSFE versus no-change benchmark}
    \label{fig:robustness_rw}
    \FigNotes{Relative RMSFEs under alternative implementation choices versus a no-change (random-walk) benchmark on the evaluation scale. .}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_ar1.png}
    \caption{Robustness: relative RMSFE versus an AR(1) benchmark}
    \label{fig:robustness_ar1}
    \FigNotes{Relative RMSFEs under an alternative implementation choice versus an AR(1) benchmark estimated on the evaluation-scale growth rates. .}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig6a_forecast_vs_actual_h1.png}
%         \caption{Short horizon}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig6b_forecast_vs_actual_h3.png}
%         \caption{Intermediate horizon}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig6c_forecast_vs_actual_h12.png}
%         \caption{Long horizon}
%     \end{subfigure}
%     \caption{CPI inflation: forecast versus realized (multiple horizons)}
%     \label{fig:cpi_forecast_paths}
%     \FigNotes{Each panel plots the model-implied predictive mean and the realized target on the evaluation scale. .}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6d_timing_diagnostic_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6e_timing_diagnostic_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig6f_timing_diagnostic_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Forecast timing diagnostic (multiple horizons)}
    \label{fig:timing_diag}
    \FigNotes{Realizations are dated at the target date and forecasts are dated at the origin date. .}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7a_cg_scatter_h1.png}
        \caption{Short horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7b_cg_scatter_h3.png}
        \caption{Intermediate horizon}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig7c_cg_scatter_h12.png}
        \caption{Long horizon}
    \end{subfigure}
    \caption{Revision diagnostic scatter (multiple horizons)}
    \label{fig:cg_scatter}
    \FigNotes{Scatter of forecast errors against forecast revisions for CPI inflation; the fitted line corresponds to the \citet{CG2015} diagnostic. .}
\end{figure}

\end{document}
