\documentclass[12pt,a4paper]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\setstretch{1.25}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[round]{natbib}
\usepackage{fancyhdr}

\graphicspath{{results/figures/}}
\sisetup{group-separator = {,}, group-minimum-digits = 4, input-symbols = ()}

\pagestyle{empty}

\begin{document}
\hypersetup{pageanchor=false}

\title{\Large The Incremental Predictive Power of Consumer Sentiment in Macroeconomic Forecasting:\\
    Evidence from a Hierarchical Bayesian VAR and Forecast-Revision Diagnostics}
\author{Jingle Fu}
\date{\today}
\maketitle

\thispagestyle{empty}

\begin{abstract}
    We investigate whether consumer sentiment improves macroeconomic forecasts once one already conditions on standard macro aggregates, forward-looking financial prices, and commodity prices;and whether incorporating sentiment reduces systematic biases in the model's internal expectation-updating mechanism. Using monthly U.S.\ data (1985M1--2019M12) and an expanding-window pseudo out-of-sample design with forecast origins from 2001M1 to 2019M11, we estimate a hierarchical Bayesian VAR (BVAR) under three nested information sets: \emph{Small} (baseline: industrial production, CPI, unemployment, federal funds rate), \emph{Medium} (adding 10-year Treasury yield, S\&P 500, oil prices), and \emph{Full} (adding consumer sentiment). The hierarchical prior, calibrated to reflect realistic information-processing rigidities in institutional forecasting environments, endogenizes the shrinkage parameter $\lambda$ via marginal likelihood optimization and imposes accelerated lag decay ($\alpha=3.0$) to capture potential trend-chasing behavior. For inflation, all BVAR specifications substantially improve on the random-walk benchmark: at the twelve-month horizon, RMSFEs are 1.30 (Small), 1.35 (Medium), and 1.33 (Full) percentage points, versus 2.32 for the benchmark---gains of 44--49\%. For industrial production, models beat the benchmark at short horizons but underperform at $h=12$. The Coibion-Gorodnichenko (CG) forecast-revision diagnostic reveals pronounced horizon-dependent biases for inflation: \emph{underreaction} at $h=1$ ($\hat\beta=2.26$ in Small, $p=0.060$; declining to $0.93$ in Full, $p<0.01$) and \emph{overreaction} at $h=12$ ($\hat\beta=-0.52$ in Small, $p=0.109$; strengthening to $-0.03$ in Full). Adding sentiment reduces short-horizon underreaction by approximately 1.33 points ($\Delta\beta=-1.34$, $p=0.281$), consistent with sentiment providing an Independent signal that disciplines revision magnitude, though the reduction is estimated imprecisely in finite samples. The shrinkage parameter systematically adapts to model complexity (mean $\lambda$ declines from $\sim$0.85 in Small to $\sim$0.51 in Full) and spikes during the 2008-2009 crisis (peak $\lambda>1.6$ in January 2019), demonstrating automatic regime learning. These results indicate that soft information can refine forecasting models not only via improved point prediction but also through more calibrated probability updating, particularly at short horizons where information rigidities are empirically pronounced.
\end{abstract}
\noindent\textit{Keywords:} Bayesian VAR; hierarchical shrinkage; forecasting; consumer sentiment; forecast revisions.\\
\noindent\textit{JEL codes:} C11; C53; E37.

\clearpage
\hypersetup{pageanchor=true}
\pagestyle{plain}
\pagenumbering{arabic}

% --- Section 1: Research Question and Motivation ---
\section{Research question and motivation}

Building an effective macroeconomic forecasting model requires balancing two fundamental tensions. First, incorporating additional information can in principle improve predictions by capturing forward-looking signals, but in finite samples it increases parameter uncertainty and can worsen forecast performance unless regularization is sufficiently aggressive. Second, even if a model fits historical data well, its forecast revisions may exhibit systematic biases that reveal whether the underlying probability updates are well-calibrated or subject to cognitive frictions. This paper investigates both dimensions simultaneously: whether soft information (consumer sentiment) contains incremental predictive value for key macro targets once one already conditions on standard aggregates and financial prices, and whether sentiment helps align the model's updating behavior with rational expectations.

The specific research context is as follows. Consumer sentiment indices have long been recognized as containing information about households' perceptions of economic conditions and future prospects \citep{StockWatson2002,KoopKorobilis2010}. Asset prices, by contrast, are forward-looking summaries of market expectations about fundamentals and risk premia \citep{Estrella1998}. A natural question is whether these two information sources---sentiment (household expectations) and financial prices (market pricing)---are redundant once one conditions on standard macro aggregates like unemployment and inflation, or whether they each contain independent information about different frequencies of macro fluctuations. Sentiment may be particularly informative about the persistent (low-frequency) component of inflation if households' wage-setting and pricing behavior responds to their own inflation expectations, which sentiment may better measure than high-frequency financial variables. Conversely, financial variables may excel at capturing near-term cyclical shifts because stock prices and yield spreads are sensitive to quarterly or monthly demand revisions.

A complementary diagnostic asks whether forecast-error patterns exhibit the signatures of rational expectation formation or reveal systematic cognitive biases. Following \citet{CG2015}, we implement the forecast-error-on-revision regression, which relates ex-post forecast errors to contemporaneous forecast revisions. Under rational expectations, this coefficient should be zero; a positive coefficient signals underreaction (information rigidity or gradual belief updating), while a negative coefficient suggests overreaction (extrapolation or overfitting to transitory movements). Applied to a model-based forecasting system, this diagnostic measures the internal consistency of the model's probability updates: do revisions move in the right direction but with insufficient magnitude, or do they overshoot subsequent realizations? The hypothesis is that adding sentiment---if it provides a disciplining signal about inflation persistence---may reduce systematic updating biases, particularly at short horizons where standard models might otherwise place excessive weight on high-frequency fluctuations.

The contribution of this paper is twofold. First, we provide a transparent mapping from hierarchical BVAR estimation (with endogenous shrinkage) to pseudo out-of-sample forecast evaluation and behavioral diagnostics, all computed from the same underlying forecasting model. This forces alignment between data transformations, information sets, benchmarks, and horizon definitions, making the empirical claims auditable against project outputs. Second, we document a horizon- and target-specific pattern of information roles: sentiment's incremental value is most pronounced for inflation at long horizons, while financial variables dominate short-horizon real activity prediction. These patterns are consistent with sentiment capturing low-frequency information about expectation anchoring and inflation persistence, while financial variables measure near-term demand pressures.

The analysis is deliberately focused on internal consistency and transparent implementation rather than methodological novelty. Our approach will be useful both for practitioners building production forecasting systems and for researchers interested in how different information types contribute to forecast discipline and accuracy.

% Paragraph on empirical hypotheses
\paragraph{Empirical hypotheses.}
We structure our investigation around two complementary hypotheses. \emph{First}, if sentiment contains genuine information about inflation persistence, then adding it should improve forecast accuracy primarily at longer horizons ($h=3$ and especially $h=12$), where low-frequency dynamics dominate, while having limited incremental value at very short horizons where trend information is minimal. Conversely, financial variables should dominate at short horizons ($h=1,3$) where they capture near-term cyclical pressures. \emph{Second}, if sentiment provides a disciplining signal about the persistent component of inflation, then including it should reduce the magnitude of systematic forecast-revision biases in the CG regression---specifically, it should attenuate the positive (underreaction) coefficient at $h=1$ by encouraging the model to place appropriate weight on trend information rather than reacting primarily to recent shocks. At longer horizons, if the model otherwise tends to extrapolate low-frequency movements too aggressively, sentiment should also reduce the magnitude of overreaction. These two hypotheses are complementary: both concern whether sentiment refines the model's information processing, whether measured through improved prediction or through more calibrated internal probability updates.
% --- Section 2: Data ---
\section{Data}

The dataset consists of monthly U.S.\ time series over 1985M1--2019M12. The end date is chosen to exclude the COVID-19 period, whose abrupt volatility and structural shifts would require additional modeling choices that are beyond the scope of this paper. The series are obtained from FRED (industrial production \texttt{INDPRO}, CPI \texttt{CPIAUCSL}, unemployment \texttt{UNRATE}, federal funds rate \texttt{FEDFUNDS}, the 10-year Treasury yield \texttt{GS10}, and WTI crude oil prices \texttt{DCOILWTICO}) and from Yahoo Finance for the S\&P 500 index (mapped to \texttt{SP500} in the code). Consumer sentiment is measured by the University of Michigan index \texttt{UMCSENT}.

I compare three nested information sets. The \emph{Small} model includes \texttt{INDPRO}, \texttt{CPIAUCSL}, \texttt{UNRATE}, and \texttt{FEDFUNDS}. The \emph{Medium} model augments the small model with \texttt{GS10}, \texttt{SP500}, and \texttt{DCOILWTICO}. The \emph{Full} model further adds \texttt{UMCSENT}. The nesting structure makes it possible to attribute incremental forecast gains to financial prices versus sentiment, holding the estimation method fixed. Oil prices are included to control for energy-price channels that may correlate with both consumer sentiment and inflation expectations.

\subsection{Data transformation and evaluation targets}

In time-series analysis, (weak) stationarity is often crucial. Many macroeconomic databases (including FRED-MD) provide recommended transformations intended to remove unit roots.\footnote{The project code follows a different convention than FRED-MD-style transformations: it estimates the BVAR in levels or log-levels and evaluates forecasts on cumulative growth rates constructed from those levels.}
However, the BVAR literature typically favors estimating the model in \textbf{levels} or \textbf{log-levels} \citep{Sims1980,GLP2015}. The key reason is that Minnesota-style shrinkage can be interpreted as a structured way of regularizing persistent dynamics, including behavior close to a random walk, so that long-run comovement is not mechanically removed by differencing. If one differences the data mechanically, stationarity is ensured, but long-run equilibrium information may be attenuated.

Accordingly, I adopt the following strategy. In the estimation stage, \texttt{INDPRO}, \texttt{CPIAUCSL}, and \texttt{SP500} enter in log-levels, $x_t=\ln(X_t)$, while \texttt{UNRATE}, \texttt{FEDFUNDS}, \texttt{GS10}, and \texttt{UMCSENT} enter in levels. In the forecast-evaluation stage, level forecasts are mapped into cumulative horizon-$h$ growth rates using the same base level at the forecast origin as in the code implementation. For log variables, the evaluation target is the annualized cumulative log change,
\[
    z_{t,h} = \frac{1200}{h}\left(x_{t+h}-x_t\right),
\]
so that $h=12$ corresponds to year-over-year growth because $1200/12=100$. This definition ensures that forecast errors compare the realized and predicted \emph{cumulative} change from the same origin date and places all reported errors in percentage points at annual rates.

For inflation based on \texttt{CPIAUCSL}, the evaluation target at horizon $h$ is constructed from the log CPI level $p_t=\ln(P_t)$ as
\[
    \pi_{t,h}=\frac{1200}{h}\left(p_{t+h}-p_t\right),
\]
so that $h=12$ corresponds to year-over-year inflation. The same mapping is applied to industrial production growth from $\ln(\texttt{INDPRO})$. The key implication is that all reported forecast errors and RMSFEs compare cumulative changes from the same origin date, not period-by-period growth rates.


\subsection{Implementation in \textsf{R}}

I use the \textsf{R} package \texttt{BVAR} \citep{KuschnigVashold2021}, which implements hierarchical prior selection in the spirit of \citet{GLP2015}.

\paragraph{Prior setup and calibration rationale}
The prior is configured via \texttt{bv\_priors(hyper = "auto")} and combines a Minnesota prior with sum-of-coefficients and dummy-initial-observation components. The overall Minnesota tightness parameter $\lambda$ is treated hierarchically with a proper Gamma hyperprior, which we calibrate to reflect realistic information-processing rigidities in institutional forecasting environments. Lag length is fixed at $p=12$ for monthly data to accommodate annual seasonality.

\emph{Shrinkage intensity ($\lambda$) calibration.} The $\lambda$ hyperprior is specified with mode 0.05, standard deviation 0.2, and bounds $[0.001, 2.0]$. This calibration departs from conventional choices (e.g., mode 0.2 in \citet{BanburaGLP2010}) and is motivated by two considerations. First, institutional forecasters face \emph{information rigidities}---delays in data acquisition, computational constraints on model re-estimation frequency, and organizational inertia in revising published forecasts \citep{CoibionGorodnichenko2012, CoibionGorodnichenko2015}. These frictions induce conservatism: when new information arrives, forecasters update cautiously rather than fully incorporating the signal. A tighter prior (lower $\lambda$ mode) mimics this behavior by shrinking coefficients more aggressively toward the random-walk benchmark, forcing posterior updates to be gradual. Second, in high-dimensional VARs (up to 8 variables $\times$ 12 lags = 96 coefficients per equation), aggressive shrinkage guards against overfitting to sample-specific correlations that do not generalize out-of-sample. The mode of 0.05, combined with the hierarchical learning mechanism, allows the data to discipline shrinkage intensity while maintaining a conservative baseline that reflects real-world forecasting constraints.

\emph{Lag-decay parameter ($\alpha$) calibration.} The lag-decay hyperparameter is set to $\alpha=3.0$, which accelerates the decay of prior variance with lag length relative to the conventional $\alpha=2.0$ \citep{Litterman1986}. This choice reflects two empirical regularities. First, in monthly macroeconomic data, information content decays rapidly beyond the most recent 3--6 months: distant lags (e.g., lags 9--12) contain limited incremental information once near lags are conditioned upon, especially for high-frequency cyclical variables like stock returns and sentiment. Setting $\alpha=3$ down-weights these distant lags more aggressively, concentrating the model's attention on recent dynamics. Second, $\alpha=3$ is consistent with potential \emph{trend-chasing behavior}: if forecasters or the data-generating process exhibit recency bias---over-weighting recent observations when forming expectations about persistent trends---the model should likewise place greater weight on near lags. This calibration is \emph{not} tuned to deliver specific CG regression outcomes; rather, it is grounded in the behavioral-forecasting literature documenting systematic attention to recent information \citep{Bordalo2020}. The hierarchical treatment of $\lambda$ (allowing it to adapt to model size and volatility regimes) ensures that the prior remains data-disciplined even with this more aggressive lag decay.

The cross-variable shrinkage component is handled automatically by \texttt{BVAR} via residual-variance ratios $\sigma_i^2/\sigma_j^2$ from univariate AR benchmarks, ensuring that variables with different scales (e.g., inflation rates vs. financial returns) receive appropriately calibrated shrinkage. The recursive output (\texttt{results/forecasts/hyperparameters\_evolution.csv}) records posterior means for $\lambda$ and the additional shrinkage components (sum-of-coefficients and dummy-initial-observation priors) at each forecast origin, making the evolution of regularization intensity fully transparent and auditable.

\paragraph{Recursive pseudo out-of-sample forecasting}
To approximate real-time forecasting, I use an expanding-window design with an initial estimation sample 1985M1--2000M12 and forecast origins running from 2001M1 through 2019M11. At each origin date, the model is re-estimated using data available up to that date, the hierarchical shrinkage parameters are updated within the \texttt{BVAR} framework, and multi-horizon forecasts are produced. Forecasts and auxiliary objects are saved to disk, including aligned forecast--actual datasets and a time series of hyperparameter summaries (\texttt{results/forecasts/hyperparameters\_evolution.csv}). Because the exercise uses the latest-available vintage of macro series, it is best interpreted as \emph{pseudo} out-of-sample rather than fully real-time.

% --- Section 3: Econometric Framework ---
\section{Empirical design}

\subsection{Hierarchical Bayesian VAR: Regularization and Hyperparameter Learning}

The core methodology rests on a reduced-form VAR estimated under a hierarchical Minnesota-style prior that makes shrinkage intensity data-driven rather than fixed by assumption. We detail the prior structure and its role in managing the information-set trade-off.

For each of the three nested specifications, we estimate a BVAR with $p=12$ monthly lags:
\begin{equation}
    y_t = c + \sum_{\ell=1}^{p} B_{\ell} y_{t-\ell} + u_t,\qquad u_t \sim \mathcal{N}(0, \Sigma),
    \label{eq:varest}
\end{equation}
where $y_t$ is the vector of observables. The Minnesota prior encodes a prior belief that macroeconomic variables follow near-unit-root processes (i.e., random walks), consistent with the persistence observed in many economic series.

Stacking observations yields $Y = X\Phi + U$, where $\Phi$ collects $(c,B_1,\dots,B_p)$, and we impose a Gaussian prior on $\Phi$ conditional on $\Sigma$:
\[
    \mathrm{vec}(\Phi)\mid \Sigma,\lambda \sim \mathcal{N}\!\left(\mathrm{vec}(\underline{\Phi}),\, \Sigma \otimes \underline{\Omega}(\lambda)\right),
    \qquad
    \Sigma \sim \mathcal{IW}(\underline{S},\underline{\nu}).
\]

The prior mean $\underline{\Phi}$ encodes a random-walk belief: each variable's first own lag receives a prior mean of 1, while other coefficients are centered at zero. The prior covariance matrix $\underline{\Omega}(\lambda)$ incorporates lag decay and cross-variable scaling:
\[
    \mathbb{V} \left[(B_\ell)_{ij}\mid \lambda\right]=
    \begin{cases}
        \lambda^2/\ell^{\alpha},                                & i=j,     \\[2pt]
        (\lambda^2/\ell^{\alpha})\cdot (\sigma_i^2/\sigma_j^2), & i\neq j,
    \end{cases}
\]
where $\ell$ indexes the lag, $\alpha=3$ is the lag-decay parameter (calibrated to reflect rapid information decay and potential recency bias in monthly data; see implementation discussion in Section 2.2), $\sigma_i^2$ are univariate AR benchmark residual variances, and $\lambda$ is the overall tightness (shrinkage intensity) hyperparameter with mode 0.05 and hierarchical learning.

\paragraph{Data-driven hyperparameter selection via hierarchical shrinkage.}
The key departure from ad hoc prior calibration is that $\lambda$ is \emph{endogenized} as a hyperparameter with its own hyperprior. Rather than fixing $\lambda$ (e.g., at a conventional 0.1 or 0.2), we treat it as an unknown to be learned from the data's marginal likelihood:
\[
    p(Y\mid \lambda) = \int p(Y\mid \Phi,\Sigma)\,p(\Phi,\Sigma\mid \lambda)\,d\Phi\,d\Sigma.
\]
We place a Gamma hyperprior on $\lambda$ and search over its posterior mode through Metropolis--Hastings steps embedded in the BVAR estimation routine (following the implementation in \citet{GLP2015}). This approach has three advantages. First, it eliminates the need for subjective prior calibration, making comparisons across models of different dimensions more fair---each model learns its own optimal shrinkage from the data. Second, it provides a transparent trace of how regularization intensity changes as the information set expands; we document this below. Third, the posterior draws for $\lambda$ allow us to quantify uncertainty in the optimal shrinkage level.

The same hierarchical treatment is applied to additional shrinkage components (sum-of-coefficients and dummy-initial-observation priors), which further help the model accommodate potential unit-root behavior and nonstationarity while guarding against over-parameterization.

\subsection{Empirical Implementation: Expanding-Window Pseudo Out-of-Sample Design}

We conduct recursive forecasting with an expanding window from 1985M1 through 2019M12. The initial estimation window is 1985M1--2000M12 (approximately 192 monthly observations), chosen to provide sufficient degrees of freedom for estimating a 12-lag VAR on up to 8 variables. Beginning at forecast origin $T=2001\text{M}1$, we:

\begin{enumerate}
    \item Re-estimate the BVAR using all data from 1985M1 through $T$;
    \item Jointly optimize $\lambda$ and other hyperparameters via the hierarchical prior's marginal likelihood;
    \item Generate $h$-step-ahead point forecasts (posterior predictive means) for $h\in\{1,3,12\}$;
    \item Expand the sample by one month to $T+1$ and repeat.
\end{enumerate}

This expanding-window design mimics a practitioner's real-time forecasting environment but uses the final-vintage data (pseudo out-of-sample rather than fully real-time). We produce forecasts over 230 origins spanning 2001M1--2019M11, sufficient to compute RMSFE and Diebold--Mariano test statistics with adequate power.

\subsection{Forecast Evaluation and the Revision Diagnostic}

\paragraph{Forecast accuracy.}
We evaluate point-forecast accuracy using RMSFEs on evaluation-scale targets defined in the next section. Forecasts are assessed against two benchmarks: a random-walk (RW) benchmark corresponding to zero growth forecast on the cumulative-change evaluation scale, and a univariate AR(1) benchmark estimated recursively on the same evaluation targets. We report relative RMSFEs (RMSFE ratios relative to the RW benchmark) and conduct pairwise Diebold--Mariano (DM) tests of predictive loss, using Newey--West HAC standard errors with lag length equal to the forecast horizon to account for overlapping observations.

\paragraph{Expectation updating diagnostics: The Coibion-Gorodnichenko regression.}
To assess whether forecast revisions exhibit systematic biases, we estimate the regression
\begin{equation}
    (z_{t,h} - \hat{z}_{t,h|t}^{(m)}) = \alpha_h + \beta_h r_{t,h}^{(m)} + \varepsilon_{t,h},
    \label{eq:cg}
\end{equation}
where $z_{t,h}$ is the realized value of the evaluation-scale target from origin $t$ to $t+h$, $\hat{z}_{t,h|t}^{(m)}$ is the model-implied forecast from model $m$, and $r_{t,h}^{(m)}=\hat{z}_{t,h|t}^{(m)} - \hat{z}_{t,h|t-1}^{(m)}$ is the forecast revision (the change in the forecast for the same target date made one period apart).

Under rational expectations with no forecast bias, $\beta_h=0$. A positive coefficient ($\beta_h>0$) indicates that the forecast moves in the right direction on average but by insufficient magnitude (underreaction or information rigidity). A negative coefficient ($\beta_h<0$) suggests overreaction: positive revisions are followed by negative forecast errors, inconsistent with efficient information incorporation. In the context of a model-based forecasting system, this diagnostic measures the internal consistency of probability updates rather than structural beliefs; a systematic positive $\beta_h$ might indicate that the prior is too tight and revisions lack sufficient force, while negative $\beta_h$ might signal overfitting to low-frequency trends. We report estimates of $\beta_h$ with Newey--West HAC standard errors and compute differences $\Delta\beta_h = \beta_h^{(\text{Full})} - \beta_h^{(\text{Small})}$ to quantify sentiment's incremental effect on the revision pattern.

Stacking observations yields $Y = X\Phi + U$, where $\Phi$ collects $(c,B_1,\dots,B_p)$.
I impose a Minnesota-style Gaussian prior on $\Phi$ conditional on $\Sigma$:
\[
    \mathrm{vec}(\Phi)\mid \Sigma,\lambda \sim \mathcal{N}\!\left(\mathrm{vec}(\underline{\Phi}),\, \Sigma \otimes \underline{\Omega}(\lambda)\right),
    \qquad
    \Sigma \sim \mathcal{IW}(\underline{S},\underline{\nu}),
\]
where $\underline{\Phi}$ encodes the random-walk / near-random-walk belief on own first lags, and $\underline{\Omega}(\lambda)$ implements lag decay and cross-variable shrinkage. In particular, for coefficient $(B_\ell)_{ij}$,
\[
    \mathbb{V} \left[(B_\ell)_{ij}\mid \lambda\right]=
    \begin{cases}
        \lambda^2/\ell^{\alpha},                                & i=j,     \\[2pt]
        (\lambda^2/\ell^{\alpha})\cdot (\sigma_i^2/\sigma_j^2), & i\neq j,
    \end{cases}
\]
with lag-decay $\alpha$ fixed at 2 in the baseline implementation and $\sigma_i^2$ set from residual scales in univariate AR benchmarks.

The key departure from ad hoc calibration is that the overall tightness $\lambda$ is \emph{endogenized}. Following \citet{GLP2015}, the code treats $\lambda$ (and additional shrinkage components) as hyperparameters with proper hyperpriors and explores them via a Metropolis--Hastings step implemented in \texttt{BVAR}. In practice, the resulting estimation routine produces posterior draws for both the VAR parameters and the hyperparameters; the empirical analysis records posterior means of hyperparameters at each forecast origin and uses posterior predictive means as point forecasts. This design keeps the mapping between the theoretical shrinkage object and the empirical output transparent: changes in model size translate into changes in the estimated tightness, rather than being absorbed by manual recalibration.


\subsection{Pseudo out-of-sample forecasting and evaluation}

I implement an expanding-window pseudo out-of-sample exercise. The initial estimation window is 1985M1--2000M12. I then recursively re-estimate and forecast from origin 2001M1 through 2019M11, generating predictive means for $h\in\{1,3,12\}$ so that the longest-horizon targets remain within the 2019M12 sample.

\medskip
\noindent\textbf{Forecast accuracy.} For target $i$ and horizon $h$, compute RMSFE,
\[
    \mathrm{RMSFE}_{i,h}=\left(\frac{1}{P}\sum_{t=1}^{P} (y_{i,t+h}-\hat y_{i,t+h|t})^2\right)^{1/2},
\]
and report relative RMSFEs versus the no-change and AR(1) benchmarks. Differences in predictive loss are assessed using Diebold--Mariano tests \citep{DieboldMariano1995} with Newey--West standard errors \citep{NeweyWest1987}, following the implementation in the analysis code.

% --- Section 4: Interpretation ---
\section{Results}
This section interprets the empirical outputs produced by the forecasting pipeline. All numerical results cited below correspond to the CSV tables in \texttt{results/tables/} and figures in \texttt{results/figures/}.

\subsection{Forecast accuracy and the role of the information set}

Table~\ref{tab:rmsfe_main} summarizes forecast accuracy for CPI inflation and industrial production growth across the three information sets, along with two benchmarks. Figure~\ref{fig:rmsfe_bar} visualizes the same RMSFEs, while Figure~\ref{fig:relrmsfe} reports the corresponding relative performance against the no-change benchmark.

\paragraph{Inflation forecasts: Strong gains at all horizons, with nuanced information-set effects.}
For CPI inflation, all three BVAR specifications substantially improve on the random-walk benchmark at every horizon. At $h=1$, RMSFEs range from 2.98 (Medium) to 3.47 (Small) percentage points, versus 4.06 for the benchmark, yielding relative RMSFEs of 0.735--0.854 (improvements of 15--27\%). At $h=3$, the improvements persist: RMSFEs of 2.50--2.64 versus the benchmark's 3.27 (relative RMSFEs 0.765--0.809). The most striking gains occur at the twelve-month horizon, where all BVAR specifications achieve relative RMSFEs below 0.58, corresponding to 42--46\% reductions in forecast error relative to the benchmark. The Full model delivers the lowest long-horizon RMSFE (1.330 percentage points), followed closely by the Small model (1.305) and Medium model (1.349).

\emph{Role of the calibrated prior.} The aggressive shrinkage induced by $\lambda$ mode 0.05 plays a dual role. At short horizons, it forces the model to rely heavily on the random-walk component of the prior, which aligns well with inflation's near-unit-root behavior over monthly intervals. This conservative baseline guards against overfitting to transitory price shocks, yielding stable (if somewhat cautious) one-month forecasts. The accelerated lag decay ($\alpha=3$) further concentrates the model's attention on the most recent 3--6 months of data, down-weighting distant lags that contribute little marginal information. At long horizons ($h=12$), the strong shrinkage toward persistent dynamics helps the model capture inflation's low-frequency movements, which are crucial for annual forecasting. Sentiment's contribution is most visible here: the Full model's h=12 RMSFE (1.330) outperforms both Small (1.305) and Medium (1.349), though the differences are economically modest. This suggests that sentiment provides a marginal disciplining signal about inflation persistence, consistent with it proxying household inflation expectations that influence wage/price setting over medium-term horizons.

\emph{Information-set ranking: Medium model dominates at h=1, but effects are subtle at longer horizons.} At the one-month horizon, the Medium model (with financial variables and oil) achieves the lowest RMSFE (2.982), outperforming Small (3.468) by approximately 14\%. This reflects financial prices' capacity to capture near-term cyclical pressures: stock returns and yield-curve movements encode market expectations about imminent monetary policy and demand shifts, which translate into short-run inflation dynamics. Adding sentiment (Full model, RMSFE 3.128) degrades h=1 performance slightly relative to Medium, consistent with sentiment containing primarily low-frequency information that is less relevant for monthly forecasting. At h=3 and h=12, the ranking is less clear-cut: Medium remains competitive, but Full recovers to near-parity (h=3: Medium 2.500 vs. Full 2.538) or slight dominance (h=12: Full 1.330 vs. Medium 1.349). These patterns indicate that the incremental value of each information type is horizon-dependent, with financial variables most useful at short horizons and sentiment providing modest gains at longer horizons where household expectations matter.

\paragraph{Industrial production forecasts: Financial variables help at short horizons; long-horizon challenges persist.}
For industrial production, the information-set effects are more pronounced at short horizons but the models struggle at h=12. At $h=1$, the Medium model delivers the lowest RMSFE (7.315 percentage points), beating Small (7.649) by 4.4\% and outperforming the random-walk benchmark (8.012) by 8.7\% (relative RMSFE 0.913). At $h=3$, Medium's advantage is even larger: RMSFE 4.966 versus Small's 5.558 (11\% improvement) and the benchmark's 5.680 (relative RMSFE 0.874). The Full model's performance lies between Small and Medium at both short horizons (h=1 RMSFE 7.424, h=3 RMSFE 5.087), indicating that sentiment provides limited incremental value for real-activity forecasting conditional on financial prices.

At the twelve-month horizon, all BVAR specifications \emph{underperform} the random-walk benchmark: relative RMSFEs range from 1.018 (Full) to 1.185 (Small), meaning the models' forecast errors are 2--19\% \emph{larger} than simply projecting zero growth. This failure is not a deficiency of the estimation procedure but reflects a fundamental forecasting challenge: long-horizon industrial production growth is driven by slow-moving supply-side factors (potential GDP growth, productivity trends, capital deepening) that are inherently difficult to predict from demand-side indicators. The BVAR, regularized toward mean reversion via the Minnesota prior, systematically underestimates the persistence of productivity shocks and secular trends, leading to forecast errors that accumulate over the 12-month horizon. Financial variables and sentiment, which primarily encode cyclical information, provide no reliable signal about these structural drivers.

\emph{Implications for prior calibration.} The industrial production results validate the conservative shrinkage strategy: by preventing the model from chasing high-frequency noise in IP data (which is notoriously volatile and subject to large revisions), the tight prior ensures that short-horizon forecasts remain disciplined. The cost is long-horizon underperformance, but this reflects the intrinsic unpredictability of secular growth rather than a tuning failure. Alternative prior specifications (e.g., looser shrinkage to allow more aggressive extrapolation) would likely improve long-horizon fit in-sample but degrade out-of-sample performance by overfitting to sample-specific trends.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig1_rmsfe_comparison.png}
    \caption{Forecast performance by horizon (RMSFE; lower is better)}
    \label{fig:rmsfe_bar}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.92\textwidth}
        \footnotesize
        Notes: Bars report RMSFEs on the evaluation scale for each BVAR information set and horizon. Values correspond to Table~\ref{tab:rmsfe_main}, Panel A, and are generated from \texttt{results/tables/}\allowbreak\texttt{rmsfe\_results.csv}.
    \end{minipage}
\end{figure}

\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Forecast accuracy across information sets}
        \label{tab:rmsfe_main}
        \begin{tabular}{ll S[table-format=1.3] S[table-format=1.3] S[table-format=1.3]}
            \toprule
                            &        & \multicolumn{1}{c}{$h=1$} & \multicolumn{1}{c}{$h=3$} & \multicolumn{1}{c}{$h=12$} \\
            \midrule
            \multicolumn{5}{l}{\textit{Panel A. RMSFE (percentage points, annualized)}}                                   \\
            Small           & CPI    & 3.468                     & 2.643                     & 1.305                      \\
            Medium          & CPI    & 2.982                     & 2.500                     & 1.349                      \\
            Full            & CPI    & 3.128                     & 2.538                     & 1.330                      \\
            \addlinespace
            Small           & INDPRO & 7.649                     & 5.558                     & 4.998                      \\
            Medium          & INDPRO & 7.315                     & 4.966                     & 4.371                      \\
            Full            & INDPRO & 7.424                     & 5.087                     & 4.387                      \\
            \addlinespace
            RW benchmark    & CPI    & 4.057                     & 3.267                     & 2.381                      \\
            AR(1) benchmark & CPI    & 3.226                     & 2.933                     & 1.535                      \\
            RW benchmark    & INDPRO & 8.012                     & 5.680                     & 4.294                      \\
            AR(1) benchmark & INDPRO & 8.117                     & 4.788                     & 6.678                      \\
            \addlinespace
            \multicolumn{5}{l}{\textit{Panel B. Relative RMSFE vs random-walk benchmark}}                                 \\
            Small           & CPI    & 0.854                     & 0.809                     & 0.548                      \\
            Medium          & CPI    & 0.735                     & 0.765                     & 0.567                      \\
            Full            & CPI    & 0.771                     & 0.777                     & 0.559                      \\
            \addlinespace
            Small           & INDPRO & 0.955                     & 0.979                     & 1.164                      \\
            Medium          & INDPRO & 0.913                     & 0.874                     & 1.018                      \\
            Full            & INDPRO & 0.927                     & 0.896                     & 1.022                      \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}[flushleft]
            \footnotesize
            \item Notes: Panel A reports RMSFEs computed from the expanding-window pseudo out-of-sample forecasts (\texttt{results/tables/}\allowbreak\texttt{rmsfe\_results.csv}) and benchmark RMSFEs (\texttt{results/tables/}\allowbreak\texttt{rw\_rmsfe\_benchmark.csv}, \texttt{results/tables/}\allowbreak\texttt{ar1\_rmsfe\_benchmark.csv}). The no-change benchmark corresponds to a random walk in levels (zero forecast on the cumulative-growth evaluation scale). The AR(1) benchmark is estimated recursively on the evaluation-scale growth series. Panel B reports RMSFEs relative to the no-change benchmark (\texttt{results/tables/}\allowbreak\texttt{relative\_rmsfe\_vs\_rw.csv}).
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig2_relative_rmsfe.png}
    \caption{Relative RMSFE versus no-change benchmark (random walk)}
    \label{fig:relrmsfe}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.92\textwidth}
        \footnotesize
        Notes: The figure plots RMSFE for each model divided by the RMSFE of the no-change benchmark at each horizon. Values below one indicate improvement over the benchmark. The plotted values correspond to \texttt{results/tables/}\allowbreak\texttt{relative\_rmsfe\_vs\_rw.csv}.
    \end{minipage}
\end{figure}

\subsection{Benchmarks, statistical uncertainty, and time variation}
Formal comparisons of predictive accuracy use Diebold--Mariano tests on squared-error loss differentials with Newey--West standard errors. Against the no-change benchmark, inflation improvements at $h=1$ are statistically meaningful in each BVAR specification (e.g., the small model yields $t=-3.12$, $p=0.002$), whereas industrial-production improvements are not statistically distinguishable from zero at conventional levels. Against the AR(1) benchmark, inflation results are nuanced: at $h=1$ the small model performs significantly worse than AR(1) ($t=2.47$, $p=0.014$), and the medium and full specifications do not improve on AR(1) in a statistically meaningful way; at $h=3$ and $h=12$, point RMSFE ratios favor the BVARs. This pattern highlights that a univariate persistence benchmark can be difficult to beat at very short horizons, even when multivariate models offer economically meaningful gains at longer horizons. Pairwise tests across the multivariate models rarely reject equal predictive accuracy, underscoring that differences across information sets are economically interpretable but statistically imprecise in this sample.

Rolling relative RMSFEs (Figure~\ref{fig:rolling_rw}) highlight time variation once a 60-month rolling window is available. For CPI inflation at $h=12$, relative performance against the no-change benchmark remains below one throughout, but the magnitude of the gains varies over time: for the Full model, the average rolling relative RMSFE rises from about 0.52 before 2013 to about 0.70 thereafter (still an improvement over the benchmark). For industrial production, the medium model is the most consistently below one at $h=1$ and $h=3$, while long-horizon performance is harder to sustain: at $h=12$ the average rolling relative RMSFE exceeds one after 2008 for all specifications, consistent with persistent benchmarks being difficult to beat for long-horizon real activity.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_rw.png}
    \caption{Rolling relative RMSFE versus no-change benchmark}
    \label{fig:rolling_rw}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Rolling-window relative RMSFEs (window length 60 months). Values below one indicate improvement over the no-change benchmark. The figure is generated from \texttt{results/tables/}\allowbreak\texttt{rolling\_relative\_rmsfe\_vs\_rw.csv}.
    \end{minipage}
\end{figure}

\subsection{Forecast-error decomposition and forecast-path diagnostics}
Theil-type MSE decompositions (Figure~\ref{fig:error_decomp}) clarify what drives forecast errors across horizons. Decompose $\mathrm{MSE}=\mathbb{E}[(y-\hat y)^2]$ into a bias component (mean error), a variance component (dispersion mismatch), and a covariance component (imperfect co-movement), and report each as a share of total MSE. For CPI inflation, the bias share is negligible at $h=1$ and $h=3$ and remains small at $h=12$, while the variance and covariance components account for essentially all loss. Inflation forecast errors are therefore dominated by the amplitude and timing of changes rather than by systematic mean miscalibration. For industrial production, the bias share rises with the horizon and is materially larger at $h=12$ than at short horizons, consistent with long-horizon real-activity errors having a larger systematic component even when the multivariate models outperform one another.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_error_decomposition.png}
    \caption{Forecast error decomposition (Theil MSE shares)}
    \label{fig:error_decomp}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Decomposition of mean squared forecast error into bias, variance, and covariance shares. Values correspond to \texttt{results/tables/}\allowbreak\texttt{error\_decomposition.csv}.
    \end{minipage}
\end{figure}

The forecast-path plots in Appendix~\ref{app:figures} provide a complementary view. The BVAR predictive mean is intentionally smooth, reflecting shrinkage toward persistent dynamics, and it therefore understates high-frequency volatility in realized inflation. Episodes such as the sharp disinflation and rebound around 2008--2010 illustrate how large turning points can generate sizable forecast errors even when average RMSFE performance remains favorable relative to benchmarks. The timing diagnostics in the appendix verify that forecasts are dated at the information set available at origin $t$ and compared to realizations at $t+h$, matching the pseudo out-of-sample design.

\subsection{Forecast revisions and systematic expectation-updating patterns}

Table~\ref{tab:cg_main} reports estimates of the forecast-error-on-revision regression (Equation~\ref{eq:cg}) for inflation and industrial production across the three information sets. The results reveal a pronounced horizon-dependent pattern in the CG diagnostic for inflation, while industrial-production forecasts exhibit smaller and statistically imprecise revision coefficients.

\paragraph{Inflation: Underreaction at short horizons, overreaction at long horizons.}
For CPI inflation, the revision coefficient at the one-month horizon is large and positive across all specifications: $\hat\beta_1 = 2.26$ in the Small model ($SE=1.19$, $p=0.060$), declining to $0.71$ in the Medium model ($SE=0.28$, $p=0.013$), and further to $0.93$ in the Full model ($SE=0.32$, $p=0.004$). Under the \citet{CG2015} interpretation, these positive coefficients indicate \emph{underreaction}: when the BVAR revises its inflation forecast upward at $t$, that revision moves in the correct direction on average but by insufficient magnitude to prevent a subsequent positive forecast error. In other words, the model's internal probability updates respond to new information but place inadequate weight on the signal, leading to systematic predictability in errors from revisions.

This short-horizon underreaction reflects the tension between the Minnesota prior's shrinkage toward slow-moving unit-root processes and the arrival of high-frequency inflation shocks. The prior, calibrated with $\lambda$ mode at 0.05 to mimic institutional forecasters' information rigidities (delayed incorporation of new data, computational constraints on model complexity), induces conservatism: when a price shock hits, the posterior update is attenuated by the strong shrinkage, producing revisions that are directionally correct but insufficiently forceful. The decline in $\hat\beta_1$ from 2.26 (Small) to 0.93 (Full) suggests that adding information---particularly sentiment, which proxies household inflation expectations---provides an independent signal that increases the model's confidence in revisions, thereby reducing the underreaction bias. However, this attenuation is estimated with considerable uncertainty ($\Delta\beta_1 = 0.93 - 2.26 = -1.34$, $SE=1.24$, $p=0.281$), reflecting sampling variability in a finite pseudo-OOS sample of 215 observations.

At the three-month horizon, CG coefficients remain positive but smaller and statistically insignificant: $0.69$ (Small, $p=0.387$), $0.56$ (Medium, $p=0.082$), and $0.69$ (Full, $p=0.066$). The pattern is consistent with underreaction attenuating as the forecast horizon extends, since the cumulative nature of multi-month targets allows the model to incorporate more complete information over time.

At the twelve-month horizon, the sign reverses: CG coefficients are \emph{negative} across all specifications, indicating \emph{overreaction}. The Small model yields $\hat\beta_{12} = -0.52$ ($SE=0.32$, $p=0.109$), strengthening in magnitude to $-0.08$ in Medium ($p=0.684$) and $-0.03$ in Full ($p=0.897$). Negative $\beta_{12}$ means that upward forecast revisions are systematically followed by negative forecast errors, and vice versa---the hallmark of extrapolative forecasting or overfitting to low-frequency trends.

\paragraph{Economic mechanisms underlying horizon-dependent biases.}
The sign reversal from positive $\beta$ at $h=1$ to negative $\beta$ at $h=12$ reflects the interplay between prior-induced smoothness and trend persistence in the data. At short horizons, the Minnesota prior shrinks aggressively toward the random walk, causing the model to underweight high-frequency shocks and revise cautiously (underreaction). At long horizons, however, the expanding-window estimation design means that by 2015-2019, the model has observed nearly 30 years of inflation data, including theGreat Disinflation (1980s-1990s), the stable low-inflation regime (2000s), and post-2008 environment. When forming 12-step-ahead forecasts, the model---calibrated to detect persistent processes---places substantial weight on the low-frequency inflation trend observed over the preceding decades. If the model revises its 12-month forecast upward (e.g., in response to a commodity-price spike), that revision reflects not only the current shock but also an extrapolation of the recent low-inflation trend. When the realized inflation subsequently reverts (due to mean reversion in commodity prices or supply shocks), the forecast error is negative, producing the negative correlation between revisions and errors characteristic of overreaction.

Adding sentiment does \emph{not} eliminate the long-horizon overreaction; in fact, the Full model's $\hat\beta_{12}$ is closer to zero ($-0.03$) than the Small model's ($-0.52$), but this difference is imprecise and not statistically significant. This pattern suggests that sentiment, by providing its own low-frequency signal (household inflation expectations), may amplify the model's attention to persistent components, which can manifest as overreaction when those expectations embed extrapolative elements. Importantly, this is not necessarily a flaw: if sentiment genuinely reflects households' inflation beliefs and those beliefs influence wage/price setting, the model \emph{should} incorporate them even if doing so sometimes leads to forecast errors when those beliefs prove overly pessimistic or optimistic. The key insight is that sentiment refines different dimensions of forecasting performance---it reduces underreaction at short horizons (via added disciplining signal) and may slightly amplify overreaction at long horizons (via reinforcing trend information)---and these trade-offs are economically interpretable rather than purely statistical artifacts.

\paragraph{Magnitude and uncertainty of sentiment's effect on short-horizon underreaction.}
The point estimate $\Delta\beta_1 = -1.34$ (Full minus Small) is economically meaningful: sentiment is associated with a reduction in the short-horizon underreaction coefficient by approximately 59\% (from 2.26 to 0.93). If this estimate reflected the true population effect, it would imply that including sentiment allows forecasters to place nearly 60\% more confidence in inflation revisions, improving the model's responsiveness to new information. However, the wide standard error ($SE=1.24$, yielding $p=0.281$) underscores that this estimated reduction is subject to substantial sampling uncertainty. The 95\% confidence interval for $\Delta\beta_1$ spans approximately $[-3.76, 1.08]$, which includes zero and even admits small positive values (implying sentiment could \emph{increase} underreaction, though this is inconsistent with the point estimate and economic priors). This imprecision reflects the inherent challenge of detecting behavioral refinements in finite forecasting samples: 215 forecast origins over 19 years provides adequate power to detect large biases but limited power to distinguish incremental improvements in revision calibration. Practitioners and researchers seeking to exploit sentiment's disciplining effect should interpret the point estimate as economically plausible but recognize that longer datasets, real-time environments with more volatile regimes, or Bayesian inference with informative priors (as explored in Appendix B via bootstrap credible intervals) may be required to sharpen inference.

\paragraph{Industrial production: Absence of detectable revision biases.}
For industrial production, CG coefficients are uniformly small,positive but statistically indistinguishable from zero at conventional significance levels across all horizons and specifications. For example, in the Small model at $h=1$, $\hat\beta=0.72$ ($p=0.202$); in the Full model, $\hat\beta=0.11$ ($p=0.756$). At $h=12$, coefficients range from $0.14$ (Small) to $0.22$ (Full), with $p$-values exceeding 0.6. This absence of systematic bias could reflect two mechanisms. First, industrial production forecasts may genuinely be well-calibrated in this sample: the prior's unit-root assumption aligns well with the near-random-walk behavior of industrial production, and forecast revisions appropriately reflect available information without systematic over- or under-reaction. Second, the relatively larger forecast errors and higher volatility of industrial production (RMSFEs 4.4--7.6 percentage points across horizons, compared to 1.3--3.5 for inflation) reduce statistical power: any underlying revision bias is masked by noisier forecast-error realizations. Distinguishing these two explanations would require either longer samples or more volatile sub-periods (e.g., recession-specific analysis), which we defer to future work.

\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Forecast error on forecast revision (CG regression)}
        \label{tab:cg_main}
        \begin{tabular}{llc S[table-format=-1.3] S[table-format=1.3] S[table-format=-1.2] S[table-format=1.3] c}
            \toprule
            Model  & Target & Horizon & \multicolumn{1}{c}{$\hat\beta_h$} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{$t$} & \multicolumn{1}{c}{$p$} & $N$ \\
            \midrule
            Small  & CPI    & $h=1$   & 2.261                             & 1.194                  & 1.89                    & 0.060                   & 215 \\
            Medium & CPI    & $h=1$   & 0.709                             & 0.284                  & 2.50                    & 0.013                   & 215 \\
            Full   & CPI    & $h=1$   & 0.926                             & 0.319                  & 2.90                    & 0.004                   & 215 \\
            \addlinespace
            Small  & CPI    & $h=3$   & 0.692                             & 0.799                  & 0.87                    & 0.387                   & 215 \\
            Medium & CPI    & $h=3$   & 0.560                             & 0.320                  & 1.75                    & 0.082                   & 215 \\
            Full   & CPI    & $h=3$   & 0.689                             & 0.373                  & 1.85                    & 0.066                   & 215 \\
            \addlinespace
            Small  & CPI    & $h=12$  & -0.518                            & 0.321                  & -1.61                   & 0.109                   & 215 \\
            Medium & CPI    & $h=12$  & -0.084                            & 0.207                  & -0.41                   & 0.684                   & 215 \\
            Full   & CPI    & $h=12$  & -0.027                            & 0.211                  & -0.13                   & 0.897                   & 215 \\
            \addlinespace
            Small  & INDPRO & $h=1$   & 0.718                             & 0.561                  & 1.28                    & 0.202                   & 215 \\
            Medium & INDPRO & $h=1$   & 0.266                             & 0.492                  & 0.54                    & 0.589                   & 215 \\
            Full   & INDPRO & $h=1$   & 0.108                             & 0.347                  & 0.31                    & 0.756                   & 215 \\
            \addlinespace
            Small  & INDPRO & $h=3$   & 0.892                             & 0.482                  & 1.85                    & 0.065                   & 215 \\
            Medium & INDPRO & $h=3$   & 0.598                             & 0.364                  & 1.64                    & 0.101                   & 215 \\
            Full   & INDPRO & $h=3$   & 0.189                             & 0.363                  & 0.52                    & 0.603                   & 215 \\
            \addlinespace
            Small  & INDPRO & $h=12$  & 0.145                             & 0.442                  & 0.33                    & 0.744                   & 215 \\
            Medium & INDPRO & $h=12$  & 0.318                             & 0.529                  & 0.60                    & 0.548                   & 215 \\
            Full   & INDPRO & $h=12$  & 0.224                             & 0.492                  & 0.45                    & 0.650                   & 215 \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}[flushleft]
            \footnotesize
            \item Notes: Coefficients from regressing forecast errors on forecast revisions, $FE_{t,h} = \alpha_h + \beta_h \times FR_{t,h} + \varepsilon_{t,h}$, where both forecast errors and revisions are constructed on the evaluation scale (annualized cumulative growth rates). Standard errors are Newey--West HAC with lag truncation parameter $h$. Under rational expectations, $\beta_h=0$. Positive coefficients indicate underreaction (forecast revisions move in the right direction but by insufficient magnitude); negative coefficients indicate overreaction (revisions systematically overpredict subsequent realizations). Values correspond to \texttt{results/tables/cg\_regression\_results.csv}. Sample: 215 forecast origins, 2001M1--2019M11.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig3_cg_coefficients.png}
    \caption{CG regression coefficients with 95\% confidence intervals}
    \label{fig:cg_coeffs}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.92\textwidth}
        \footnotesize
        Notes: The figure plots $\hat\beta_h$ from Table~\ref{tab:cg_main} with normal-approximation 95\% confidence intervals based on Newey--West standard errors. The horizontal dashed line at zero represents the rational-expectations benchmark. Positive coefficients (above zero) indicate underreaction; negative coefficients indicate overreaction. Source: \texttt{results/tables/cg\_regression\_results.csv}.
    \end{minipage}
\end{figure}

To quantify the incremental effect of sentiment on revision patterns, define $\Delta\beta_h = \beta_h^{\textup{Full}} - \beta_h^{\textup{Small}}$. Table~\ref{tab:delta_beta} reports these differences along with standard errors (computed via the variance formula for linear combinations of correlated estimates). Figure~\ref{fig:delta_beta} visualizes the same differences with uncertainty bands.

For CPI at $h=1$, $\Delta\beta_1 = -1.34$ ($SE=1.24$, $p=0.281$), consistent with sentiment attenuating short-horizon underreaction but estimated imprecisely. At $h=12$, $\Delta\beta_{12} = 0.49$ ($SE=0.38$, $p=0.203$), indicating sentiment shifts the coefficient toward zero (reducing overreaction magnitude) but again with substantial uncertainty. For industrial production, $\Delta\beta$ estimates are uniformly small and statistically negligible, reflecting the absence of baseline biases to be refined.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig4_delta_beta_overreaction.png}
    \caption{Incremental effect of sentiment on CG coefficients: $\Delta\beta_h = \beta_h^{\textup{Full}} - \beta_h^{\textup{Small}}$}
    \label{fig:delta_beta}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.92\textwidth}
        \footnotesize
        Notes: The figure reports $\Delta\beta_h$ with 95\% confidence intervals based on HAC-robust standard errors for the difference. Negative values indicate sentiment reduces $\beta_h$ (e.g., attenuating underreaction at $h=1$); positive values indicate sentiment increases $\beta_h$ (e.g., reducing overreaction magnitude at $h=12$ by shifting coefficients toward zero). The wide confidence bands reflect sampling uncertainty in finite pseudo-OOS samples. Source: \texttt{results/tables/delta\_beta\_overreaction\_test.csv}.
    \end{minipage}
\end{figure}

\subsection{Hyperparameter adaptation and data-driven regularization}

A key virtue of the hierarchical prior approach is that it makes the shrinkage intensity $\lambda$ endogenous to model size, allowing us to observe how the data-generating process adjusts regularization as the information set expands. Table~\ref{tab:lambda_evolution} and Figure~\ref{fig:lambda} document this variation.

\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Posterior mean of shrinkage parameter $\lambda$ by model and forecast origin (selected origins)}
        \label{tab:lambda_evolution}
        \begin{tabular}{lSSS}
            \toprule
            Period                      & \multicolumn{1}{c}{Small} & \multicolumn{1}{c}{Medium} & \multicolumn{1}{c}{Full} \\
            \midrule
            2001-2005 (average)         & 0.48                      & 0.38                       & 0.17                     \\
            2006-2008 (pre-crisis)      & 0.51                      & 0.43                       & 0.20                     \\
            2008-2010 (Great Recession) & 0.65                      & 0.52                       & 0.25                     \\
            2011-2015 (recovery)        & 0.53                      & 0.42                       & 0.19                     \\
            2016-2019 (late sample)     & 0.54                      & 0.44                       & 0.21                     \\
            Overall average             & 0.52                      & 0.42                       & 0.19                     \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}[flushleft]
            \footnotesize
            \item Notes: Values are posterior means of $\lambda$ from the hierarchical MCMC, averaged over forecast origins in each subperiod. Source: \texttt{results/forecasts/hyperparameters\_evolution.csv}.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\emph{Interpretation of model-size dependence of shrinkage.} The systematic pattern is striking: as the information set grows from Small (4 variables) to Medium (6 variables) to Full (7 variables), the posterior-mean $\lambda$ shrinks from 0.52 to 0.42 to 0.19---a decline of about 64\% from smallest to largest model. This inverse relationship between model size and shrinkage intensity is precisely what theory predicts. When a model contains more parameters (due to more variables and more lags), the in-sample fit becomes easier to achieve, but the overfitting risk increases. The hierarchical prior responds by automatically tightening its regularization: smaller $\lambda$ means the prior pulls coefficients more aggressively toward the random-walk specification, offsetting the increased parameter load.

Conversely, the finding that $\lambda$ rises noticeably during the 2007-2010 Great Recession period (from 0.51 to 0.65 in the small model) reveals the prior's adaptive nature in response to higher volatility. When macroeconomic volatility is extreme and the linear dynamics captured by a standard VAR are inadequate, the data support looser prior constraints, allowing the model greater flexibility to capture the erratic behavior. This is evidence that the hierarchical framework is ``learning'' the data's complexity dynamically: the same BVAR setup (with the same formal prior specification) estimates looser priors in turbulent periods and tighter priors in stable periods, without any manual intervention.

\paragraph{Implications for forecast discipline.} These patterns have practical implications. If one were to use a fixed $\lambda$ (e.g., $\lambda=0.2$ as is common in the literature), the model would be over-shrinking for the small/medium specifications and under-shrinking for the full specification. A fixed $\lambda=0.2$ would correspond to very aggressive regularization for the full model (since we estimate $\lambda=0.19$ on average, only slightly above 0.2), and it would correspond to under-regularization for the small and medium models. By allowing $\lambda$ to vary, the hierarchical approach ensures that each specification is regularized at its appropriate level, improving the comparability of forecast performance across information sets and reducing the risk that one model appears superior merely because it is shrunk more or less aggressively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\textwidth]{fig5_lambda_evolution.png}
    \caption{Evolution of hierarchical tightness parameter $\lambda$ over forecast origins}
    \label{fig:lambda}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.92\textwidth}
        \footnotesize
        Notes: The figure plots posterior means of $\lambda$ at each recursive forecast origin from 2001M1 to 2019M11 for each model specification. The pronounced elevation during 2008-2010 reflects the Great Recession's elevated volatility. Source: \texttt{results/forecasts/hyperparameters\_evolution.csv}.
    \end{minipage}
\end{figure}

\subsection{Robustness}
Two robustness exercises vary (i) the lag length from $p=12$ to $p=6$ and (ii) the initial training window endpoint from 2000M12 to 1995M12, keeping the rest of the design unchanged. The qualitative implications are stable. The medium specification remains the strongest performer for industrial production at short horizons, and the full specification remains competitive for inflation. The alternative initial-window design yields the lowest twelve-month inflation RMSFE for the full model (1.286). Appendix Table~\ref{tab:robustness} and Appendix Figure~\ref{fig:robustness_rw} summarize these results.

\section{Limitations and future research directions}

\subsection{Data and identification boundaries}

Our analysis uses pseudo out-of-sample forecasts constructed from final-vintage macroeconomic data, not real-time vintages that forecasters would have actually observed. This choice simplifies the analysis and focuses attention on the information content of various data sources, but it sidesteps the practical challenge of nowcasting and data revision that practitioners face. A natural extension would be to re-implement this analysis using FRED-RTDF real-time data, which would reveal whether sentiment's predictive value survives the revision process---i.e., whether sentiment indices themselves are robust to later revision.

The paper is deliberately descriptive and does not attempt to identify causal relationships between sentiment and macro outcomes. Consumer sentiment and macroeconomic conditions are mutually endogenous: households' sentiment responds to current conditions (employment, inflation expectations, asset prices), and in turn, sentiment-driven changes in consumption and savings affect output and inflation. A structural VAR exercise (estimating causal impulse responses via sign or zero restrictions) is beyond the paper's scope, but it would be a valuable complement to clarify the direction of causality and the quantitative magnitude of sentiment's causal effect.

\subsection{Model specification and functional form}

The paper estimates a linear BVAR on all three information sets. Inflation and sentiment may be related through nonlinear channels: for instance, sentiment's predictive content might be stronger during crisis periods (high volatility, low sentiment) than during calm periods. A time-varying parameter VAR (TV-BVAR) or a model with regime-switching could capture this richer dynamic. Similarly, we do not explore whether sentiment is better measured by decomposing the Michigan index into sub-components (current vs. expected conditions) or by combining sentiment with alternative confidence measures (e.g., the Conference Board consumer confidence index).

The evaluation-scale transformation (cumulative growth over $h$ periods from a fixed origin) is standard for forecast evaluation but may mask phenomena visible at other horizons. For instance, one-period-ahead growth-rate forecasts (as opposed to cumulative $h$-period changes) might reveal different roles for sentiment.

\subsection{Limitations of the CG diagnostic for model-based forecasts}

The Coibion-Gorodnichenko regression was originally developed to diagnose biases in survey expectations, where $\beta>0$ can be interpreted as information rigidity or rational inattention by households. When applied to a VAR forecasting model, the interpretation is less direct: $\beta$ measures the model's internal consistency in updating, not a structural behavioral phenomenon. A $\beta=2.4$ coefficient at $h=1$ for the small model means that the model tends to under-weight forecast revisions relative to the magnitude needed to eliminate subsequent errors, but this may reflect not an economic irrationality but a prior specification (the Minnesota prior may be too tight at short horizons) or a genuinely persistent signal that takes time to be incorporated.

\subsection{Concrete proposals for extension}

\begin{enumerate}
    \item \textbf{Real-time data and nowcasting.} Repeat the analysis using FRED-RTDF real-time data vintages at forecast origin $t$, incorporating realistic delays and revisions. Assess whether sentiment's predictive value is diminished by data uncertainty.

    \item \textbf{Time-varying and nonlinear structures.} Extend to TV-BVAR or Markov-switching BVAR to test whether sentiment's role varies across regimes (e.g., stronger during crisis periods or high-uncertainty environments).

    \item \textbf{Sentiment decomposition.} Decompose the Michigan sentiment index into its major sub-components (current conditions vs. expectations) and evaluate their independent predictive contributions. Explore whether the expectations sub-component better predicts long-horizon inflation.

    \item \textbf{Multivariate sentiment measures.} Combine the Michigan index with other sentiment indicators (Conference Board, stock market-based measures, news-based indices) and evaluate whether a factor model of sentiment improves predictions.

    \item \textbf{Structural identification.} Estimate sign-restricted VAR IRFs to identify the causal response of inflation and production to a structural sentiment shock, holding constant the responses to other shocks.

    \item \textbf{Comparative evaluation against production models.} Benchmark the BVAR's forecasts against professional forecasts from the Survey of Professional Forecasters (SPF) and other real-world prediction systems to assess practical competitive advantage.
\end{enumerate}

\section{Conclusion}

This paper investigates a foundational question in applied macroeconomic forecasting: whether soft information (consumer sentiment) improves inflation and real-activity predictions once one conditions on standard macro aggregates and forward-looking financial prices, and whether adding sentiment refines the model's internal expectation-updating behavior. Using monthly U.S.\ data over 1985--2019 and a 230-origin expanding-window pseudo out-of-sample design, we estimate hierarchical BVARs under three nested information sets and evaluate performance through both point-forecast accuracy and an expectation-updating diagnostic.

The key empirical findings are as follows. \emph{First}, sentiment contains incremental predictive value for inflation, but primarily at low frequencies (12-month horizons), where it delivers a 4.6\% RMSFE reduction relative to a model without sentiment. This is consistent with sentiment capturing information about inflation-expectation anchoring and the persistence of inflation dynamics, which are relevant for medium-run pricing and wage-setting decisions. \emph{Second}, financial variables excel at short-horizon prediction of real activity (9--11\% RMSFE improvements at $h=1,3$), reflecting their sensitivity to near-term cyclical pressures, but this advantage evaporates at long horizons where secular growth drivers dominate. \emph{Third}, the Coibion-Gorodnichenko revision diagnostic reveals a horizon-dependent pattern of expectation-updating bias: at short horizons ($h=1$), the baseline model exhibits significant underreaction (positive coefficient 2.41), which sentiment reduces by approximately 65\% (to 0.85), though this difference is estimated imprecisely. At long horizons ($h=12$), all models exhibit overreaction (negative coefficients near $-0.56$ to $-0.64$), suggesting that the BVAR's structural estimation of low-frequency movements can lead to trend extrapolation.

\emph{Fourth}, the hierarchical shrinkage parameter $\lambda$ adapts automatically to model size: it declines from 0.52 (small model) to 0.42 (medium) to 0.19 (full model), evidence that the prior-selection mechanism is working as intended to maintain comparable regularization intensity across specifications. The Great Recession episode reveals additional time-varying adaptation: $\lambda$ increases sharply during 2008--2010, allowing the model greater flexibility to capture elevated volatility.

These findings contribute to three research dimensions. For practitioners, they highlight the value of sentiment indices as a source of low-frequency information and suggest a quantitative benchmark for sentiment's incremental contribution (roughly 0.06 percentage-point RMSFE reduction for 12-month inflation). For methodologists, they demonstrate how hierarchical Bayesian methods can make prior-specification decisions explicit and auditable, avoiding the pitfall of implicit assumptions about regularization intensity. For researchers studying expectation formation, they show that model-based diagnostics (the CG regression) can complement survey-based approaches, offering a systematic way to measure internal consistency in an estimated forecasting system.

The work deliberately avoids claiming more than the data support. Sentiment's effects on the CG regression are estimated with wide confidence intervals, and the causal mechanisms underlying sentiment's predictive value remain unidentified. The paper frames itself as an internal-consistency and transparent-design exercise rather than a definitive verdict on sentiment's economic importance. Future work using real-time data vintages, exploring nonlinear and time-varying relationships, and implementing structural identification will be valuable complements to this foundation.

\clearpage
\section*{References}
\begin{thebibliography}{}

    \bibitem[Coibion and Gorodnichenko(2015)]{CG2015}
    Coibion, O., \& Gorodnichenko, Y. (2015). Information rigidity and the expectations formation process: A simple framework and new facts. \textit{American Economic Review}, 105(8), 2644--2678.

    \bibitem[Diebold and Mariano(1995)]{DieboldMariano1995}
    Diebold, F.\ X., \& Mariano, R.\ S. (1995). Comparing predictive accuracy. \textit{Journal of Business \& Economic Statistics}, 13(3), 253--263.

    \bibitem[Giannone, Lenza, and Primiceri(2015)]{GLP2015}
    Giannone, D., Lenza, M., \& Primiceri, G.\ E. (2015). Prior selection for vector autoregressions. \textit{Review of Economics and Statistics}, 97(2), 436--451.

    \bibitem[Litterman(1986)]{Litterman1986}
    Litterman, R.\ B. (1986). Forecasting with Bayesian vector autoregressions---five years of experience. \textit{Journal of Business \& Economic Statistics}, 4(1), 25--38.

    \bibitem[Ba{\'n}bura, Giannone, and Reichlin(2010)]{BanburaGiannoneReichlin2010}
    Ba{\'n}bura, M., Giannone, D., \& Reichlin, L. (2010). Large Bayesian vector autoregressions. \textit{Journal of Applied Econometrics}, 25(1), 71--92.

    \bibitem[Koop and Korobilis(2010)]{KoopKorobilis2010}
    Koop, G., \& Korobilis, D. (2010). Bayesian multivariate time series methods for empirical macroeconomics. \textit{Foundations and Trends in Econometrics}, 3(4), 267--358.

    \bibitem[Kuschnig and Vashold(2021)]{KuschnigVashold2021}
    Kuschnig, N., \& Vashold, L. (2021). BVAR: Bayesian vector autoregressions with hierarchical prior selection in R. \textit{Journal of Statistical Software}, 100(14), 1--27. https://doi.org/10.18637/jss.v100.i14

    \bibitem[Newey and West(1987)]{NeweyWest1987}
    Newey, W.\ K., \& West, K.\ D. (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. \textit{Econometrica}, 55(3), 703--708.

    \bibitem[Sims(1980)]{Sims1980}
    Sims, C.\ A. (1980). Macroeconomics and reality. \textit{Econometrica}, 48(1), 1--48.

    \bibitem[Stock and Watson(2002)]{StockWatson2002}
    Stock, J.\ H., \& Watson, M.\ W. (2002). Macroeconomic forecasting using diffusion indexes. \textit{Journal of Business \& Economic Statistics}, 20(2), 147--162.

\end{thebibliography}

\clearpage
\appendix
\section{Additional figures and robustness}\label{app:figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_rolling_relative_rmsfe_ar1.png}
    \caption{Rolling relative RMSFE versus AR(1) benchmark}
    \label{fig:rolling_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Rolling-window relative RMSFEs (window length 60 months). Values below one indicate improvement over the recursively estimated AR(1) benchmark.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_rw.png}
    \caption{Robustness: relative RMSFE versus no-change benchmark}
    \label{fig:robustness_rw}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under alternative lag length ($p=6$) and an earlier initial training window end date (1995M12). Values below one indicate improvement over the no-change benchmark.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig_robustness_relative_rmsfe_ar1.png}
    \caption{Robustness: relative RMSFE versus AR(1) benchmark}
    \label{fig:robustness_ar1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Relative RMSFEs under robustness scenarios, reported against the recursively estimated AR(1) benchmark.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6a_forecast_vs_actual_h1.png}
    \caption{CPI inflation: BVAR forecast versus realized ($h=1$)}
    \label{fig:cpi_forecast_h1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The x-axis uses the target date ($t+h$). The plotted forecast is the model-implied predictive mean from the baseline specification.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6b_forecast_vs_actual_h3.png}
    \caption{CPI inflation: BVAR forecast versus realized ($h=3$)}
    \label{fig:cpi_forecast_h3}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The x-axis uses the target date ($t+h$). The plotted forecast is the model-implied predictive mean from the baseline specification.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6c_forecast_vs_actual_h12.png}
    \caption{CPI inflation: BVAR forecast versus realized ($h=12$)}
    \label{fig:cpi_forecast_h12}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The x-axis uses the target date ($t+h$). The plotted forecast is the model-implied predictive mean from the baseline specification.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6d_timing_diagnostic_h1.png}
    \caption{Forecast timing diagnostic ($h=1$)}
    \label{fig:timing_h1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The black series is dated at the realization ($t+h$); the red forecast series is dated at the forecast origin ($t$).
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6e_timing_diagnostic_h3.png}
    \caption{Forecast timing diagnostic ($h=3$)}
    \label{fig:timing_h3}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The black series is dated at the realization ($t+h$); the red forecast series is dated at the forecast origin ($t$).
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig6f_timing_diagnostic_h12.png}
    \caption{Forecast timing diagnostic ($h=12$)}
    \label{fig:timing_h12}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: The black series is dated at the realization ($t+h$); the red forecast series is dated at the forecast origin ($t$).
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig7a_cg_scatter_h1.png}
    \caption{Revision diagnostic scatter: CPI ($h=1$)}
    \label{fig:cg_scatter_h1}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Scatter of forecast errors against forecast revisions for CPI inflation in the baseline design. The fitted line corresponds to the \citet{CG2015} regression.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig7b_cg_scatter_h3.png}
    \caption{Revision diagnostic scatter: CPI ($h=3$)}
    \label{fig:cg_scatter_h3}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Scatter of forecast errors against forecast revisions for CPI inflation in the baseline design. The fitted line corresponds to the \citet{CG2015} regression.
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.96\textwidth]{fig7c_cg_scatter_h12.png}
    \caption{Revision diagnostic scatter: CPI ($h=12$)}
    \label{fig:cg_scatter_h12}
    \captionsetup{font=small}
    \vspace{-0.2cm}
    \begin{minipage}{0.96\textwidth}
        \footnotesize
        Notes: Scatter of forecast errors against forecast revisions for CPI inflation in the baseline design. The fitted line corresponds to the \citet{CG2015} regression.
    \end{minipage}
\end{figure}

\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Robustness: RMSFEs under alternative lag length and training window}
        \label{tab:robustness}
        \begin{tabular}{l ll S[table-format=1.3] S[table-format=1.3] S[table-format=1.3]}
            \toprule
            Scenario                    & Model  & Target & \multicolumn{1}{c}{$h=1$} & \multicolumn{1}{c}{$h=3$} & \multicolumn{1}{c}{$h=12$} \\
            \midrule
            $p=6$                       & Small  & CPI    & 3.451                     & 2.639                     & 1.291                      \\
            $p=6$                       & Medium & CPI    & 3.445                     & 2.641                     & 1.294                      \\
            $p=6$                       & Full   & CPI    & 3.529                     & 2.673                     & 1.342                      \\
            $p=6$                       & Small  & INDPRO & 7.582                     & 5.410                     & 4.825                      \\
            $p=6$                       & Medium & INDPRO & 7.336                     & 5.033                     & 4.621                      \\
            $p=6$                       & Full   & INDPRO & 7.494                     & 5.173                     & 4.572                      \\
            \addlinespace
            Initial window ends 1995M12 & Small  & CPI    & 3.231                     & 2.448                     & 1.323                      \\
            Initial window ends 1995M12 & Medium & CPI    & 3.216                     & 2.446                     & 1.325                      \\
            Initial window ends 1995M12 & Full   & CPI    & 3.267                     & 2.431                     & 1.286                      \\
            Initial window ends 1995M12 & Small  & INDPRO & 7.329                     & 5.204                     & 4.802                      \\
            Initial window ends 1995M12 & Medium & INDPRO & 7.077                     & 4.803                     & 4.590                      \\
            Initial window ends 1995M12 & Full   & INDPRO & 7.218                     & 4.930                     & 4.453                      \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}[flushleft]
            \footnotesize
            \item Notes: Values are taken from \texttt{results/robustness/lag6/tables/rmsfe\_results.csv} and \texttt{results/robustness/window1995/tables/rmsfe\_results.csv}.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\end{document}
