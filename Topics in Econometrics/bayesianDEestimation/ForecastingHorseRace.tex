\documentclass[12pt,a4paper]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\setstretch{1.25}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}

\pagestyle{empty}

\begin{document}

\begin{center}
    {\Large \textbf{Forecasting Horse Races and ``Belief Distortions'':\\
            A Hierarchical Bayesian VAR Study with Sentiment Signals}}\\[2pt]
    % {\normalsize Term Paper Proposal --- Topics in Econometrics (EI137)}\\
    % {\normalsize Geneva Graduate Institute (IHEID)}\\[2pt]
    {\normalsize \textit{Jingle Fu}}\\
    {\normalsize \today}
\end{center}

% --- Section 1: Motivation ---
\section*{1. Research Question and Motivation}


Recent literature on Diagnostic Expectations (DE) suggests that economic agents overreact to news,
whereas information rigidity models predict underreaction. This project investigates two precise questions:


\textit{1. Does expanding the information set of a hierarchical BVAR to include forward-looking financial prices and consumer sentiment reduce Root Mean Squared Forecast Error (RMSFE) relative to smaller baselines?}

\textit{2. Whether adding sentiment changes the Coibion-Gorodnichenko(CG) error-revision coefficient in a direction consistent with diagnostic overreaction.}
% --- Section 2: Data ---
\section*{2. Data}

I use monthly U.S. macroeconomic series from the FRED-MD database, covering the period \textbf{1985M1--2019M12}.
The sample ends in 2019 to avoid COVID-19 outliers that would require complex volatility modeling beyond the scope of this term paper.
To ensure consistent evaluation, variables are estimated in log-levels (to preserve cointegration) but evaluated in growth rates.
The analysis compares three nested information sets:
\begin{itemize}[leftmargin=*]
    \item \textbf{Small Model (Baseline):} Industrial Production (INDPRO), Consumer Price Index (CPIAUCSL), Unemployment Rate (UNRATE), and Federal Funds Rate (FEDFUNDS).
    \item \textbf{Medium Model (Financial Extension):} Adds the 10-Year Treasury Yield (GS10) and S\&P 500 Index (S\&P500) to capture forward-looking financial cycles.
    \item \textbf{Full Model (Sentiment Extension):} Adds the University of Michigan Consumer Sentiment Index (UMCSENT) to test the marginal predictive power of ``soft'' data.
\end{itemize}

\subsection*{2.1\quad Data Transformation}

In time-series analysis, (weak) stationarity is often crucial. The FRED-MD database provides, for each variable, a recommended \emph{transformation code} (``tcode'') intended to remove unit roots.\footnote{See the FRED-MD documentation for the definition of tcodes and their recommended transformations.}
However, the BVAR literature (e.g., Sims; and Giannone, Lenza, and Primiceri, 2015) typically favors estimating the model in \textbf{levels} or \textbf{log-levels}. The key reason is that the Minnesota prior includes a ``unit-root prior'' (setting $\delta_i=1$), which is designed explicitly for nonstationary data: by shrinking coefficients toward a random-walk specification, the prior allows the model---when supported by the data---to capture long-run comovement and potential cointegration relationships. If one differences the data mechanically, stationarity is ensured, but valuable long-run equilibrium information (e.g., the long-run link between interest rates and inflation) may be lost.

Accordingly, we adopt the following strategy:
\begin{itemize}
    \item \textbf{Estimation stage.}
          \begin{itemize}
              \item \texttt{INDPRO}, \texttt{CPIAUCSL}, \texttt{S\&P 500}: use log-levels,
                    \[
                        x_t = \ln(X_t).
                    \]
              \item \texttt{UNRATE}, \texttt{FEDFUNDS}, \texttt{GS10}, \texttt{UMCSENT}: keep in levels (no logs), since these are already rates or index-type series.
          \end{itemize}

    \item \textbf{Forecast evaluation stage.}
          The model produces forecasts in levels or log-levels. For forecast evaluation, we transform forecasts into annualized growth rates (for \texttt{CPIAUCSL} and \texttt{INDPRO}) or into changes in levels,
          so that our RMSFE comparisons align with standard practice in the literature.
\end{itemize}

For example, inflation forecast errors based on \texttt{CPIAUCSL} can be computed using the annualized monthly inflation rate
\[
    \pi_t^{\text{ann}} = 1200 \times \ln\!\left(\frac{P_t}{P_{t-1}}\right),
\]
or alternatively using year-over-year inflation,
\[
    \pi_t^{\text{yoy}} = 100 \times \ln\!\left(\frac{P_t}{P_{t-12}}\right).
\]
Analogous transformations are applied to \texttt{INDPRO} to obtain annualized growth rates for forecast evaluation.


\subsection*{2.2\quad Implementation with R}

We employ the \textsf{R} package \texttt{BVAR} developed by Kuschnig and Vashold (2021),
which provides an authoritative and convenient implementation of the hierarchical prior framework in Giannone, Lenza, and Primiceri (2015).
\footnote{See Kuschnig and Vashold (2021) for details on the \texttt{BVAR} package and its implementation of hierarchical shrinkage priors.}

\paragraph{Step 1: Prior Setup}
We specify priors using the function \texttt{bv\_priors()}.
\begin{itemize}
    \item \textbf{Minnesota prior.} We place a distribution on the key shrinkage hyperparameter $\lambda$.
          Unlike the conventional approach that fixes $\lambda$ (e.g., \texttt{lambda = 0.2}), we set a \emph{mode} and \emph{standard deviation},
          and use the data to select the optimal $\lambda$ via marginal likelihood maximization.
    \item \textbf{Dummy observations.} We include \emph{sum-of-coefficients} and \emph{dummy-initial-observation} priors to further help the model accommodate unit-root behavior and nonstationarity.
          These components are also treated hierarchically and are automatically tuned within the estimation.
    \item \textbf{Lag length.} We fix the lag order at $p=12$ (12 monthly lags). Rather than truncating higher-order lags manually,
          we rely on the strong shrinkage implied by the hierarchical prior to control the parameters on distant lags.
\end{itemize}

\paragraph{Step 2: Recursive Pseudo-Out-of-Sample Forecasting}
To mimic a real-time forecasting environment, we conduct recursive forecasting with an expanding window:
\begin{itemize}
    \item \textbf{Initial window:} 1985M1--2000M12 (approximately 15 years of data for initial training).
    \item \textbf{Recursive loop:} At each forecast origin $T$ (starting at 2000M12),
          \begin{enumerate}
              \item estimate the model using data from $1985\text{M}1$ through $T$, jointly optimizing hyperparameters;
              \item produce $h=1,3,12$ step-ahead forecasts;
              \item expand the sample by one month to $T+1$, re-estimate the model, and re-optimize the hyperparameters.
          \end{enumerate}
    \item \textbf{Output:} This procedure yields three forecast time series (for $h=1$, $h=3$, and $h=12$), covering roughly 230 forecast origins over 2001--2019.
\end{itemize}

% --- Section 3: Econometric Framework ---
\section*{3. Econometric Framework}
The core methodology relies on a reduced-form VAR estimated with a Minnesota-style Normal-Inverse-Wishart prior.
\begin{enumerate}
    \item \textbf{Hierarchical BVAR:} Let $y_t$ be the vector of endogenous variables. We estimate three nested BVAR systems with $p=12$ lags:
          \begin{equation}
              y_t^{Small, Medium, Full} = c + \sum_{\ell=1}^{p} B_{\ell} y_{t-\ell} + u_t, \quad u_t \sim \mathcal{N}(0, \Sigma)
          \end{equation}
          across three distinct information sets.
          Shrinkage is selected endogenously by treating $\lambda $ as a hyperparameter with a hyperprior and choosing it by marginal likelihood.
          We compute RMSFE for $h = {1,3,12}$ and report RMSFE ratios relative to the benchmark, together with Diebold-Mariano tests for pairwise comparisons.
          Benchmarks are a random-walk-type forecast and an AR(1) forecast defined on the same evaluation transforms used for the BVAR outputs.


    \item \textbf{Identification of ``Behavioral'' Bias:}
          Let $z_{t+h}$ denote the realized growth rate of a target variable ($z \in \{\text{INDPRO}, \text{CPI}\}$) at horizon $h$.
          Let $\hat{z}_{t+h|t}^{(m)}$ denote the forecast generated by Model $m \in \{\text{Small, Med, Full}\}$ at time $t$.
          We estimate the following regression linking ex-post forecast errors to forecast revisions:
          \begin{equation}
              (z_{t+h} - \hat{z}_{t+h|t}^{(m)}) = \alpha_h + \beta_h (\hat{z}_{t+h|t}^{(m)} - \hat{z}_{t+h|t-1}^{(m)}) + \varepsilon_{t+h}
          \end{equation}
          where the term in the first parenthesis represents the forecast error and the term in the second parenthesis represents the forecast revision.
          Since the forecast horizon $h$ creates overlapping observations, inference on $\beta_h$ relies on Newey-West HAC standard errors.
\end{enumerate}

\subsection*{3.1\quad Minnesota prior and hierarchical tightness}

Stacking observations yields $Y = X\Phi + U$, where $\Phi$ collects $(c,B_1,\dots,B_p)$.
I impose a Minnesota-style Gaussian prior on $\Phi$ conditional on $\Sigma$:
\[
    \mathrm{vec}(\Phi)\mid \Sigma,\lambda \sim \mathcal{N}\!\left(\mathrm{vec}(\underline{\Phi}),\, \Sigma \otimes \underline{\Omega}(\lambda)\right),
    \qquad
    \Sigma \sim \mathcal{IW}(\underline{S},\underline{\nu}),
\]
where $\underline{\Phi}$ encodes the random-walk / near-random-walk belief on own first lags, and $\underline{\Omega}(\lambda)$ implements lag decay and cross-variable shrinkage. In particular, for coefficient $(B_\ell)_{ij}$,
\[
    \mathbb{V} \left[(B_\ell)_{ij}\mid \lambda\right]=
    \begin{cases}
        \lambda^2/\ell^2,                                & i=j,     \\[2pt]
        (\lambda^2/\ell^2)\cdot (\sigma_i^2/\sigma_j^2), & i\neq j,
    \end{cases}
\]
with $\sigma_i^2$ set from residual scales in univariate AR benchmarks.

The key departure from ad hoc calibration is that the overall tightness $\lambda$ is \emph{endogenized}.
Following Giannone--Lenza--Primiceri, I treat $\lambda$ as a hyperparameter selected by the data through the marginal data density (MDD),
\[
    p(Y\mid \lambda) = \int p(Y\mid \Phi,\Sigma)\,p(\Phi,\Sigma\mid \lambda)\,d\Phi\,d\Sigma,
\]
and use the \texttt{BVAR} package implementation to obtain $\hat\lambda$ (and other prior hyperparameters) via hierarchical prior selection.
Conjugacy implies that, conditional on $\lambda$, the posterior is Normal--Inverse-Wishart,
which yields closed-form posterior moments and a tractable posterior predictive distribution for $y_{t+h}$.

We first give $\lambda$ a Gamma hyperprior, $\lambda \sim \mathcal{G}(a,b)$, and based on data $Y$, we compute the best posterior mode $\hat\lambda$ by maximizing $p(Y\mid \lambda)p(\lambda)$ over a grid of $\lambda$ values.
Then, conditional on $\hat\lambda$, we obtain the posterior of $(\Phi,\Sigma)$ and the predictive distribution of $y_{t+h}$.


\subsection*{3.2\quad Pseudo out-of-sample forecasting and evaluation}

I implement an expanding-window pseudo out-of-sample exercise. The initial estimation window is 1985M1--2000M12;
I then recursively re-estimate and forecast through 2019M12, generating predictive means for $h\in\{1,3,12\}$.

\medskip
\noindent\textbf{Forecast accuracy.} For target $i$ and horizon $h$, compute RMSFE,
\[
    \mathrm{RMSFE}_{i,h}=\left(\frac{1}{P}\sum_{t=1}^{P} (y_{i,t+h}-\hat y_{i,t+h|t})^2\right)^{1/2},
\]
and report relative RMSFEs versus RW/AR(1). I will conduct Diebold--Mariano tests for differences in predictive loss between Small and Medium specifications.

\subsection*{3.3\quad Evaluation of Forecasting Competition}
We evaluate forecasting performance using the following metrics:

\begin{itemize}
    \item \textbf{Absolute accuracy:} For each model and forecast horizon, we compute the root mean squared forecast error (RMSFE).

    \item \textbf{Relative accuracy:} We normalize each model's RMSFE by the RMSFE of a random-walk (RW) benchmark to obtain the \emph{relative RMSFE}:
          \[
              \text{RelRMSFE}_{m,h} \;=\; \frac{\text{RMSFE}_{m,h}}{\text{RMSFE}_{\text{RW},h}}.
          \]
          Values below one indicate that model $m$ outperforms the benchmark at horizon $h$.

    \item \textbf{Statistical significance:} We apply the Diebold--Mariano (DM) test to assess whether the medium-scale BVAR delivers significantly better predictive accuracy than (i) the small-scale BVAR and (ii) univariate benchmark models.
\end{itemize}

% --- Section 4: Interpretation ---
\section*{4. Interpretation and Expected Results}

First, We verify whether the Hierarchical BVAR outperforms AR(1) benchmarks.
The key test is whether the \textit{Full Model} lowers RMSFE for real activity variables at short horizons ($h=1, 3$) (and at long horizens $h=12$).

Our key object is $\Delta \beta_h$.
A shift in $\beta_h$ \emph{toward zero} when adding UMCSENT is interpreted as sentiment improving the informational content of revisions,
making updates less systematically biased.
By contrast, if this pushes $\beta_h$ \emph{further away from zero}---especially into negative---it suggests that sentiment
generating an overreaction pattern consistent with the diagnostic-expectations sign prediction.

\end{document}
