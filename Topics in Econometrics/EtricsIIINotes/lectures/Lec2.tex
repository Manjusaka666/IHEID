With prior $\lambda $ and $\sigma ^2 = 1$, the MDD expression from above can be simplified to:
\begin{equation}
    p(y) = \frac{\left(2 \pi \right)^{-\frac{n}{2}} \exp \left( -\frac{1}{2} Y^{\prime} Y \right) \left|\, \lambda^{-1} I_k \right|^{ - \frac{1}{2}} }{\left|\, X^{\prime} X + \lambda I \right|^{\frac{1}{2}} \exp \left( - \frac{1}{2} \overline{\beta }^{\prime} \overline{V}^{-1} \overline{\beta} \right) }
\end{equation}

Taking logs, we get:
\begin{align*}
    \log p(y) &= c - \frac{1}{2} Y^{\prime} Y + \frac{1}{2} \overline{\beta }^{\prime} \overline{V}^{-1} \overline{\beta } - \frac{1}{2} \log \left|\, X^{\prime} X + \lambda I_k \right| \\
    &= c - \frac{1}{2} \left[  Y^{\prime} Y\, - Y^{\prime} X \overline{V} X^{\prime} Y\, \right] - \frac{1}{2} \log \lambda^{- k} \left|\, X^{\prime} X + \lambda I \right| \\
    &= c - \frac{1}{2} \left[  Y^{\prime} Y\, - Y^{\prime} X \left(X^{\prime} X + \lambda I_k \right)^{-1} X^{\prime} Y\, \right] - \frac{1}{2} \log \left|\, \lambda^{-1} X^{\prime} X + I \right|    
\end{align*}
where $c = -\frac{n}{2} \log(2 \pi)$ is a constant that doesn't depend on $Y$ or $\lambda $.

The penalty term:
\begin{align*}
    \log \left|\, \lambda^{-1} X^{\prime} X + I \right| &= - \frac{1}{2} \log \left|\,n \left( \frac{1}{n} \sum_{i} x_i x_i^{\prime} + \frac{\lambda}{n} I \right) \right| \\
    &= - \frac{k}{2} \log n - \frac{1}{2} \log \left|\, Q_n + \frac{\lambda}{n} I \right|
\end{align*}

\begin{definition}
    The Bayesian Information Criterion (BIC) is defined as:
    \begin{equation}
        BIC = \log p(y | \hat{\beta }_{ML}) - \frac{k}{2} \log n
    \end{equation}
    where $\hat{\beta }_{ML}$ is the maximum likelihood estimate of $\beta $.
\end{definition}

For $\beta \sim \mathcal{N} \left( \beta_0, \sigma ^2 V_0 \right)$,
\begin{itemize}
    \item Ridge: $\beta \sim \mathcal{N} (0, \sigma ^2 \lambda^{-1} I)$, $\beta _j \sim \mathcal{N} (0, \sigma ^2 \lambda^{-1})$
    \item Lasso: $\beta \sim Laplace(\cdot)$, $p(\beta_j ) \propto \exp \left( -\lambda \left| \beta _j \right| \right)$,
                where the mode solves
                \[\min_{\beta} \left\{ \left( Y - X \beta \right)^{\prime} \left( Y - X \beta \right) + \tilde{\lambda} \sum_{j=1}^{k} \left| \beta_j \right| \right\}\].
\end{itemize}

\subsection{Model Selection}
Suppose we have regression models $M_j$, with parameters $\theta _j$


Suppose we have two regression models:
\begin{equation}
    M_1: y_i = \beta _1^1 x_{1i} + \beta_2^1 x_{2i} + u_i, \quad M_2: y_i = \beta _1^2 x_{1i}+ v_i
\end{equation}
with $p\left(\beta _1^1, \beta _2^1 | M_1 \right) = \mathcal{N} \left( \begin{bmatrix}
     0 \\
     0 \\
\end{bmatrix}, \begin{bmatrix}
    1 &  0 \\
    0 &  1 \\
\end{bmatrix} \right)$,
and for model 2, we have
$p\left( \beta _1^2 | M_2 \right) = \mathcal{N} \left( 0, 1 \right)$
$p\left( \beta _1^2 | M_2 \right) = \delta _0$
Then $p\left( \beta _1^2, \beta _2^2 | M_2 \right) = \mathcal{N} \left( \begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}, \begin{bmatrix}
    1 &  0 \\
    0 &  0 \\
\end{bmatrix} \right)$,
so, we can write:
\begin{equation*}
    p(\beta ) = \mathcal{N} \left( \begin{bmatrix}
        0 \\
        0 \\
    \end{bmatrix}, \begin{bmatrix}
        1 &  0 \\
        0 &  \lambda \\
        \end{bmatrix} \right)
\end{equation*}
with $\lambda \in \{0, 1\}$.

\[
p(\theta | y) = \sum_{j} \pi_{j,n} p(\theta | y, M_j)
\]

For $y_i = \beta _1 x_{1i} + \beta _2 x_{2i} + u_i$,
with $p(\beta | M_j) = \mathcal{N} \left( \begin{bmatrix}
     0 \\
     0 \\
\end{bmatrix}, \begin{bmatrix}
    1 & 0 \\
    0 & \lambda \\
\end{bmatrix} \right)$,
we have:
\begin{equation*}
    \lambda = \begin{dcases}
        0, & \pi_{2,0} \\
        1, & \pi_{1,0} 
    \end{dcases}
\end{equation*},
then
\begin{align*}
    p(\beta , \lambda ) &= p(\beta | \lambda ) p(\lambda ) \\
            &= \pi_{1,0} \cdot \mathcal{N} \left( \begin{bmatrix}
                0 \\
                0 \\
            \end{bmatrix}, \begin{bmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{bmatrix} \right) + \pi_{2,0} \cdot \mathcal{N} \left( \begin{bmatrix}
                0 \\
                0 \\
            \end{bmatrix}, \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
            \end{bmatrix} \right)
\end{align*}
so,
\begin{align*}
    p(\beta , \lambda | y) &= p(\beta | \lambda , y) p(\lambda | y) \\
            &= \pi_{1, n} p(\beta | y, \lambda = 1) + \pi_{2, n} p(\beta | y, \lambda = 0)
\end{align*}

To get $p(\theta , \lambda | y)$, we can consider $p(\lambda | y)$, whose mode is given by: $\arg \max_{\lambda} p(y | \lambda )$,
and we can also consider the posterior $p(\theta | y)$, which equals $\int p(\theta | y, \lambda ) p(y | \lambda ) d \lambda $.