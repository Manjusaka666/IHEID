\begin{solution}
    \

   \textbf{Step 1:} Write the pdf of observations $x_i | \theta$
   
   Since $x_i = \theta + u_i$, and we assume $u_i \sim \mathcal{N}(0, \sigma^2)$, then $x_i \sim \mathcal{N}(\theta, \sigma^2)$, and we have:
   \[
   p(x_i | \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left\{-\frac{1}{2} \frac{(x_i - \theta)^2}{\sigma^2}\right\}}.
   \]

   \textbf{Step 2:} Define Likelihood Function

   We have assumed that observations in the sample are independent. Thus, 
   \[
   L_n(\theta) = \prod\limits_{i=1}^{n}p(x_i | \theta) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \theta)^2\right\}}
   \]
   Log-linearize the function, and we define the log-likelihood function:
   \[
   \ell_{n}(\theta) = n\log{\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)} - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \theta)^2
   \]

   \textbf{Step 3:} Define the Likelihood Estimation problem and find the $\hat{\theta}$

   For maximum likelihood estimation, we need to solve the following problem:
   \[
       \hat{\theta}_{ML} = \arg\max\limits_{\theta \in \Theta}{L_n(\theta)} = \arg\max\limits_{\theta \in \Theta}{\ell_n(\theta)}
   \]
   So, we need to maximize:
   \[
   \ell_{n}(\theta) = n\log{\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)} - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \theta)^2
   \]
   Take the derivative of $\ell_n(\theta)$ with respect to $\theta$, and set it to zero for maximization,
   \begin{align*}
       \frac{\partial \ell_n(\theta)}{\partial \theta} &= \frac{1}{\sigma^2} \sum_{i=1}^{n}(x_i - \theta) \\
       &= \frac{1}{\sigma^2}(\sum_{i=1}^{n}x_i - n\theta) \\
       &= 0
   \end{align*}
   Thus, we have
   \[
   \hat{\theta}_{ML} = \frac{1}{n}\sum_{i=1}^{n}x_i
   \]
   
\end{solution}

\begin{solution}
    \ 

    \textbf{Step 1:} Find the likelihood function
    
    For $\mathcal{H}_0: \theta = \theta_0$, the likelihood function is:
    \[
    L_n(\theta_0) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \theta_0)^2\right\}}
    \]
    For $\mathcal{H}_1: \theta \neq \theta_0$, the maximum likelihood estimator is $\hat{\theta}_{ML} = \frac{1}{n}\sum\limits_{i=1}^{n}x_i = \hat{\theta}$. The likelihood function is:
    \[
    L_n(\theta_1) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \hat{\theta})^2\right\}}
    \]

    \textbf{Step 2:} Define the likelihood ratio test and $c_\alpha$

    The likelihood ratio and the test is firstly defined as follows(we'll simplify to another version later):
    \[
    \varphi_{LR}(x) = \mathbf{1}\left\{LR_n < c\right\}
    \]
    \begin{align*}
         LR_n &= \frac{L_n(\theta_1)}{L_n(\theta_0)} \\
              &= \exp{\left\{ \frac{1}{2\sigma^2}\left[\sum_{i=1}^{n} (x_i - \theta_0)^2 - \sum_{i=1}^{n} (x_i - \hat{\theta})^2\right]\right\}}
    \end{align*}
    Denote 
    \[
    D = \sum\limits_{i=1}^{n} (x_i - \theta_0)^2 - \sum\limits_{i=1}^{n} (x_i - \hat{\theta})^2
    \]
    Using the identity:
    \[
    \sum\limits_{i=1}^{n} (x_i - \theta_0)^2 = \sum\limits_{i=1}^{n} (x_i - \hat{\theta} + \hat{\theta} - \theta_0)^2 = \sum\limits_{i=1}^{n} (x_i - \hat{\theta})^2 + n(\hat{\theta} - \theta_0)^2
    \]
    Thus, 
    \begin{align*}
        D &= \sum\limits_{i=1}^{n} (x_i - \theta_0)^2 - \sum\limits_{i=1}^{n} (x_i - \hat{\theta})^2 \\
          &= \sum\limits_{i=1}^{n} (x_i - \hat{\theta})^2 + n(\hat{\theta} - \theta_0)^2 - \sum\limits_{i=1}^{n} (x_i - \hat{\theta})^2 \\
          &= n(\hat{\theta} - \theta_0)^2
    \end{align*}
    So, the likelihood ratio $LR_n$ is:
    \[
    LR_n = \exp{\left\{\frac{n}{2\sigma^2} (\hat{\theta} - \theta_0)^2\right\}}
    \]
    Then, we simplify the expression and define the test statistic $T(x)$ as below:
    \[
    T(x) = 2\log{(LR_n)} = \frac{n}{\sigma^2}(\hat{\theta} - \theta_0)^2
    \]
    And our LR test would be:
    \[
    \varphi_{LR}(x) = \mathbf{1}\left\{ T(x) = \frac{n}{\sigma^2}(\hat{\theta} - \theta_0)^2 < c'\right\}
    \]
    where $c' = 2\log(c)$.
    To get a size $\alpha$ test, we find $c'$ so as to set the Type I error to $\alpha$, which is:
    \[
    \mathbb{P}\left[ T(x) \geq c' | \mathcal{H}_0\right] = \alpha
    \]
    we can denote that $c' = c_\alpha$.

    \textbf{Step 3:} Determine the distribution of $T(x)$ under $\mathcal{H}_0$ and find the value of $c_\alpha$

    Under $\mathcal{H}_0$, $\hat{\theta} \sim \mathcal{N}(\theta_0, \frac{\sigma^2}{n})$, because:
    \begin{align*}
        \mathbb{E}[\hat{\theta}] &= \mathbb{E}\left[\frac{1}{n}\sum\limits_{i=1}^{n}x_i\right] = \frac{1}{n}\sum\limits_{i=1}^{n}\mathbb{E}[x_i] = \frac{1}{n}\cdot n\theta_0 = \theta_0 \\
        \mathbb{V}[\hat{\theta}] &= \mathbb{V}\left[ \frac{1}{n}\sum\limits_{i=1}^{n}x_i\right] = \frac{1}{n^2} \sum\limits_{i=1}^{n}\mathbb{V}[x_i] = \frac{1}{n}\cdot n\sigma^2 = \frac{\sigma^2}{n}
    \end{align*}
    Then, standardizing $\hat{\theta}$, we'll have:
    \[
    Z = \frac{\hat{\theta} - \theta_0}{\sqrt{\sigma^2/n}} \sim \mathcal{N}(0, 1)
    \]
    Using the hint, we know that
    \[
    Z^2 = \left(\frac{\hat{\theta} - \theta_0}{\sqrt{\sigma^2/n}}\right)^2 = \frac{n}{\sigma^2}(\hat{\theta} - \theta_0)^2 \sim \chi_{1}^{2}
    \]
    Therefore, under $\mathcal{H}_0$,
    \[
    T(x) = \frac{n}{\sigma^2}(\hat{\theta} - \theta_0)^2 = Z^2 \sim \chi_{1}^2
    \]
    Given $\alpha=0.05$, 
    \[
    c_\alpha = \chi_{1, 0.95}^{2} \approx 3.8415
    \]
    \textbf{Step 4:} Set the decision rule
    \begin{itemize}
        \item Reject $\mathcal{H}_0$: $T(x) > c_\alpha = 3.8415$
        \item Do not reject  $\mathcal{H}_0$: $T(x) \leq c_\alpha = 3.8415$
    \end{itemize}
\end{solution}

\begin{solution}
    \ 

    We have $\sigma^2=6$, $n=4$, $x_1=178$, $x_2=161$,$x_3=168$, $x_4=172$, $\theta_0=175$, so $\hat{\theta}=169.75$.

    Put this data back into our $T(x)$ and LR test, we have:
    \[
    T(x) = \frac{n}{\sigma^2}(\hat{\theta}-\theta_0)^2 = \frac{4}{6}(169.75-175)^2 = 18.735 > 3.8415
    \]
    We reject $\mathcal{H}_0$.
\end{solution}

\begin{solution}
    \

Numerical approximation of $c_\alpha$: 3.6266

Analytical$ c_\alpha$ from chi-squared distribution: 3.8415

Difference between numerical and analytical $c_\alpha$: 0.2148, which is about 5.6\% of the analytical $c_\alpha$, so our approximation is not very close to the true value $c_\alpha$.

I expect the estimated approximation to get closer to the real analytical value of $c_\alpha$ as $M$ is larger.

Since the $T(x)$ we get is 18.735 which is greatly larger than 3.84 and 3.62, which is our numerical result, the conclusion from previous exercise doesn't change, we still reject $\mathcal{H}_{0}$.
\end{solution}


\begin{solution}
    \

    Based on our previous LR test, we have:
    \begin{align*}
        \varphi_{LR}(x) &= \mathbf{1}\left\{ T(x) = \frac{n}{\sigma^2}(\hat{\theta} - \theta_0)^2 < c_\alpha\right\} \\
        &= \mathbf{1}\left\{(\hat{\theta} - \theta_0)^2 < \frac{c_\alpha\sigma^2}{n}\right\} \\
        &= \mathbf{1}\left\{-\sqrt{\frac{c_\alpha\sigma^2}{n}} < (\hat{\theta} - \theta_0) < \sqrt{\frac{c_\alpha\sigma^2}{n}}\right\} \\
        &= \mathbf{1}\left\{\hat{\theta} -\sqrt{\frac{c_\alpha\sigma^2}{n}} < \theta_0 < \hat{\theta} + \sqrt{\frac{c_\alpha\sigma^2}{n}}\right\}
    \end{align*}
    Thus, we can define $C(X)$ as:
    \[
    C(X) = \left[\hat{\theta} -\sqrt{\frac{c_\alpha\sigma^2}{n}}, \hat{\theta} +\sqrt{\frac{c_\alpha\sigma^2}{n}}  \right]
    \]
    Apply our previous data: $\sigma^2=6$, $n=4$, $x_1=178$, $x_2=161$,$x_3=168$, $x_4=172$, $\theta_0=175$, $\hat{\theta}=169.75$, and $c_\alpha=3.8415$, we have:
    \[
    C(X) = [169.75-2.4, 169.75+2.4] = [167.35, 172.15]
    \]
    $\theta_0=175$ is not in this interval.

    Because we rejected $\mathcal{H}_0: \theta=175$, it's consistent that 175 is not within the 95\% confidence interval.
\end{solution}