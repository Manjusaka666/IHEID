In previous lectures we have discussed the least squares estimation of the linear regression model. In this lecture we will discuss the likelihood-based inference.
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
         & ch2 & ch3(LRM) \\
        \hline
        LS & $\mathbb{E}[y_i|\theta]$ & $\mathbb{E}[y_i|x_i, \beta]$ \\
        ML & $p(y_i | \theta )$ & $p(y_i|x_i, \beta )$ \\
        \hline
    \end{tabular}
\end{table}

\section{ML for LRM}
\[y_i = x^{\prime} _i \beta + u_i\]
For LS, we assume $\mathbb{E}[u_i|x_i] = 0$, which gives $\mathbb{E}[y_i|x_i] = x^{\prime} _i \beta.$
For ML, we assume $u_i \sim N(0, \sigma^2)$, which gives $y_i|x_i \sim N(x^{\prime} _i \beta, \sigma^2).$
\[
p(y_i|x_i) = (2\pi \sigma^2)^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2 \sigma ^2} (y_i - x^{\prime} _i \beta )^2\right\}
\]
We use $\theta $ to represent parameters $(\beta, \sigma^2).$
\begin{align*}
    \Rightarrow \mathcal{L}(\theta |y_i, x) &= p(y | x_i, \theta ) \\
    &= \prod_{i=1}^{n}p(y_i|x_i,\theta )\\
    &= \prod_{i=1}^{n} (2\pi \sigma^2)^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2 \sigma ^2} (y_i - x^{\prime} _i \beta )^2\right\}\\
    &= (2\pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma ^2} \sum_{i=1}^{n}(y_i - x^{\prime} _i \beta )^2\right\}
\end{align*}

Then, we can get the log-likelihood function:
\begin{align*}
    \ell(\theta |y_i, x) &= \log \mathcal{L}(\theta |y_i, x)\\
    &= -\frac{n}{2} \log (2\pi \sigma^2) - \frac{1}{2 \sigma ^2} (Y - X\beta )^{\prime} (Y - X\beta )\\
    \Rightarrow \hat{\theta }_{ML} &= (\hat{\beta}_{ML}, \hat{\sigma}^2_{ML}) = \arg\max_{\theta=(\beta, \sigma^2) } \ell(\theta |y_i, x)
\end{align*}

Take the first order condition (FOC) of $\beta$ and $\sigma^2$:
\begin{align*}
    \beta: &\frac{1}{\sigma^2}X'(Y - X\beta ) = 0\\
    \Rightarrow\hat{\beta}_{ML} &= (X'X)^{-1}X'Y\\
    \sigma^2:& -\frac{n}{2\sigma^2} + \frac{2}{(2\sigma^2)^2} (Y - X\beta)'(Y - X\beta) = 0\\
    \Rightarrow \hat{\sigma}^2_{ML} &= \frac{1}{n} (Y - X\beta)'(Y - X\beta) = \frac{1}{n} \sum_{i=1}^{n} \hat{u}^2_i = \hat{\sigma}^2_{LS}
\end{align*}

As we know that:
\[\hat{\beta } = (X^{\prime} X)^{-1}X^{\prime} Y = \beta +(X^{\prime} X)^{-1}X^{\prime} U = \beta + \left(\frac{1}{n}\sum x_i x^{\prime} _i \right)^{-1}\sum x_i u_i\]
\[\Rightarrow \hat{\beta }|X \sim \mathcal{N} (\beta , V)\]
where
\begin{align*}
    V &= \mathbb{V}\left[(X^{\prime} X)^{-1}X^{\prime} U |X\right]\\
    &= (X^{\prime} X)^{-1}\mathbb{V}[X^{\prime} U|X](X^{\prime} X)^{-1}\\
    &= (X^{\prime} X)^{-1}X^{\prime} \mathbb{V}[U|X] X(X^{\prime} X)^{-1}\\
    &= (X^{\prime} X)^{-1}X^{\prime} \sigma^2 I X(X^{\prime} X)^{-1}\\
    &= \sigma^2 (X^{\prime} X)^{-1}
\end{align*}

We define the \textbf{Score Function} as:
\begin{definition}[Score Function]
    \begin{align*}
        S(\theta ) &= \frac{\partial \ell(\theta |y_i, x)}{\partial \theta }\\
        &= \begin{bmatrix}
            \frac{\partial \ell(\theta |y_i, x)}{\partial \beta }\\
            \frac{\partial \ell(\theta |y_i, x)}{\partial \sigma^2}
        \end{bmatrix}
    \end{align*}
\end{definition}


As we know that:
\begin{align*}
    \frac{\partial \ell(\theta |y_i, x)}{\partial \beta } &= \frac{1}{\sigma^2}X'(Y - X\beta )\\
    \frac{\partial \ell(\theta |y_i, x)}{\partial \sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} (Y - X\beta )'(Y - X\beta )
\end{align*}

We take the Hessians of $\beta$ and $\sigma^2$:
\begin{align*}
    \mathcal{H} (\theta ) &= \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \theta \partial \theta ^{\prime}} = \frac{\partial S(\theta )}{\partial \theta^{\prime}}\\
    &= \begin{bmatrix}
        \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \beta} & \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \sigma^2}\\
        \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \sigma^2 \partial \beta} & \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \sigma^2 \partial \sigma^2}
    \end{bmatrix}\\
    \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \beta } &= -\frac{1}{\sigma^2}X^{\prime} X\\
    \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \sigma^2 \partial \sigma^2} &= \frac{n}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} (Y - X\beta )'(Y - X\beta )\\
    \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \sigma^2} &= \frac{1}{(\sigma^2)^2}X'(Y - X\beta )
\end{align*}

    
Then, we can get the \textbf{Information Matrix} as: 
\begin{definition}[Information Matrix]
    \begin{align*}
        I(\theta ) &= \mathbb{E}[s(\theta )s(\theta )^{\prime}] =  -\mathbb{E}\left[\frac{\partial^2 \ell(\theta |y_i, x)}{\partial \theta \partial \theta ^{\prime}}\right]\\
        &= -\mathbb{E}\left[\begin{bmatrix}
            \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \beta } & \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \beta \partial \sigma^2}\\
            \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \sigma^2 \partial \beta} & \frac{\partial^2 \ell(\theta |y_i, x)}{\partial \sigma^2 \partial \sigma^2}
        \end{bmatrix}\right]
    \end{align*}
\end{definition}

\begin{align*}
    \mathbb{E}[s(\beta )s(\beta )^{\prime} ] &= -\mathbb{E}\left[ \frac{1}{\sigma ^2}X^{\prime}(Y - X\beta)\left[\frac{1}{\sigma^2} X^{\prime} (Y-X\beta) \right]^{\prime} \right]\\
    &= \mathbb{E}\left[\frac{1}{\sigma^4}X^{\prime}U U^{\prime} X\right]\\
    &= \frac{1}{\sigma^4}X^{\prime} \mathbb{E}[U U^{\prime}] X\\
    &= \frac{1}{\sigma^4}X^{\prime} \sigma^2 I X\\
    &= \frac{1}{\sigma^2}X^{\prime}X \\
    &= \mathbb{E}[-\mathcal{H}(\beta )]
\end{align*}

Then, we could have the \textbf{Cramer-Rao Lower Bound}:
\begin{definition}[Cramer-Rao Lower Bound]
    \ 

    Let $\tilde{\theta }$ be an unbiased estimator of $\theta $, then:
    \begin{align*}
        \mathbb{V}[\tilde{\theta }|X] &\geq I^{-1}(\theta )\\
        &= \sigma^2 (X^{\prime}X)^{-1}
    \end{align*}
\end{definition}

Take a model $\ell(\theta |y)$, and
\begin{align*}
    \hat{\theta} &= \argmax_{\theta}\ell(\theta|y) \\
    \bar{\theta} &= \argmax_{\theta, g(\theta)=0} \ell(\theta|y)
\end{align*}

and $\sqrt{n}(\hat{\theta_0} - \theta) \overset{d}{\rightarrow} \mathcal{N}(0, V)$. 
We want to test: $\mathcal{H}_0: g(\theta)=0.$
Previously, we have three tests:
\begin{itemize}
    \item t-test: $t = \frac{\hat{\theta} - \theta_0}{\sqrt{\frac{1}{n}\hat{V}}} \overset{d}{\rightarrow} \mathcal{N}(0, 1)$
    \item Wald Test: $W = n g(\hat{\theta})^{\prime} [G(\hat{\theta})\hat{V}G(\hat{\theta})^{\prime}]^{-1}g(\hat{\theta}) \sim \chi_k^2$
    \item LR Test: $LR = -2(\ell(\hat{\theta}_0) - \ell(\bar{\theta})) \sim \chi_k^2$
    \item LM Test: $LM = S(\bar{\theta})^{\prime}  I(\bar{\theta})^{-1} S(\bar{\theta}) \sim \chi_k^2$
\end{itemize}

