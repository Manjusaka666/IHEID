\section{Asymptotic Normality of M-Estimators}

The objective function for M-estimators is 
\begin{equation}
    Q_n(\theta) = -\frac{1}{n} \sum_{t=1}^n m(w_t; \theta).
\end{equation}

It will be convenient to give symbols to the gradient (vector of first derivatives) and the Hessian (matrix of second derivatives) of the $m$ function as
\begin{align}
    s(w_t; \theta) &= \frac{\partial m(w_t; \theta)}{\partial \theta} \\
    H(w_t; \theta) &= \frac{\partial s(w_t; \theta)}{\partial \theta'} = \frac{\partial^2 m(w_t; \theta)}{\partial \theta \partial \theta'}.
\end{align}

In analogy to ML, $s(w_t; \theta)$ will be referred to as the \textbf{score vector for observation $t$}. 
This $s(w_t; \theta)$ should not be confused with the score in the usual sense, 
which is the gradient of the objective function $Q_n(\theta)$. 
The score in the latter sense will be denoted $s_n(\theta)$ later in this chapter. 
The same applies to the Hessian: $H(w_t; \theta)$ will be referred to as the \textbf{Hessian for observation $t$}, 
and the Hessian of $Q_n(\theta)$ will be denoted $H_n(\theta)$.

The goal of this subsection is the asymptotic normality of $\hat{\theta}$ described below. 
In the process of deriving it, we will make a number of assumptions, which will be collected in Proposition 7.8 below. Assume that $m(w_t; \theta)$ is differentiable in $\theta$ and that $\hat{\theta}$ is in the interior of $\Theta$. So $\hat{\theta}$, being the interior solution to the problem of maximizing $Q_n(\theta)$, satisfies the first-order conditions
\begin{equation}\label{eq: 9.4}
    0 = \frac{\partial Q_n(\hat{\theta})}{\partial \theta} = \frac{1}{n} \sum_{t=1}^n s(w_t; \hat{\theta}).
\end{equation}

We now use the following result from calculus:

\begin{theorem}[Mean Value Theorem]
    \

    Let $h : \mathbb{R}^p \rightarrow \mathbb{R}^q$ be continuously differentiable. Then $h(x)$ admits the mean value expansion
    \begin{equation}
        h(x) = h(x_0) + \frac{\partial h(\bar{x})}{\partial x'} (x - x_0),
    \end{equation}
    where $\tilde{x}$ is a mean value lying between $x$ and $x_0$.
    \footnote{The Mean Value Theorem only applies to individual elements of $h$, so that $\overline{x}$ actually differs from element to
    element of the vector equation. This complication does not affect the discussion in the text.}
\end{theorem}

Setting $q = p$, $x = \hat{\theta}$, $x_0 = \theta_0$, and $h(\cdot) = \frac{\partial Q_n(\cdot)}{\partial \theta}$ in the Mean Value Theorem, 
we obtain the following mean value expansion:
\begin{align}
    \frac{\partial Q_n(\hat{\theta})}{\partial \theta} &= \frac{\partial Q_n(\theta_0)}{\partial \theta} + \frac{\partial^2 Q_n(\bar{\theta})}{\partial \theta \partial \theta'} (\hat{\theta} - \theta_0) \\
    &= \frac{1}{n} \sum_{t=1}^n s(w_t; \theta_0) + \left[\frac{1}{n} \sum_{t=1}^n H(w_t; \bar{\theta}) \right] (\hat{\theta} - \theta_0),
\end{align}
where $\tilde{\theta}$ is a mean value that lies between $\hat{\theta}$ and $\theta_0$. 
The continuous differentiability requirement of the Mean Value Theorem is satisfied if $m(w_t; \theta)$ is twice continuously differentiable with respect to $\theta$. 
Combining this equation with the first-order condition above, we obtain
\begin{equation}
    0 = \frac{1}{n} \sum_{t=1}^n s(w_t; \theta_0) + \left[\frac{1}{n} \sum_{t=1}^n H(w_t; \bar{\theta}) \right] (\hat{\theta} - \theta_0).
\end{equation}
Assuming that $\frac{1}{n} \sum_{t=1}^n H(w_t; \bar{\theta})$ is nonsingular, this equation can be solved for $\hat{\theta} - \theta_0$ to yield
\begin{equation}
    \sqrt{n}(\hat{\theta} - \theta_0) = -\left[\frac{1}{n} \sum_{t=1}^n H(w_t; \bar{\theta})\right]^{-1} \frac{1}{\sqrt{n}} \sum_{t=1}^n s(w_t; \theta_0).
\end{equation}
This expression for $(\sqrt{n}$ times) the sampling error will be referred to as the mean value expansion for the sampling error. 
Note that the score vector $s(w_t; \theta_0)$ is evaluated at the true parameter value $\theta_0$.

Now, since $\bar{\theta}$ lies between $\theta_0$ and $\hat{\theta}$, $\bar{\theta}$ is consistent for $\theta_0$ if $\hat{\theta}$ is. If $\{w_t\}$ is ergodic stationary, 
it is natural to conjecture that
\begin{equation}\label{eq: 9.10}
    \frac{1}{n} \sum_{t=1}^n H(w_t; \bar{\theta}) \rightarrow \mathbb{E}[H(w_t; \theta_0)].
\end{equation}

The ergodic stationarity of $w_t$ and consistency of $\hat{\theta}$ alone, however, 
are not enough to ensure this; some technical condition needs to be assumed. 
One such technical condition is the uniform convergence of $\frac{1}{n} \sum_{t=1}^n H(w_t; \cdot)$ to $\mathbb{E}[H(w_t; \cdot)]$ in a neighborhood of $\theta_0$. 
By the Uniform Convergence Theorem, the uniform convergence of $\frac{1}{n} \sum_{t=1}^n H(w_t; \cdot)$ is satisfied if the following dominance condition is satisfied for the Hessian: 
for some neighborhood $\mathcal{N}$ of $\theta_0$, $\mathbb{E} \left[ \sup_{\mu \in \mathcal{N}} \| H(w_t; \theta) \| \right] < \infty$. This is a sufficient condition for (\ref{eq: 9.10}); 
if you can directly verify (\ref{eq: 9.10}), then there is no need to verify the dominance condition for asymptotic normality.

Finally, if (\ref{eq: 9.10}) holds and if 
\begin{equation}
    \frac{1}{\sqrt{n}} \sum_{t=1}^n s(w_t; \theta_0) \xrightarrow{d} N(0, \Sigma),
\end{equation}
then by the Slutsky theorem we have
\begin{equation}
    \sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} N \left(0, \left(\mathbb{E}[H(w_t; \theta_0)]\right)^{-1} \Sigma \left(\mathbb{E}[H(w_t; \theta_0)]\right)^{-1} \right).
\end{equation}

\begin{proposition}[Asymptotic normality of M-estimators]\label{Prop: Asy-nor-M-est}
    \

    Suppose that the conditions of either Proposition 7.3 or Proposition 7.4 are satisfied, so that $\{w_t\}$ is ergodic stationary and the M-estimator $\hat{\theta}$ defined by (7.1.1) and (7.1.2) is consistent. Suppose, further, that
    \begin{itemize}
        \item[(1)] $\theta_0$ is in the interior of $\Theta$,
        \item[(2)] $m(w_t; \theta)$ is twice continuously differentiable in $\theta$ for any $w_t$,
        \item[(3)] $\frac{1}{\sqrt{n}} \sum_{t=1}^n s(w_t; \theta_0) \xrightarrow{d} N(0, \Sigma), \Sigma$ positive definite, 
        where $s(w_t; \theta)$ is defined previously,
        \item[(4)] \textbf{Local dominance condition on the Hessian} for some neighborhood $\mathcal{N}$ of $\theta_0$,
        \[
        \mathbb{E} \left[ \sup_{\theta \in \mathcal{N}} \| H(w_t; \theta) \| \right] < \infty,
        \]
        so that for any consistent estimator $\tilde{\theta}$,
        \[
        \frac{1}{n} \sum_{t=1}^n H(w_t; \tilde{\theta}) \xrightarrow{p} \mathbb{E}[H(w_t; \theta_0)],
        \]
        where $H(w_t; \theta)$ is defined previously.
        \item[(5)] $\mathbb{E}[H(w_t; \theta_0)]$ is nonsingular.
    \end{itemize}
    Then $\hat{\theta}$ is asymptotically normal with
    \[
    \operatorname{Avar}(\hat{\theta}) = \left( \mathbb{E}[H(w_t; \theta_0)] \right)^{-1} \Sigma \left( \mathbb{E}[H(w_t; \theta_0)] \right)^{-1}.
    \]
    (This is Theorem 4.1.3 of Amemiya (1985)\cite{amemiya1985advanced} adapted to M-estimators). Two remarks are in order.
\end{proposition}

\begin{remark}
    \

    \begin{itemize}
        \item Of the assumptions we have made in the derivation of asymptotic normality, 
        the following are not listed in the proposition: 
        \begin{enumerate}
            \item[(i)] $\hat{\theta}$ is an interior point,
            \item[(ii)] $\frac{1}{n} \sum_{t=1}^n H(w_t; \hat{\theta})$ is nonsingular.
        \end{enumerate}
        It is intuitively clear that these conditions hold because $\hat{\theta}$ converges in probability to an interior point $\theta_0$, 
        and $\frac{1}{n} \sum_{t=1}^n H(w_t; \hat{\theta})$ converges in probability to a nonsingular matrix $\mathbb{E}[H(w_t; \theta_0)]$. 
        
        See Newey and McFadden (1994, p. 2152) for a rigorous proof. This sort of technicality will be ignored in the rest of this chapter.
    
        \item If $w_t$ is ergodic stationary, then so is $s(w_t; \theta_0)$ and the matrix $\Sigma$ is the long run variance matrix of $\{s(w_t; \theta_0)\}$. 
        A sufficient condition for (3) is Gordin's condition introduced in Section 6.5. 
        So condition (3) in the proposition can be replaced by Gordin's condition on $\{s(w_t; \theta_0)\}$. 
        
        It is satisfied, for example, if $w_t$ is i.i.d. and $\mathbb{E}[s(w_t; \theta_0)] = 0$. 
        The assumption that $\Sigma$ is positive definite is not really needed for the conclusion of the proposition, 
        but we might as well assume it here because in virtually all applications it is satisfied (or assumed) and also 
        because it will be required in the discussion of hypothesis testing later in this chapter.
    \end{itemize}
\end{remark}

\subsection{Consistent Asymptotic Variance Estimation}
To use this asymptotic result for hypothesis testing, we need a consistent estimate of
\[
\operatorname{Avar}(\hat{\theta}) = \left( \mathbb{E}[H(w_t; \theta_0)]^{-1} \Sigma \mathbb{E}[H(w_t; \theta_0)]^{-1} \right)^{-1}.
\]
The long-run variance was introduced in Section 6.5.

Since $\hat{\theta} \stackrel{p}{\rightarrow} \theta_0$, condition (4) of Proposition \ref{Prop: Asy-nor-M-est} implies that
\[
\frac{1}{n} \sum_{t=1}^n H(w_t; \hat{\theta}) \xrightarrow{p} \mathbb{E}[H(w_t; \theta_0)].
\]
Therefore, provided that there is available a consistent estimator $\hat{\Sigma}$ of $\Sigma$,
\[
\operatorname{Avar}(\hat{\theta}) = \left[ \frac{1}{n} \sum_{t=1}^n H(w_t; \hat{\theta}) \right]^{-1} \hat{\Sigma} \left[ \frac{1}{n} \sum_{t=1}^n H(w_t; \hat{\theta}) \right]^{-1}
\]
is a consistent estimator of the asymptotic variance matrix. 

To obtain $\hat{\Sigma}$, the methods introduced in Section 6.6, 
such as the VARHAC, can be applied to the estimated series $\{s(w_t; \hat{\theta})\}$ under some suitable technical conditions.

\subsection{Asymptotic Normality of Conditional ML}