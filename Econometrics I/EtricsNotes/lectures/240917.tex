\[ x : X \sim p(x|\theta) \]
$\theta$ is the parameter setting the shape of the distribution.

\begin{definition}
  \textbf{\color{blue}Point Estimator} $\delta(X)$
  
  A mapping $\delta$ from sample space of $X$ to the parameter space $\Theta$:
  $\delta : X \rightarrow \Theta$
  
  Given $X$, what's the best $\Theta$. $\delta(x)$ is an estimate.
\end{definition}

\section{Methods to get $\delta(X)$}

\subsection{LS Estimation}

\[ X_1 \sim p(x|\theta), \quad \mathbb{E}[X_1|\theta] = \theta, \quad X_1|\theta \sim N(0, 1) \]
Point estimator $\hat{\theta}$ is the argument that minimizes the objective function
\[ \hat{\theta}_{\text{LS}} = \arg \min_{\theta} \sum_{i=1}^n (x_i - \theta)^2 \]
where we assume that $\theta = \mathbb{E}[x_i|\theta]$. Using the First Order Condition (FOC) to solve, we have
\[ \frac{\partial (\cdot)}{\partial \theta} = \sum_{i=1}^n -2(x_i - \theta) = 0 \]
We get $\hat{\theta}_{\text{LS}} = \frac{1}{n} \sum_{i=1}^n x_i$

\subsection{Method of Moments}

Find $\hat{\theta}$ such that
\[ \mathbb{E}[X|\hat{\theta}_{\text{MM}}] = \frac{1}{n} \sum_{i=1}^n x_i \]
Or, such that
\[ \mathbb{E}[X^2|\theta]_{\theta = \hat{\theta}_{\text{MM}}} = \frac{1}{n} \sum_{i=1}^n x_i^2 \]
$\mathbb{V}[X|\theta] = 1$, thus $\mathbb{E}[X^2|\theta] = \mathbb{V}[X|\theta] + (\mathbb{E}[X|\theta]^2) = 1 + \theta^2$. 
\[ 1 + \hat{\theta}_{\text{MM}}^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 \]

We get
\[ \hat{\theta}_{\text{MM}} = \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2 - 1} \]
\begin{note}
  Choose $\hat{\theta}_{\text{MM}}$ s.t. $\mathbb{E}[h(X)|\theta]$ under $\theta = \hat{\theta}_{\text{MM}}$ is the mean of samples $\frac{1}{n} \sum_{i=1}^n x_i^2$.
\end{note}

\subsection{Maximum Likelihood}

\[ \hat{\theta}_{\text{ML}} = \arg \max_{\theta} \mathcal{L}(\theta|x) \]
where
\[ \mathcal{L}(\theta|x) = p(x|\theta) \]
is the PDF of the RV $X|\theta$.

\begin{note}
  Specify the whole distribution.
\end{note}

With $X$ as i.i.d. distribution, we have
\begin{align*}
  \mathcal{L}(\theta|x) & = p(x|\theta) \\
  & = \prod_{i=1}^n p(x_i|\theta) \\
  & = \prod_{i=1}^n (2\pi\sigma^2)^{-\frac{1}{2}} \exp \left\{ -\frac{1}{2} \left( \frac{x_i - \mu}{\sigma} \right)^2 \right\} \\
  & = (2\pi \sigma^2)^{-\frac{n}{2}} \exp \left\{ -\frac{1}{2 \sigma^2} \sum (x_i - \mu)^2 \right\} \\
  & \sim N(0, \sigma^2)
\end{align*}
Then, let's define $\ell(\theta|x) = \log \mathcal{L}(\theta|x) = -\frac{n}{2} \log(2\pi) -\frac{1}{2\sigma^2} \sum (x_i - \mu)^2$
\[ \hat{\theta}_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n x_i = \hat{\theta}_{\text{MM}} = \hat{\theta}_{\text{LS}} \]
\begin{align*}
  \hat{\theta}_{\text{ML}} & = \arg \max_{\theta} -\frac{n}{2} \log(2\pi) - \frac{1}{2\sigma^2} \sum (x_i - \mu)^2\\
  & = \arg \min_{\theta} \sum (x_i - \mu)^2
\end{align*}
\begin{definition}
  $\delta(X)$ is \underline{unbiased} if $\mathbb{E}[\delta(X)|\theta] = \theta$.
\end{definition}

e.g. $\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i$ is unbiased because
\begin{align*}
  \mathbb{E}[\hat{\theta}|\theta] & = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n X_i|\theta \right] \\
  & = \frac{1}{n} \mathbb{E}\left[\sum X_i \right]\\
  & = \frac{1}{n} \sum \mathbb{E}[X_i]\\
  & = \theta
\end{align*}
However, $\hat{\theta}_{*} = \frac{1}{n-1} \sum_{i=1}^n X$ is biased, because
\begin{align*}
  \mathbb{E}[\hat{\theta}_{*}|\theta] & = \mathbb{E}\left[\frac{1}{n-1} \sum_{i=1}^n X_i|\theta \right] \\
  & = \frac{1}{n-1} \mathbb{E}\left[\sum X_i \right]\\
  & = \frac{n}{n-1} \theta\\
  & \neq \theta
\end{align*}
For the variance,
\[ \mathbb{V}[\hat{\theta}|\theta] = \mathbb{V}\left[\frac{1}{n} \sum_{i=1}^n X_i \right] = \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}[X_i|\theta] = \frac{\sigma^2}{n} \]
and,
\[ \mathbb{V}[\hat{\theta}_{*}|\theta] = \mathbb{V}\left[\frac{n}{n-1} \hat{\theta} \right] = \frac{n^2}{(n-1)^2} \mathbb{V}[\hat{\theta}] = \frac{n \sigma^2}{(n-1)^2} \]
To get the estimation of $\mathbb{E}$ and $\mathbb{V}$, we have to assume the mean and variance of the distribution.

Under $X_i|\theta \sim N(0, 1)$, we have $\hat{\theta} = \frac{1}{n} \sum X_i \sim N\left(0, \frac{1}{n} \right)$.

\section{Asymptotic Properties}

\begin{definition}
  Point estimator $\delta(X)$ is \underline{consistent} if $\delta(X) \overset{p}{\rightarrow} \theta$.
\end{definition}

By Weak Law of Large Numbers (WLLN),
\[ \hat{\theta} = \frac{1}{n} \sum X_i \overset{p}{\rightarrow} \mathbb{E}[X] = \theta \]
Now let's look at $\hat{\theta}_{*}$ again,
\[ \hat{\theta}_{*} = \frac{1}{n-1} \sum X_i = \frac{n}{n-1} \frac{1}{n} \sum X_i = \frac{n}{n-1} \mathbb{E}[X] = \frac{n}{n-1} \hat{\theta} \overset{p}{\rightarrow} \theta \]
as $\lim_{n \rightarrow \infty} \frac{n}{n-1} = 1$ and $\hat{\theta} \overset{p}{\rightarrow} \theta$. Slutsky's theorem tells us that we can form the limit of their product as the product of the limits.

Central Limit Theorem (CLT):
\begin{align*}
  & \sqrt{n} \frac{\frac{1}{n} \sum X_i - \mathbb{E}[X_i]}{\sqrt{\mathbb{V}[X_i]}} \overset{p}{\rightarrow} N(0, 1) \\
  \Rightarrow & \sqrt{n} \frac{\bar{X}_n - \mu}{\sigma} \overset{d}{\rightarrow} N(0, 1) \text{ Set the variance to 1} \\
  \Rightarrow & \sqrt{n} (\hat{\theta} - \theta) \overset{d}{\rightarrow} N(0, 1) \\
  \Rightarrow & \sqrt{n} (\hat{\theta} - \theta) \overset{\text{approx}}{\sim} N(0, 1) \text{ in our finite sample of } n \\
  \Rightarrow & (\hat{\theta} - \theta) \sim N\left(0, \frac{1}{n}\right) \\
  \Rightarrow & \hat{\theta} \overset{\text{approx}}{\sim} N\left(\theta, \frac{1}{n}\right) 
\end{align*}

\textbf{RECALL}: Statistical inference given $x$, what can we say?
\begin{description}
  \item[1.] Point inference, best guess for $\theta$
  \item[2.] Hypothesis testing, is $\theta$ larger than 1 or not?
  \item[3.] Interval inference, give an interval where you are sure that $\theta$ lies in.
\end{description}