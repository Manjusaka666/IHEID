\section{Finite Sample Properties}

\[ y_i = x'_i \beta + u \]
\[ Y = X \beta + U \]
where
\begin{equation*}
  Y = \begin{bmatrix}
    y_1\\
    \vdots\\
    y_n
  \end{bmatrix}_{n \times 1}, \quad
  X = \begin{bmatrix}
    x'_1\\
    \vdots\\
    x'_n
  \end{bmatrix}_{n \times k}, \quad
  U = \begin{bmatrix}
    u_1\\
    \vdots\\
    u_n
  \end{bmatrix}_{n \times 1}
\end{equation*}

\begin{assumption}\label{Assumption 1}
  (Independent Sampling). Observations $z_i = \{ y_t, x_i \}_{i = 1}^n$ are independent across $i$.
\end{assumption}

\begin{assumption}\label{Assumption 2}
  (Full rank). The matrix $X'X = \sum x_i x'_i$ is of full rank.
\end{assumption}

\begin{assumption}\label{Assumption 3}
  (Conditional Independence). $\mathbb{E}[u_i | x_i] = 0$.

  $\mathbb{E}[y_i] = \mathbb{E}[x'_i \beta + u_i | x_i] = \mathbb{E}[x'_i \beta | x_i] + \mathbb{E}[u_i | x_i] = x'_i \beta$
\end{assumption}

\begin{assumption}\label{Assumption 4}  
  (Homoskedasticity). $\mathbb{V}[u_i | x_i] = \sigma^2$ for all $i$.

  $\mathbb{V}[y_i] = \mathbb{V}[x'_i \beta + u_i | x_i] = \sigma^2$
\end{assumption}

The OLS estimator:
\[ \hat{\beta}_{\text{OLS}} = \arg\min_{\beta \in \mathbb{R}^k} 
   \sum u_i^2 = \arg\min_{\beta \in \mathbb{R}^k} \sum_{i = 1}^n
   (y_i - x'_i \beta)^2 = \arg\min_{\beta \in \mathbb{R}^k} (Y - X
   \beta)'(Y - X \beta) \]
   
\begin{align*}
  \frac{\partial (Y - X \beta)'(Y - X \beta)}{\partial \beta} &= X'(Y - X
  \beta) = 0\\
  &\Rightarrow X'Y - X'X\beta = 0\\
  &\Rightarrow (X'X)^{-1}X'Y = \hat{\beta}
\end{align*}

\[ \hat{Y} = X\hat{\beta} = X(X'X)^{-1}X'Y = P_X Y \]
where $P_X = X(X'X)^{-1}X'$ is the projection matrix.
\[ y = x\hat{\beta} + \hat{u} \rightarrow y = \hat{y} + \hat{u} \]
thus,
\[ \hat{U} = Y - \hat{Y} = Y - P_X Y = (I - P_X)Y = M_X Y \]
where $M_X$ is another projection matrix.
\[ Y = P_X Y + M_X Y = (P_X + M_X)Y. \]

In another sense:
\[\hat{U} = Y - \hat{Y} = X\beta + U - X(X^{\prime} X)^{-1}X^{\prime}(X \beta +U) = (I_n - X(X^{\prime} X)^{-1}X^{\prime})U = (I_n - P_X)U = M_{X} U.\]

$P_X$ and $M_X$ are idempotent: $P_X = P'_X$ and $P_X P_X = P_X$, and are orthogonal to each other: $P_X M_X = M_X P_X = 0$.

The total sum of squares (SST) is given by:
\[ \sum_{i = 1}^n y_i^2 = Y'Y \]
It measures the variability in $y_i$ across observations $i$.

We can decompose it into the explained sum of squares (SSE) and the residual sum of squares (SSR)
\begin{align*}
  \text{SST} = Y'Y &= (P_X Y + M_X Y)'(P_X Y + M_X Y)\\
  &= Y'P_X P_X Y + Y'P_X M_X Y + Y'M_X P_X Y + Y'M_X M_X Y \\
  &= Y'P_X P_X Y + Y'M_X M_X Y\\
  &= \hat{Y}'\hat{Y} + \hat{U}'\hat{U}\\
  &= \text{SSE} + \text{SSR}
\end{align*}

Based on that, we get the $R^2$-statistic as a measure of how well $X$ accounts for the variation in $Y$ in the linear regression model:
\[ R^2 = \frac{\hat{Y}'\hat{Y}}{Y'Y} = 1 - \frac{\hat{U}'\hat{U}}{Y'Y}
   \in [0, 1] \left(\frac{\text{SSE}}{\text{SST}} = 1 - \frac{\text{SSR}}{\text{SST}}\right) \]

Look at \textbf{Assumption \ref{Assumption 3}} again, it always gives product have intercept $x_{i1} = 1$.
\begin{align*}
  \mathbb{E}[\hat{\beta} | X] &= \mathbb{E}[(X'X)^{-1}X'Y | X]\\
  &= \mathbb{E}[(X'X)^{-1}X'(X\beta + U) | X]\\
  &= \mathbb{E}[\beta | X] + \mathbb{E}[(X'X)^{-1}X'U | X]\\
  &= \beta + (X'X)^{-1}X'\mathbb{E}[U | X]\\
  &= \beta\\
  \Rightarrow \mathbb{E}[\hat{\beta}] &= \mathbb{E}[\mathbb{E}[\hat{\beta} | X]] = \beta
\end{align*}

For \textbf{Assumption \ref{Assumption 4}}, the conditional variance of $\hat{\beta}_{OLS} $ is given by:
\begin{align*}
  \mathbb{V}[\hat{\beta} | X] &= \mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)' | X]\\
  &= \mathbb{E}[((X'X)^{-1}X'U)((X'X)^{-1}X'U)' | X] \\
  &= \mathbb{E}[(X'X)^{-1}X'UU'X(X'X)^{-1} | X]\\
  &= (X'X)^{-1}X'\mathbb{E}[UU' | X]X(X'X)^{-1}\\
  &= (X'X)^{-1}X'\sigma^2X(X'X)^{-1}\\
  &= \sigma^2(X'X)^{-1}
\end{align*}

\begin{note}
  \begin{eqnarray*}
    UU^{\prime} &=& \begin{bmatrix}
      u_1\\
      \vdots\\
      u_n 
    \end{bmatrix} \begin{bmatrix}
      u_1 & \cdots & u_n
    \end{bmatrix} = \begin{bmatrix}
      u_1^2 & u_1 u_2 & \cdots & u_1 u_n\\
      u_2 u_1 & u_2^2 & \cdots & u_2 u_n\\
      \vdots & \vdots & \ddots & \vdots\\
      u_n u_1 & u_n u_2 & \cdots & u_n^2
    \end{bmatrix}\\
    \mathbb{E}[UU^{\prime} | X] &=& \begin{bmatrix}
      \mathbb{E}[u_1^2 | X] & \mathbb{E}[u_1 u_2 | X] & \cdots & \mathbb{E}[u_1 u_n | X]\\
      \mathbb{E}[u_2 u_1 | X] & \mathbb{E}[u_2^2 | X] & \cdots & \mathbb{E}[u_2 u_n | X]\\
      \vdots & \vdots & \ddots & \vdots\\
      \mathbb{E}[u_n u_1 | X] & \mathbb{E}[u_n u_2 | X] & \cdots & \mathbb{E}[u_n^2 | X]
    \end{bmatrix}
  \end{eqnarray*}
  By \textbf{Assumption \ref{Assumption 3}} and \textbf{Assumption \ref{Assumption 4}}, we have:
  $\mathbb{E}[u_i| X] = 0$ and $\mathbb{E}[u_i^2 | X] = \sigma^2$. Furthermore, $\mathbb{E}[u_i u_j | X] = 0$ for $i \neq j$.
  \begin{eqnarray*}
    \mathbb{E}[UU^{\prime} | X] &=& \sigma^{2} I_{n} 
  \end{eqnarray*}
\end{note}

By LIE again, the unconditional variance of $\hat{\beta}_{OLS}$ is given by:
\begin{align*}
  \mathbb{V}[\hat{\beta}] &= \mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)']\\
  &= \mathbb{E}[\mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)' | X]]\\
  &= \mathbb{E}[\mathbb{V}[\hat{\beta} | X]]\\
  &= \mathbb{E}[\sigma^2(X'X)^{-1}]\\
  &= \sigma^2\mathbb{E}[(X'X)^{-1}]
\end{align*}

\begin{theorem}[Gauss-Markov Theorem]\label{Gauss-Markov Theorem}
  \

  If \textbf{Assumption \ref{Assumption 1}} to \textbf{Assumption \ref{Assumption 4}} hold, then the OLS estimator $\hat{\beta}_{OLS}$ is the best linear unbiased estimator (\textbf{\textcolor{blue}{BLUE}}) of $\beta$.
  \begin{note}
  \
  
    \begin{itemize}
      \item \textbf{Best} means that the OLS estimator has the smallest variance among all linear unbiased estimators. $\mathbb{V}[\hat{\beta}_{OLS}] \leq \mathbb{V}[\hat{\beta}]$ for all linear unbiased estimators $\hat{\beta}$.
      \item \textbf{Linear} means that the estimator is a linear function of the dependent variable. $\hat{\beta} = c + dY$.
      \item \textbf{Unbiased} means that the expected value of the estimator is equal to the true value of the parameter. $\mathbb{E}[\hat{\beta}] = \beta$.
    \end{itemize}
  \end{note}
\end{theorem}

If, we know $\hat{\beta }$, $\mathbb{E}[\hat{\beta}]$ and $\mathbb{V}[\hat{\beta}]$. To find the unconditional expectation of $\hat{\beta}_{OLS}$, we could only use asymptotic properties of the OLS estimator. We can show that the OLS estimator is consistent and asymptotically normal.

\begin{align*}
  \hat{\beta }_{OLS} &= (X'X)^{-1}X'Y\\
  &= (X'X)^{-1}X'(X\beta + U)\\
  &= \beta + (X'X)^{-1}X'U\\
  &= \beta + \left(\sum x_i x'_i\right)^{-1} \left(\sum x_i u_i \right)\\
  &= \beta + \left(\frac{1}{n}\sum x_i x'_i\right)^{-1} \frac{1}{n}\left(\sum x_i u_i \right)\\
  &\overset{p}{\rightarrow} \beta + \mathbb{E}[x_i x'_i]^{-1} \mathbb{E}[x_i u_i]
\end{align*}

\begin{note}
  By WLLN, we know that $\frac{1}{n}\sum{z_i} \overset{p}{\rightarrow} \mathbb{E}[z_i]$.
  So, $\frac{1}{n}\sum x_i u_i \overset{p}{\rightarrow} \mathbb{E}[x_i u_i] = Q$, and $\left[\frac{1}{n}\sum{x_i x^{\prime} _i}\right]^{-1} \overset{p}{\rightarrow} \left\{\mathbb{E}[x_i x^{\prime} _i]\right\}^{-1} \overset{p}{\rightarrow} \mathbb{E}[x_i x^{\prime} _i]^{-1} = Q^{-1} .$
\end{note}

So, we have:
\begin{align*}
  \hat{\beta}_{OLS} - \beta &\overset{p}{\rightarrow} \mathbb{E}[x_i x'_i]^{-1} \mathbb{E}[x_i u_i]\\
  &= \mathbb{E}[x_i x'_i]^{-1} \mathbb{E}[\mathbb{E}[x_i u_i | x_i]]\\
  &= \mathbb{E}[x_i x'_i]^{-1} \mathbb{E}[x_i \mathbb{E}[u_i | x_i]]\\
  &= \mathbb{E}[x_i x'_i]^{-1} \mathbb{E}[x_i \cdot 0]\\
  &= 0
\end{align*}

\begin{note}
  By the Central Limit Theorem, 
  we know that:
  \begin{align*}
    \sqrt{n}(\hat{\beta}_{OLS} - \beta) &= (X^{\prime} X)^{-1}X^{\prime} U \\
    &= \underset{\mathbb{E}[x_i x^{\prime} _i]^{-1}}{\underbrace{\left(\frac{1}{n}\sum x_i x'_i\right)^{-1}}} \sqrt{n} \underset{\overset{d}{\rightarrow} \mathcal{N}(0, \mathbb{V}[x_i u_i])}{\underbrace{\frac{1}{n}\sum x_i u_i}} \\
    & \overset{p}{\rightarrow} \mathbb{E}[x_i x^{\prime} _i]^{-1} \mathcal{N}(0, \mathbb{V}[x_i u_i])
  \end{align*}
\end{note}

Thus, we have:
\begin{align*}
  \sqrt{n}(\hat{\beta}_{OLS} - \beta) &\overset{d}{\rightarrow} \mathcal{N} (0, \mathbb{E}[x_i x^{\prime} _i]^{-1}\mathbb{V}[x_i u_i] \mathbb{E}[x_i x^{\prime} _i]^{-1^{\prime} } )\\
  \mathbb{V}[x_i u_i] &= \mathcal{N}(0, \mathbb{E}\left[(x_i u_i - \mathbb{E}[x_i u_i])(x_i u_i - \mathbb{E}[x_i u_i])^{\prime} \right]) \\
  &= \mathcal{N} (0, \mathbb{E}[x_i u_i u_i^{\prime} x_i] - \mathbb{E}[x_i u_i] \mathbb{E}[x_i u_i]^{\prime})\\
  &= \mathcal{N} (0, \mathbb{E}[x_i u_i u_i^{\prime} x_i^{\prime} ])\\
  &= \mathcal{N} (0, \mathbb{E}[\mathbb{E}[u_i^2 | x_i] x_i x_i^{\prime}])\\
  &= \mathcal{N} (0, \mathbb{E}[\sigma^2 x_i x_i^{\prime}])\\
  &= \sigma^2 \mathbb{E}[x_i x_i^{\prime}]\\
  &= \sigma ^2 Q \\
  \Rightarrow \sqrt{n}(\hat{\beta}_{OLS} - \beta) &\overset{d}{\rightarrow} \mathcal{N} (0, \mathbb{E}[x_i x^{\prime} _i]^{-1} \sigma^2 \mathbb{E}[x_i x_i^{\prime}] \mathbb{E}[x_i x^{\prime} _i]^{-1} ) = \mathcal{N} (0, \sigma^2 Q^{-1})
\end{align*}

Then,  we could say that:
\begin{itemize}
  \item For finite $n$, $\sqrt{n}(\hat{\beta}_{OLS} - \beta) \overset{approx}{\sim} \mathcal{N} (0, \sigma ^2 Q^{-1});$
  \item $\hat{\beta } \overset{approx}{\sim} \mathcal{N} (\beta , \frac{\sigma^2}{n} Q^{-1})$;
  \item Replace unknown $\sigma^2$ and $Q^{-1}$ by $\hat{\sigma}^2$ and $\hat{Q}^{-1}$ to get the t-distribution.
  We would have: \[\hat{\beta } \overset{approx}{\sim} \mathcal{N} \left(\beta , \frac{\hat{\sigma}^2}{n} \hat{Q}^{-1}\right).\]
\end{itemize}

\section{Hypothesis Testing}
As $\hat{\beta } \overset{approx}{\sim} \mathcal{N} \left(\beta , \frac{\hat{\sigma}^2}{n} \hat{Q}^{-1}\right)$,
we know that:
\[\hat{\beta}_j \overset{approx}{\sim} \mathcal{N}(\beta_{j}, \frac{\hat{\sigma}^2}{n}[\hat{Q}^{-1}]_{jj})\] for a single parameter $\beta_j \in \beta$
where $[\hat{Q}^{-1}]_{jj}$ is the $j$-th diagonal element of $\hat{Q}^{-1}$.

This enables us to test a point hypothesis $\mathcal{H}_{0}: \beta_j = \beta_{j, 0}$ against the alternative $\mathcal{H}_{1}: \beta_j \neq \beta_{j, 0}$ using the t-test:
\[\varphi_t(x) =\mathbf{1}\left\{T_x < c\right\}, \text{with } T_t = \left| \frac{\hat{\beta}_{j,0} - \beta_j}{\hat{\sigma}_{\beta_{j,0}}} \right|,\]
where $\hat{\sigma}_{\beta_{j,0}} = \sqrt{\frac{\hat{\sigma}^2}{n}\hat{Q}_{jj}^{-1}}.$

Because the distribution of $\hat{\beta}_j$ is not exact, but only asymptotically valid, so too does the
resulting test-statistic only asymptotically converge to a standard Normal distribution:
\[t = \frac{\hat{\beta}_j - \beta_{j, 0}}{\sqrt{\frac{\hat{\sigma}^2}{n}[\hat{Q}^{-1}]_{jj}}} \overset{d}{\rightarrow} \mathcal{N}(0, 1)\]

\begin{align*}
  \hat{\beta _j} &\overset{approx}{\sim} \mathcal{N} \left(\beta _j , \underset{V_j}{\underbrace{\frac{\sigma^2}{n}\left(\frac{1}{n} \sum{x_i x_i^{\prime} }\right)^{-1}_{jj}}} \right)\\
  t &= \frac{\hat{\beta _j} - \beta _j}{\sqrt{V_j}} \overset{approx}{\sim} \mathcal{N} (0, 1)\\
\end{align*}

\begin{definition}[Wald Test]\label{Wald Test}
  \ 

  The asymptotic distribution of the Wald-test-statistic, $T_W$,
  follows from asymptotic Normality of $\hat{\beta}$, 
  the Delta Method and the fact that $(X-\mu)^\prime\Sigma^{-1}(X-$ $\mu)\sim\chi_{dim(X)}^2$ for $X\sim N(\mu,\Sigma).$

  Using $\sqrt{n}(\hat{\beta}-\beta_{0})\stackrel{d}{\rightarrow}N(0,V)$ and the Delta method, we get
  $$\sqrt{n}\left(g(\hat{\beta})-g(\beta_0)\right)\overset{d}{\to}G\cdot N(0,V)=N(0,GVG')\:,\quad\mathrm{with}\quad G=\left.\frac{\partial g(\beta)}{\partial\beta}\right|_{\beta=\beta_0}\:.$$
  Therefore,
  $$\sqrt{n}\left(g(\hat{\beta})-g(\beta_0)\right)'[GVG']^{-1}\sqrt{n}\left(g(\hat{\beta})-g(\beta_0)\right)\stackrel{d}{\to}\chi_m^2\:.$$

  Under $\mathcal{H}_0: g(\beta_0)=0.$ Also, because we do not know $\beta_0$, we replace $G$ with $G(\hat{\beta})$, as $\hat{\beta}$ is our
  estimator of $\beta_0.$
\end{definition}

More general hypotheses  $\mathcal{H}_{0}: g(\beta)=0 $ vs.  $\mathcal{H}_{1}: g(\beta) \neq 0$  for some function  $g: \mathbb{R}^{k} \rightarrow \mathbb{R}^{m}$  (i.e. $ m \leq k $ restrictions) can be tested using the Wald test. It uses the following statistic:

$$T_{W}=n g\left(\hat{\beta}_{O L S}\right)^{\prime}\left[G\left(\hat{\beta}_{O L S}\right) \hat{V} G\left(\hat{\beta}_{O L S}\right)^{\prime}\right]^{-1} g\left(\hat{\beta}_{O L S}\right) \xrightarrow{d} \chi_{m}^{2},$$

where $\hat{V}=\hat{\sigma}^{2} \hat{Q}^{-1}$ and where  $G\left(\hat{\beta}_{O L S}\right)=\partial g(\beta) /\left.\partial \beta\right|_{\beta=\hat{\beta}_{O L S}}$  is the $m \times k$ matrix of derivatives of $g$ with respect to $\beta$  evaluated at  $\hat{\beta}_{OLS}$. 
The short derivation in the Appendix illustrates that the Wald test-statistic is based on the idea that if $\mathcal{H}_{0}$  is true, then the difference between  $g\left(\hat{\beta}_{O L S}\right)$ and $g(\beta)=0$ should be small. Suppose we are interested in testing  $\mathcal{H}_{0}:\left\{\beta_{2}+\beta_{3}=5, \beta_{4}=0\right\}$ under a five-dimensional vector $\beta$. 
Then we would take
\[
g(\beta)=\left[\begin{array}{lllll}
0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0
\end{array}\right] \beta-\left[\begin{array}{l}
5 \\
0
\end{array}\right], \quad \text { with } \quad G(\beta)=\left[\begin{array}{lllll}
0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0
\end{array}\right]
\]

If $g(\beta)=0$ is s.t. it tests only $\beta_{j}=\beta_{j, 0}$ for a single $\beta_{j}$ , then the Wald test is equivalent to the t-test:  $\varphi_{W}=\varphi_{t}$.

\begin{theorem}[Delta Method]
  \

  $X \overset{d}{\rightarrow}\mathcal{N} (\mu, \sigma^2)$, and $g: \mathbb{R}^k \rightarrow \mathbb{R}^q$ is a differentiable function. 
  Then, $g(X) \overset{d}{\rightarrow} \mathcal{N} (g(\mu), \sigma^2(g'(\mu))^2)$.

  Let $\beta \in \mathbb{R}^k$ and $g: \mathbb{R}^k \rightarrow \mathbb{R}^q$ be a differentiable function.
  If $\sqrt{n}(\hat{\beta} - \beta) \overset{d}{\rightarrow} \xi$, then
  \[ 
  \sqrt{n}(g(\hat{\beta}) - g(\beta)) \overset{d}{\rightarrow} G^{\prime} \xi,
  \]
  where $G = G(\beta) = \frac{\partial}{\partial \beta}g(\beta)^{\prime}.$

  In particular, if $\xi \sim \mathcal{N}(0, V)$, then
  \[\sqrt{n}(g(\hat{\beta}) - g(\beta)) \overset{d}{\rightarrow} \mathcal{N}(0, G V G^{\prime}).\]

  By previous results, if we have $V = \sigma^2 Q^{-1}$, then
  \[ 
    \sqrt{n}(g(\hat{\beta}) - g(\beta)) \overset{d}{\rightarrow} \mathcal{N} (0, G \sigma^2 Q^{-1} G^{\prime})
  \]
  where $G(u) = \frac{\partial}{\partial u}g(u)^{\prime} $ and $G = G(\beta )$.
\end{theorem}

\section{Violations of Ideal Conditions}

First of all, note that while unbiasedness requires the conditional independence assumption
3 to hold, both consistency and asymptotic Normality go through even under the weaker
exogeneity assumption $\mathbb{E}[u_i x_i] = 0.$\footnote{This is because it's implied by the conditional independence assumption.}

\subsection{Singular $X^{\prime} X$}
If $X^{\prime} X$ is not of full rank, then the OLS estimator is not even defined. There are two reasons
that lead to this case.

\subsection{Heteroskedasticity}
Suppose we replace the Assumption \ref{Assumption 4} with the weaker assumption that $\mathbb{V}[u_i | x_i] = \sigma^2_i$ for all $i$.
Then, the OLS estimator is still unbiased, but the variance of the OLS estimator is now given by:
\begin{align*}
  \mathbb{V}[x_i u_i] &= \mathbb{E}[\left(x_i u_i - \mathbb{E}[x_i u_i]\right) \left(x_i u_i - \mathbb{E}[x_i u_i]\right)^{\prime}]\\
  &= \mathbb{E}[x_i u_i u_i^{\prime}  x_i^{\prime}] - \mathbb{E}[x_i u_i] \mathbb{E}[x_i u_i]^{\prime}\\
  &= \mathbb{E}[\mathbb{E}[u_i^2 | x_i] x_i x_i^{\prime}]\\
  &= \mathbb{E}[x_i x_i^{\prime} u_i^2] \\
  &= \mathbb{E}[x_i x_i^{\prime} \sigma^2_i] \\
  \Rightarrow \sqrt{n}(\hat{\beta}_{OLS} - \beta) &\overset{d}{\rightarrow} \mathcal{N} (0, Q^{-1} \mathbb{E}[x_i x_i^{\prime} u_i^2] Q^{-1})  
\end{align*}

The asymptotic variance can again be estimated by replacing $\mathbb{E}[x_i x_i^{\prime} u_i^2]$ with its sample analogue:
as a consistent estimator: $\frac{1}{n}\sum_{i=1}^{n} x_i x_i^{\prime} \hat{u}_i^2.$

Note that if the variances $\{\sigma_i^2\}_{i=1}^n$ were known, we could transform the heteroskedastic model
into a homoskedastic one by writing the regression as:
\[ \frac{y_i}{\sigma_i} = \frac{x_i^{\prime} }{\sigma_i} \beta + \frac{u_i}{\sigma_i}. \]

In this model, observations are weighted by the inverses of their standard deviations and, as
a result, less noisy observations are given more weight as they are more informative about
the relation between $X$ and $Y$. Let $\mathbb{V}[U|X] = \Sigma = diag{(\sigma_1^2, \cdots, \sigma_n^2)}.$
We can then write the regression in matrix form as:
$\Sigma^{-\frac{1}{2}}Y = \Sigma^{-\frac{1}{2}}X \beta + \Sigma^{-\frac{1}{2}}U,$
with $\mathbb{V}[\Sigma^{-\frac{1}{2}}U|X] = I.$

The OLS estimator in this weighted regression model is referred to as the Generalized Least Squares (GLS) estimator.
It is given by:
\[ \hat{\beta}_{GLS} = \left(\left(\Sigma^{-\frac{1}{2}}X \right)^{\prime}\left(\Sigma^{-\frac{1}{2}}X \right)\right)\left(\Sigma^{-\frac{1}{2}}X \right)^{\prime} \left(\Sigma^{-\frac{1}{2}}Y \right)  = (X^{\prime} \Sigma^{-1} X)^{-1} X^{\prime} \Sigma^{-1} Y. \]

Under otherwise the same conditions as for OLS, this estimator is unbiased\footnote{$\mathbb{E}\left[\hat{\beta}_{GLS}|X\right] = \mathbb{E}\left[\left(X^{\prime} \Sigma^{-1} X\right)^{-1}(X\beta+U)|X \right] = \beta + \mathbb{E}\left[\left(X^{\prime} \Sigma^{-1} X\right)^{-1}U|X\right]=\beta.$} 
and consistent\footnote{$\hat{\beta}_{GLS} - \beta = (X^{\prime} \Sigma^{-1} X)^{-1}X^{\prime} \Sigma^{-1} U \overset{p}{\rightarrow} \frac{1}{n}\mathbb{E}\left[\frac{x_i x_i^{\prime}}{\sigma_i^2}\right] \frac{1}{n}\mathbb{E}\left[\frac{x_i u_i}{\sigma_i^2}\right] \overset{p}{\rightarrow} 0.$}.
and has variance: 
\[
\mathbb{V}[\hat{\beta}_{GLS}] = \mathbb{E}\left[(X^{\prime} \Sigma^{-1} X)^{-1} X^{\prime} \Sigma^{-1} U U^{\prime} \Sigma^{-1} X (X^{\prime} \Sigma^{-1} X)^{-1}\right] = \mathbb{E}\left[(X^{\prime} \Sigma^{-1} X)^{-1}\right].
\]

\subsection{Endogeneity}
\subsubsection{Omitted Variables}
Suppose the true model is given by:
$$y_i=x_i'\beta+z_i'\delta+\varepsilon_i\:,\text{with } \mathbb{E}[x_i\varepsilon_i]=0,$$
i.e. exogeneity holds in this true model, whereas the researcher estimates
$$y_i = x_i^{\prime} \gamma+u_i.$$

Notice that we have written the coefficient as $\gamma$ rather than $\beta$ and the error as $u$ rather than $\varepsilon .$ Goldberger (1991) introduced the catchy labels long regression and short regression to emphasize the distinction.
Typically, $\beta\neq\gamma$,except in special cases. To see this, we calculate
\begin{align*}
  \gamma&=\left(\mathbb{E}\left[XX^{\prime}\right]\right)^{-1}\mathbb{E}\left[XY\right]\\
  &=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}\mathbb{E}\left[X \left(X^{\prime}\beta + Z^{\prime}\delta+\varepsilon\right)\right]\\
  &=\beta+\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}\mathbb{E}\left[X Z^{\prime}\right]\delta\\
  &=\beta+\Gamma\delta
\end{align*}

where $\Gamma=Q^{-1}Q_{XZ}$ is the coefficient matrix from a projection of $Z$ on $X$.

Observe that $\gamma=\beta+\Gamma\delta\neq\beta$ unless $\Gamma=0$ or $\delta=0.$ Thus the short and long regressions have different coefficients. 
They are the same only under one of two conditions. 
First, if the projection of $Z$ on $X$ yields a set of zero coefficients (they are uncorrelated), 
or second, if the coefficient on $Z$ in is zero. 
The difference $\Gamma\delta$ between $\gamma$ and $\beta$ is known as omitted variable bias. 
It is the consequence of omission of a relevant correlated variable.

